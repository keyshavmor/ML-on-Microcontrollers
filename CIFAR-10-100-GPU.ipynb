{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad436e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images (batch x height x width x channel). Sum of ten runs.\n",
      "GPU (s):\n",
      "0.0404721899976721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-03 01:16:33.524132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-03 01:16:33.524327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-03 01:16:33.524440: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-03 01:16:33.524600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-03 01:16:33.524718: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-03 01:16:33.524809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 5701 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Conv2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import timeit\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')\n",
    "\n",
    "def gpu():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    random_image_gpu = tf.random.normal((100, 100, 100, 3))\n",
    "    net_gpu = tf.keras.layers.Conv2D(32, 7)(random_image_gpu)\n",
    "    return tf.math.reduce_sum(net_gpu)\n",
    "  \n",
    "# We run each op once to warm up; see: https://stackoverflow.com/a/45067900\n",
    "gpu()\n",
    "\n",
    "# Run the op several times.\n",
    "print('Time (s) to convolve 32x7x7x3 filter over random 100x100x100x3 images '\n",
    "      '(batch x height x width x channel). Sum of ten runs.')\n",
    "print('GPU (s):')\n",
    "gpu_time = timeit.timeit('gpu()', number=10, setup=\"from __main__ import gpu\")\n",
    "print(gpu_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "305e8256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75d7c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "data_path = 'cifar-10-batches-py/'\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for i in range(1, 6):\n",
    "    batch = unpickle(data_path + 'data_batch_' + str(i))\n",
    "    train_data.append(batch[b'data'])\n",
    "    train_labels += batch[b'labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496d35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate(train_data)\n",
    "train_data = train_data.reshape((50000, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "test_data = unpickle(data_path + 'test_batch')[b'data']\n",
    "test_data = test_data.reshape((10000, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(unpickle(data_path + 'test_batch')[b'labels'])\n",
    "\n",
    "# Load the CIFAR-100 dataset\n",
    "data_path = 'cifar-100-python/'\n",
    "train_data_100 = unpickle(data_path + 'train')[b'data']\n",
    "train_data_100 = train_data_100.reshape((50000, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "test_data_100 = unpickle(data_path + 'test')[b'data']\n",
    "test_data_100 = test_data_100.reshape((10000, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "train_labels_100 = np.array(unpickle(data_path + 'train')[b'fine_labels'])\n",
    "test_labels_100 = np.array(unpickle(data_path + 'test')[b'fine_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d5a372e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10\n",
      "Number of features: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "Number of training samples: 50000\n",
      "\n",
      "CIFAR-100\n",
      "Number of features: (32, 32, 3)\n",
      "Number of classes: 100\n",
      "Number of training samples: 50000\n"
     ]
    }
   ],
   "source": [
    "# Print the number of features, classes, and training samples for CIFAR-10\n",
    "print(\"CIFAR-10\")\n",
    "print(\"Number of features:\", train_data.shape[1:])\n",
    "print(\"Number of classes:\", len(np.unique(train_labels)))\n",
    "print(\"Number of training samples:\", train_data.shape[0])\n",
    "\n",
    "# Print the number of features, classes, and training samples for CIFAR-100\n",
    "print(\"\\nCIFAR-100\")\n",
    "print(\"Number of features:\", train_data_100.shape[1:])\n",
    "print(\"Number of classes:\", len(np.unique(train_labels_100)))\n",
    "print(\"Number of training samples:\", train_data_100.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e294dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Shuffle your dataset\n",
    "train_data, train_labels = shuffle(train_data, train_labels, random_state=42)\n",
    "train_data_100, train_labels_100 = shuffle(train_data_100, train_labels_100, random_state=42)\n",
    "test_data, test_labels = shuffle(test_data, test_labels, random_state=42)\n",
    "test_data_100, test_labels_100 = shuffle(test_data_100, test_labels_100, random_state=42)\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train = train_data.astype('float32') / 255.0\n",
    "x_test = test_data.astype('float32') / 255.0\n",
    "x_train_100 = train_data_100.astype('float32') / 255.0\n",
    "x_test_100 = test_data_100.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y_train = to_categorical(train_labels, num_classes=10)\n",
    "y_test = to_categorical(test_labels, num_classes=10)\n",
    "y_train_100 = to_categorical(train_labels_100, num_classes=100)\n",
    "y_test_100 = to_categorical(test_labels_100, num_classes=100)\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# create data generator\n",
    "datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "# prepare iterator\n",
    "it_train = datagen.flow(x_train, y_train, batch_size=64)\n",
    "# fit model\n",
    "steps = int(x_train.shape[0] / 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76844843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build ResNet-50 model\n",
    "base_model = ResNet50(include_top=False, weights=None, input_shape=(32, 32, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "model_10 = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# define the ModelCheckpoint callback to save the model after each epoch\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_10_gpu.{epoch:02d}.h5', save_freq='epoch')\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Define a learning rate schedule function\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"\n",
    "    Returns a learning rate based on the epoch number.\n",
    "    \"\"\"\n",
    "    learning_rate = 0.001\n",
    "    if (epoch>1 and epoch<=3):\n",
    "        learning_rate = 0.00075\n",
    "    if (epoch>3 and epoch<=5):\n",
    "        learning_rate = 0.0005\n",
    "    if (epoch>5 and epoch<=7):\n",
    "        learning_rate = 0.00025\n",
    "    if (epoch>7 and epoch<=9):\n",
    "        learning_rate = 0.000125\n",
    "    if (epoch>9 and epoch<=11):\n",
    "        learning_rate = 0.0000625\n",
    "    if (epoch>11 and epoch<=13):\n",
    "        learning_rate = 0.00003125\n",
    "    if (epoch>13 and epoch<= 15):\n",
    "        learning_rate = 0.00001575\n",
    "    if (epoch>15):\n",
    "        learning_rate = 0.000007375\n",
    "    return learning_rate\n",
    "\n",
    "# Create a LearningRateScheduler callback\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "model_10.compile(optimizer=Adam(learning_rate=learning_rate),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "affed6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 1.4341 - accuracy: 0.4784 - val_loss: 1.6464 - val_accuracy: 0.3920 - lr: 0.0010\n",
      "Epoch 2/25\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 1.2852 - accuracy: 0.5405 - val_loss: 1.6181 - val_accuracy: 0.4538 - lr: 0.0010\n",
      "Epoch 3/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 1.2506 - accuracy: 0.5536 - val_loss: 3.0872 - val_accuracy: 0.3677 - lr: 7.5000e-04\n",
      "Epoch 4/25\n",
      "781/781 [==============================] - 25s 31ms/step - loss: 1.1465 - accuracy: 0.5916 - val_loss: 1.9163 - val_accuracy: 0.4807 - lr: 7.5000e-04\n",
      "Epoch 5/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.9990 - accuracy: 0.6515 - val_loss: 1.2009 - val_accuracy: 0.5980 - lr: 5.0000e-04\n",
      "Epoch 6/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 1.0823 - accuracy: 0.6154 - val_loss: 1.0325 - val_accuracy: 0.6445 - lr: 5.0000e-04\n",
      "Epoch 7/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.8900 - accuracy: 0.6877 - val_loss: 0.8546 - val_accuracy: 0.7045 - lr: 2.5000e-04\n",
      "Epoch 8/25\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.8835 - accuracy: 0.6893 - val_loss: 0.8135 - val_accuracy: 0.7194 - lr: 2.5000e-04\n",
      "Epoch 9/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.7544 - accuracy: 0.7356 - val_loss: 0.7842 - val_accuracy: 0.7322 - lr: 1.2500e-04\n",
      "Epoch 10/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.7124 - accuracy: 0.7493 - val_loss: 0.8085 - val_accuracy: 0.7190 - lr: 1.2500e-04\n",
      "Epoch 11/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.6713 - accuracy: 0.7656 - val_loss: 0.6901 - val_accuracy: 0.7622 - lr: 6.2500e-05\n",
      "Epoch 12/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.6426 - accuracy: 0.7736 - val_loss: 0.7641 - val_accuracy: 0.7357 - lr: 6.2500e-05\n",
      "Epoch 13/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.6139 - accuracy: 0.7849 - val_loss: 0.6709 - val_accuracy: 0.7692 - lr: 3.1250e-05\n",
      "Epoch 14/25\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5979 - accuracy: 0.7903 - val_loss: 0.6680 - val_accuracy: 0.7670 - lr: 3.1250e-05\n",
      "Epoch 15/25\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5811 - accuracy: 0.7954 - val_loss: 0.6637 - val_accuracy: 0.7711 - lr: 1.5750e-05\n",
      "Epoch 16/25\n",
      "781/781 [==============================] - 25s 31ms/step - loss: 0.5774 - accuracy: 0.7959 - val_loss: 0.6748 - val_accuracy: 0.7685 - lr: 1.5750e-05\n",
      "Epoch 17/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.5650 - accuracy: 0.8015 - val_loss: 0.6572 - val_accuracy: 0.7719 - lr: 7.3750e-06\n",
      "Epoch 18/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.5690 - accuracy: 0.8017 - val_loss: 0.6609 - val_accuracy: 0.7718 - lr: 7.3750e-06\n",
      "Epoch 19/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.5600 - accuracy: 0.8044 - val_loss: 0.6534 - val_accuracy: 0.7736 - lr: 7.3750e-06\n",
      "Epoch 20/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.5600 - accuracy: 0.8025 - val_loss: 0.6499 - val_accuracy: 0.7742 - lr: 7.3750e-06\n",
      "Epoch 21/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.5540 - accuracy: 0.8040 - val_loss: 0.6650 - val_accuracy: 0.7721 - lr: 7.3750e-06\n",
      "Epoch 22/25\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5479 - accuracy: 0.8075 - val_loss: 0.6612 - val_accuracy: 0.7707 - lr: 7.3750e-06\n",
      "Epoch 23/25\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5464 - accuracy: 0.8085 - val_loss: 0.6535 - val_accuracy: 0.7768 - lr: 7.3750e-06\n",
      "Epoch 24/25\n",
      "781/781 [==============================] - 24s 31ms/step - loss: 0.5456 - accuracy: 0.8090 - val_loss: 0.6570 - val_accuracy: 0.7735 - lr: 7.3750e-06\n",
      "Epoch 25/25\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.5472 - accuracy: 0.8084 - val_loss: 0.6506 - val_accuracy: 0.7767 - lr: 7.3750e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5fec72d5b0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Fit\n",
    "model_10.fit(it_train, steps_per_epoch=steps, epochs=25, validation_data=(x_test, y_test), callbacks=[lr_scheduler,checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "058c511a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 9ms/step - loss: 0.6506 - accuracy: 0.7767\n",
      "Test accuracy: 0.7767000198364258\n"
     ]
    }
   ],
   "source": [
    "# test the accuracy of the model on the test set\n",
    "loss, accuracy = model_10.evaluate(x_test, y_test)\n",
    "\n",
    "# print the accuracy of the model on the test set\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14760815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "781/781 [==============================] - 15s 18ms/step - loss: 4.6402 - accuracy: 0.0100 - val_loss: 4.6052 - val_accuracy: 0.0110\n",
      "Epoch 2/200\n",
      "781/781 [==============================] - 14s 18ms/step - loss: 4.6042 - accuracy: 0.0100 - val_loss: 4.5970 - val_accuracy: 0.0123\n",
      "Epoch 3/200\n",
      "781/781 [==============================] - 16s 20ms/step - loss: 4.5304 - accuracy: 0.0220 - val_loss: 4.4067 - val_accuracy: 0.0414\n",
      "Epoch 4/200\n",
      "781/781 [==============================] - 14s 18ms/step - loss: 4.3638 - accuracy: 0.0373 - val_loss: 4.2061 - val_accuracy: 0.0746\n",
      "Epoch 5/200\n",
      "781/781 [==============================] - 14s 17ms/step - loss: 4.2109 - accuracy: 0.0570 - val_loss: 4.0179 - val_accuracy: 0.0974\n",
      "Epoch 6/200\n",
      "781/781 [==============================] - 13s 17ms/step - loss: 4.0810 - accuracy: 0.0723 - val_loss: 3.8621 - val_accuracy: 0.1278\n",
      "Epoch 7/200\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 3.9738 - accuracy: 0.0887 - val_loss: 3.7737 - val_accuracy: 0.1447\n",
      "Epoch 8/200\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 3.8814 - accuracy: 0.1033 - val_loss: 3.6530 - val_accuracy: 0.1566\n",
      "Epoch 9/200\n",
      "781/781 [==============================] - 14s 18ms/step - loss: 3.8058 - accuracy: 0.1092 - val_loss: 3.5938 - val_accuracy: 0.1764\n",
      "Epoch 10/200\n",
      "781/781 [==============================] - 14s 18ms/step - loss: 3.7347 - accuracy: 0.1221 - val_loss: 3.4705 - val_accuracy: 0.1894\n",
      "Epoch 11/200\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 3.6667 - accuracy: 0.1331 - val_loss: 3.4110 - val_accuracy: 0.1906\n",
      "Epoch 12/200\n",
      "781/781 [==============================] - 14s 18ms/step - loss: 3.6031 - accuracy: 0.1442 - val_loss: 3.3774 - val_accuracy: 0.1973\n",
      "Epoch 13/200\n",
      "781/781 [==============================] - 15s 19ms/step - loss: 3.5473 - accuracy: 0.1551 - val_loss: 3.2865 - val_accuracy: 0.2125\n",
      "Epoch 14/200\n",
      "781/781 [==============================] - 14s 18ms/step - loss: 3.4953 - accuracy: 0.1636 - val_loss: 3.2206 - val_accuracy: 0.2331\n",
      "Epoch 15/200\n",
      "781/781 [==============================] - 14s 18ms/step - loss: 3.4506 - accuracy: 0.1704 - val_loss: 3.1446 - val_accuracy: 0.2432\n",
      "Epoch 16/200\n",
      "781/781 [==============================] - 14s 18ms/step - loss: 3.4073 - accuracy: 0.1755 - val_loss: 3.0967 - val_accuracy: 0.2501\n",
      "Epoch 17/200\n",
      "781/781 [==============================] - 14s 18ms/step - loss: 3.3558 - accuracy: 0.1851 - val_loss: 3.1145 - val_accuracy: 0.2515\n",
      "Epoch 18/200\n",
      "652/781 [========================>.....] - ETA: 2s - loss: 3.3276 - accuracy: 0.1933"
     ]
    }
   ],
   "source": [
    "# create data generator\n",
    "datagen_100 = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "# prepare iterator\n",
    "it_train_100 = datagen.flow(x_train_100, y_train_100, batch_size=64)\n",
    "# fit model\n",
    "steps = int(x_train_100.shape[0] / 64)\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\n",
    "\n",
    "model_100 = Sequential()\n",
    "model_100.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "model_100.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_100.add(MaxPooling2D((2, 2)))\n",
    "model_100.add(Dropout(0.2))\n",
    "model_100.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_100.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_100.add(MaxPooling2D((2, 2)))\n",
    "model_100.add(Dropout(0.3))\n",
    "model_100.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_100.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_100.add(MaxPooling2D((2, 2)))\n",
    "model_100.add(Dropout(0.4))\n",
    "model_100.add(Flatten())\n",
    "model_100.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model_100.add(Dropout(0.5))\n",
    "model_100.add(Dense(100, activation='softmax'))\n",
    "\n",
    "# define the ModelCheckpoint callback to save the model after each epoch\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_100_gpu.{epoch:02d}.h5', save_freq='epoch')\n",
    "\n",
    "learning_rate = 0.0001\n",
    "\n",
    "# Define a learning rate schedule function\n",
    "def lr_schedule(epoch):\n",
    "    \"\"\"\n",
    "    Returns a learning rate based on the epoch number.\n",
    "    \"\"\"\n",
    "    learning_rate = 0.001\n",
    "    if (epoch>1 and epoch<=3):\n",
    "        learning_rate = 0.00075\n",
    "    if (epoch>3 and epoch<=5):\n",
    "        learning_rate = 0.0005\n",
    "    if (epoch>5 and epoch<=7):\n",
    "        learning_rate = 0.00025\n",
    "    if (epoch>7 and epoch<=9):\n",
    "        learning_rate = 0.000125\n",
    "    if (epoch>9 and epoch<=11):\n",
    "        learning_rate = 0.0000625\n",
    "    if (epoch>11 and epoch<=13):\n",
    "        learning_rate = 0.00003125\n",
    "    if (epoch>13 and epoch<= 15):\n",
    "        learning_rate = 0.00001575\n",
    "    if (epoch>15):\n",
    "        learning_rate = 0.000007375\n",
    "    return learning_rate\n",
    "\n",
    "# Create a LearningRateScheduler callback\n",
    "lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "\n",
    "model_100.compile(optimizer=Adam(learning_rate=learning_rate),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# Model Fit\n",
    "model_100.fit(it_train_100, steps_per_epoch=steps, epochs=200, validation_data=(x_test_100, y_test_100), callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f336e51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
