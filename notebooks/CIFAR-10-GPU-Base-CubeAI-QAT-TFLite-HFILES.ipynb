{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aac31db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 18:44:53.426181: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 18:44:53.426488: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 18:44:53.426631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 18:44:53.426815: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 18:44:53.426952: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 18:44:53.427055: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /device:GPU:0 with 5779 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:07:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Conv2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import timeit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  print(\n",
    "      '\\n\\nThis error most likely means that this notebook is not '\n",
    "      'configured to use a GPU.  Change this in Notebook Settings via the '\n",
    "      'command palette (cmd/ctrl-shift-P) or the Edit menu.\\n\\n')\n",
    "  raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cea4e6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10\n",
      "Number of features: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "Number of training samples: 50000\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "data_path = 'cifar-10-batches-py/'\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for i in range(1, 6):\n",
    "    batch = unpickle(data_path + 'data_batch_' + str(i))\n",
    "    train_data.append(batch[b'data'])\n",
    "    train_labels += batch[b'labels']\n",
    "    \n",
    "    \n",
    "train_data = np.concatenate(train_data)\n",
    "train_data = train_data.reshape((50000, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "test_data = unpickle(data_path + 'test_batch')[b'data']\n",
    "test_data = test_data.reshape((10000, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(unpickle(data_path + 'test_batch')[b'labels'])\n",
    "\n",
    "# Print the number of features, classes, and training samples for CIFAR-10\n",
    "print(\"CIFAR-10\")\n",
    "print(\"Number of features:\", train_data.shape[1:])\n",
    "print(\"Number of classes:\", len(np.unique(train_labels)))\n",
    "print(\"Number of training samples:\", train_data.shape[0])\n",
    "\n",
    "# Shuffle your dataset\n",
    "train_data, train_labels = shuffle(train_data, train_labels, random_state=42)\n",
    "test_data, test_labels = shuffle(test_data, test_labels, random_state=42)\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train = train_data.astype('float32') / 255.0\n",
    "x_test = test_data.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y_train = to_categorical(train_labels, num_classes=10)\n",
    "y_test = to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "# create data generator\n",
    "datagen = ImageDataGenerator(zoom_range=[0.9,1.1],\n",
    "                             rotation_range=30,\n",
    "                             brightness_range=[0.9,1.1],\n",
    "                             width_shift_range=0.3, \n",
    "                             height_shift_range=0.3,\n",
    "                             vertical_flip=True,\n",
    "                             horizontal_flip=True)\n",
    "# prepare iterator\n",
    "it_train = datagen.flow(x_train, y_train, batch_size=64)\n",
    "# fit model\n",
    "steps = int(x_train.shape[0] / 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4489905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 8, 8, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 353,962\n",
      "Trainable params: 353,962\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 15:03:35.290749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 15:03:35.290943: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 15:03:35.291066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 15:03:35.291349: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 15:03:35.291479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 15:03:35.291600: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 15:03:35.291772: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 15:03:35.291899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-17 15:03:35.291988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5759 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:07:00.0, compute capability: 8.6\n",
      "/home/themandalorian/anaconda3/envs/ml-on-mc/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', \n",
    "                    input_shape=(32, 32, 3)))\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.2))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.3))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((4, 4)))\n",
    "model_10.add(Dropout(0.4))\n",
    "model_10.add(Flatten())\n",
    "model_10.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model_10.add(Dropout(0.5))\n",
    "model_10.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_10.summary()\n",
    "\n",
    "# define the ModelCheckpoint callback to save the model after each epoch\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_10_gpu_Base-CubeAI.h5', save_freq='epoch')\n",
    "\n",
    "# compile model\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "model_10.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55789588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-17 15:03:41.303600: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8100\n",
      "2023-05-17 15:03:42.753793: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 7s 6ms/step - loss: 2.2705 - accuracy: 0.1367 - val_loss: 2.1441 - val_accuracy: 0.2177\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0634 - accuracy: 0.2140 - val_loss: 1.9059 - val_accuracy: 0.2737\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8974 - accuracy: 0.2657 - val_loss: 1.8181 - val_accuracy: 0.3277\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7788 - accuracy: 0.3157 - val_loss: 1.6390 - val_accuracy: 0.3989\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6987 - accuracy: 0.3476 - val_loss: 1.5518 - val_accuracy: 0.4307\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6314 - accuracy: 0.3803 - val_loss: 1.5057 - val_accuracy: 0.4511\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5726 - accuracy: 0.4080 - val_loss: 1.4266 - val_accuracy: 0.4867\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5227 - accuracy: 0.4322 - val_loss: 1.4063 - val_accuracy: 0.4931\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.4720 - accuracy: 0.4540 - val_loss: 1.3014 - val_accuracy: 0.5250\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.4293 - accuracy: 0.4732 - val_loss: 1.2994 - val_accuracy: 0.5398\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.3978 - accuracy: 0.4876 - val_loss: 1.2435 - val_accuracy: 0.5595\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.3551 - accuracy: 0.5061 - val_loss: 1.2358 - val_accuracy: 0.5570\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.3202 - accuracy: 0.5205 - val_loss: 1.2017 - val_accuracy: 0.5687\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.2843 - accuracy: 0.5351 - val_loss: 1.2551 - val_accuracy: 0.5552\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.2596 - accuracy: 0.5450 - val_loss: 1.1070 - val_accuracy: 0.6068\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.2289 - accuracy: 0.5580 - val_loss: 1.0850 - val_accuracy: 0.6096\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.2032 - accuracy: 0.5682 - val_loss: 1.0544 - val_accuracy: 0.6245\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1726 - accuracy: 0.5780 - val_loss: 1.0543 - val_accuracy: 0.6186\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1575 - accuracy: 0.5848 - val_loss: 1.0036 - val_accuracy: 0.6472\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1320 - accuracy: 0.5971 - val_loss: 0.9892 - val_accuracy: 0.6464\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1134 - accuracy: 0.6032 - val_loss: 0.9671 - val_accuracy: 0.6571\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0942 - accuracy: 0.6110 - val_loss: 0.9828 - val_accuracy: 0.6471\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0756 - accuracy: 0.6170 - val_loss: 0.9951 - val_accuracy: 0.6407\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0535 - accuracy: 0.6252 - val_loss: 0.9853 - val_accuracy: 0.6471\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0403 - accuracy: 0.6313 - val_loss: 0.9142 - val_accuracy: 0.6769\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0317 - accuracy: 0.6339 - val_loss: 0.9221 - val_accuracy: 0.6681\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0138 - accuracy: 0.6402 - val_loss: 0.8714 - val_accuracy: 0.6845\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0003 - accuracy: 0.6467 - val_loss: 0.8675 - val_accuracy: 0.6915\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9830 - accuracy: 0.6509 - val_loss: 0.8921 - val_accuracy: 0.6824\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9679 - accuracy: 0.6577 - val_loss: 0.8399 - val_accuracy: 0.7022\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9531 - accuracy: 0.6632 - val_loss: 0.8488 - val_accuracy: 0.6968\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9469 - accuracy: 0.6655 - val_loss: 0.8374 - val_accuracy: 0.6987\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9273 - accuracy: 0.6726 - val_loss: 0.7917 - val_accuracy: 0.7173\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9182 - accuracy: 0.6758 - val_loss: 0.7919 - val_accuracy: 0.7175\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9118 - accuracy: 0.6797 - val_loss: 0.7734 - val_accuracy: 0.7254\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8970 - accuracy: 0.6839 - val_loss: 0.7899 - val_accuracy: 0.7196\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8911 - accuracy: 0.6857 - val_loss: 0.7678 - val_accuracy: 0.7246\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8788 - accuracy: 0.6905 - val_loss: 0.7525 - val_accuracy: 0.7301\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8721 - accuracy: 0.6918 - val_loss: 0.7469 - val_accuracy: 0.7325\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8576 - accuracy: 0.6992 - val_loss: 0.7272 - val_accuracy: 0.7420\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8482 - accuracy: 0.7034 - val_loss: 0.7569 - val_accuracy: 0.7335\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8331 - accuracy: 0.7084 - val_loss: 0.7333 - val_accuracy: 0.7407\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8346 - accuracy: 0.7067 - val_loss: 0.7704 - val_accuracy: 0.7272\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8220 - accuracy: 0.7132 - val_loss: 0.7250 - val_accuracy: 0.7427\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8147 - accuracy: 0.7142 - val_loss: 0.7112 - val_accuracy: 0.7467\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8028 - accuracy: 0.7203 - val_loss: 0.6899 - val_accuracy: 0.7539\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7966 - accuracy: 0.7226 - val_loss: 0.6924 - val_accuracy: 0.7566\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.7892 - accuracy: 0.7239 - val_loss: 0.6847 - val_accuracy: 0.7581\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7859 - accuracy: 0.7268 - val_loss: 0.6672 - val_accuracy: 0.7640\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7791 - accuracy: 0.7282 - val_loss: 0.6831 - val_accuracy: 0.7580\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7651 - accuracy: 0.7346 - val_loss: 0.6848 - val_accuracy: 0.7589\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.7650 - accuracy: 0.7323 - val_loss: 0.6977 - val_accuracy: 0.7535\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.7631 - accuracy: 0.7339 - val_loss: 0.6696 - val_accuracy: 0.7630\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.7498 - accuracy: 0.7397 - val_loss: 0.6860 - val_accuracy: 0.7569\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7463 - accuracy: 0.7397 - val_loss: 0.6659 - val_accuracy: 0.7633\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.7388 - accuracy: 0.7440 - val_loss: 0.6438 - val_accuracy: 0.7764\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.7320 - accuracy: 0.7456 - val_loss: 0.6398 - val_accuracy: 0.7743\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7238 - accuracy: 0.7506 - val_loss: 0.6713 - val_accuracy: 0.7620\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.7180 - accuracy: 0.7511 - val_loss: 0.6487 - val_accuracy: 0.7712\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7127 - accuracy: 0.7531 - val_loss: 0.6203 - val_accuracy: 0.7847\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7064 - accuracy: 0.7539 - val_loss: 0.6241 - val_accuracy: 0.7815\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7012 - accuracy: 0.7556 - val_loss: 0.6045 - val_accuracy: 0.7868\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.6989 - accuracy: 0.7585 - val_loss: 0.6357 - val_accuracy: 0.7783\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.6904 - accuracy: 0.7622 - val_loss: 0.6102 - val_accuracy: 0.7861\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6875 - accuracy: 0.7623 - val_loss: 0.6062 - val_accuracy: 0.7876\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.6786 - accuracy: 0.7648 - val_loss: 0.5991 - val_accuracy: 0.7923\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.6787 - accuracy: 0.7650 - val_loss: 0.6017 - val_accuracy: 0.7899\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6695 - accuracy: 0.7693 - val_loss: 0.6413 - val_accuracy: 0.7764\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.6686 - accuracy: 0.7692 - val_loss: 0.5752 - val_accuracy: 0.8000\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6591 - accuracy: 0.7745 - val_loss: 0.5781 - val_accuracy: 0.8008\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6546 - accuracy: 0.7745 - val_loss: 0.5780 - val_accuracy: 0.8002\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.6559 - accuracy: 0.7750 - val_loss: 0.6006 - val_accuracy: 0.7884\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6456 - accuracy: 0.7775 - val_loss: 0.5786 - val_accuracy: 0.7978\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6455 - accuracy: 0.7775 - val_loss: 0.5821 - val_accuracy: 0.7959\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.6316 - accuracy: 0.7835 - val_loss: 0.5859 - val_accuracy: 0.7965\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6311 - accuracy: 0.7834 - val_loss: 0.5749 - val_accuracy: 0.8010\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6326 - accuracy: 0.7825 - val_loss: 0.5658 - val_accuracy: 0.8039\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6308 - accuracy: 0.7830 - val_loss: 0.5612 - val_accuracy: 0.8066\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6236 - accuracy: 0.7865 - val_loss: 0.5538 - val_accuracy: 0.8101\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6209 - accuracy: 0.7870 - val_loss: 0.5762 - val_accuracy: 0.8000\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.6148 - accuracy: 0.7890 - val_loss: 0.5693 - val_accuracy: 0.8041\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6053 - accuracy: 0.7904 - val_loss: 0.5531 - val_accuracy: 0.8091\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6046 - accuracy: 0.7929 - val_loss: 0.5622 - val_accuracy: 0.8053\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5985 - accuracy: 0.7962 - val_loss: 0.5568 - val_accuracy: 0.8085\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5983 - accuracy: 0.7942 - val_loss: 0.5522 - val_accuracy: 0.8093\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5976 - accuracy: 0.7957 - val_loss: 0.5566 - val_accuracy: 0.8063\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5955 - accuracy: 0.7963 - val_loss: 0.5351 - val_accuracy: 0.8176\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5870 - accuracy: 0.7990 - val_loss: 0.5423 - val_accuracy: 0.8122\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5824 - accuracy: 0.8017 - val_loss: 0.5660 - val_accuracy: 0.8034\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.5776 - accuracy: 0.8027 - val_loss: 0.5337 - val_accuracy: 0.8157\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.5789 - accuracy: 0.8021 - val_loss: 0.5339 - val_accuracy: 0.8153\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 4s 6ms/step - loss: 0.5730 - accuracy: 0.8040 - val_loss: 0.5345 - val_accuracy: 0.8159\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5665 - accuracy: 0.8072 - val_loss: 0.5492 - val_accuracy: 0.8120\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5670 - accuracy: 0.8063 - val_loss: 0.5432 - val_accuracy: 0.8176\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5615 - accuracy: 0.8070 - val_loss: 0.5293 - val_accuracy: 0.8189\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5599 - accuracy: 0.8081 - val_loss: 0.5239 - val_accuracy: 0.8210\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5553 - accuracy: 0.8084 - val_loss: 0.5262 - val_accuracy: 0.8183\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5514 - accuracy: 0.8112 - val_loss: 0.5238 - val_accuracy: 0.8218\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5543 - accuracy: 0.8111 - val_loss: 0.5285 - val_accuracy: 0.8165\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.5472 - accuracy: 0.8122 - val_loss: 0.5236 - val_accuracy: 0.8181\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1e9c8430d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Fit\n",
    "model_10.fit(x_train,y_train,batch_size=64, epochs=100, validation_data=(x_test, y_test),callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58905c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.5236 - accuracy: 0.8181\n",
      "Test accuracy: 0.8180999755859375\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load the pre-saved model\n",
    "model_10 = load_model('model_10_gpu_Base-CubeAI.h5')\n",
    "\n",
    "# test the accuracy of the model on the test set\n",
    "loss, accuracy = model_10.evaluate(x_test, y_test)\n",
    "\n",
    "# print the accuracy of the model on the test set\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5807047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp2ge0xf38/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp2ge0xf38/assets\n",
      "/home/themandalorian/anaconda3/envs/ml-on-mc/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-05-17 15:19:18.816609: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-05-17 15:19:18.816632: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-05-17 15:19:18.817175: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp2ge0xf38\n",
      "2023-05-17 15:19:18.819507: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-17 15:19:18.819524: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp2ge0xf38\n",
      "2023-05-17 15:19:18.826415: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2023-05-17 15:19:18.827707: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-05-17 15:19:18.870606: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmp2ge0xf38\n",
      "2023-05-17 15:19:18.881072: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 63897 microseconds.\n",
      "2023-05-17 15:19:18.906689: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_conv2d_input:0\n",
      "shape: [ 1 32 32  3]\n",
      "type: <class 'numpy.int8'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.int8'>\n",
      "Accuracy of quantized to int8 model is 82.05%\n"
     ]
    }
   ],
   "source": [
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_10)\n",
    "\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "tflite_model_name = 'model_10_gpu_Base-CubeAI-PTQ'\n",
    "        \n",
    "# Set the optimization flag.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Enforce full-int8 quantization\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "# Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name + '.tflite')\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "predictions = np.zeros((len(x_test),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(len(x_test)):\n",
    "    val_batch = x_test[i]\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()\n",
    "    \n",
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 10000\n",
    "print(\"Accuracy of quantized to int8 model is {}%\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a3c7bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_2 (QuantizeL  (None, 32, 32, 3)        3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_conv2d_12 (QuantizeWr  (None, 32, 32, 32)       963       \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_13 (QuantizeWr  (None, 32, 32, 32)       9315      \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_6 (Quan  (None, 16, 16, 32)       1         \n",
      " tizeWrapperV2)                                                  \n",
      "                                                                 \n",
      " quant_dropout_8 (QuantizeWr  (None, 16, 16, 32)       1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_14 (QuantizeWr  (None, 16, 16, 64)       18627     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_15 (QuantizeWr  (None, 16, 16, 64)       37059     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_7 (Quan  (None, 8, 8, 64)         1         \n",
      " tizeWrapperV2)                                                  \n",
      "                                                                 \n",
      " quant_dropout_9 (QuantizeWr  (None, 8, 8, 64)         1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_16 (QuantizeWr  (None, 8, 8, 128)        74115     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_17 (QuantizeWr  (None, 8, 8, 128)        147843    \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_8 (Quan  (None, 2, 2, 128)        1         \n",
      " tizeWrapperV2)                                                  \n",
      "                                                                 \n",
      " quant_dropout_10 (QuantizeW  (None, 2, 2, 128)        1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_flatten_2 (QuantizeWr  (None, 512)              1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_4 (QuantizeWrap  (None, 128)              65669     \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      " quant_dropout_11 (QuantizeW  (None, 128)              1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_5 (QuantizeWrap  (None, 10)               1295      \n",
      " perV2)                                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 354,897\n",
      "Trainable params: 353,962\n",
      "Non-trainable params: 935\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 1.9776 - accuracy: 0.2553 - val_loss: 1.5797 - val_accuracy: 0.4193\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 1.5486 - accuracy: 0.4260 - val_loss: 1.3384 - val_accuracy: 0.5127\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 1.3827 - accuracy: 0.4979 - val_loss: 1.1645 - val_accuracy: 0.5789\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 1.2408 - accuracy: 0.5552 - val_loss: 1.0751 - val_accuracy: 0.6203\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 1.1298 - accuracy: 0.5969 - val_loss: 1.0024 - val_accuracy: 0.6437\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 1.0515 - accuracy: 0.6298 - val_loss: 0.9325 - val_accuracy: 0.6690\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.9839 - accuracy: 0.6539 - val_loss: 0.8506 - val_accuracy: 0.7043\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.9342 - accuracy: 0.6729 - val_loss: 0.7998 - val_accuracy: 0.7172\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.8897 - accuracy: 0.6908 - val_loss: 0.7797 - val_accuracy: 0.7277\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.8612 - accuracy: 0.7019 - val_loss: 0.7542 - val_accuracy: 0.7385\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.8348 - accuracy: 0.7114 - val_loss: 0.7265 - val_accuracy: 0.7503\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.8069 - accuracy: 0.7210 - val_loss: 0.7386 - val_accuracy: 0.7470\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.7861 - accuracy: 0.7285 - val_loss: 0.7019 - val_accuracy: 0.7597\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.7619 - accuracy: 0.7376 - val_loss: 0.6942 - val_accuracy: 0.7615\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.7507 - accuracy: 0.7435 - val_loss: 0.6991 - val_accuracy: 0.7606\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.7346 - accuracy: 0.7476 - val_loss: 0.6763 - val_accuracy: 0.7728\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.7175 - accuracy: 0.7510 - val_loss: 0.6482 - val_accuracy: 0.7812\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.7009 - accuracy: 0.7591 - val_loss: 0.6531 - val_accuracy: 0.7775\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6875 - accuracy: 0.7630 - val_loss: 0.6517 - val_accuracy: 0.7815\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.6799 - accuracy: 0.7665 - val_loss: 0.6475 - val_accuracy: 0.7797\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.6645 - accuracy: 0.7724 - val_loss: 0.6245 - val_accuracy: 0.7856\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.6580 - accuracy: 0.7747 - val_loss: 0.6513 - val_accuracy: 0.7840\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.6456 - accuracy: 0.7782 - val_loss: 0.6197 - val_accuracy: 0.7909\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6457 - accuracy: 0.7795 - val_loss: 0.6291 - val_accuracy: 0.7935\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6233 - accuracy: 0.7868 - val_loss: 0.6064 - val_accuracy: 0.7959\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6273 - accuracy: 0.7869 - val_loss: 0.6094 - val_accuracy: 0.7970\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6194 - accuracy: 0.7863 - val_loss: 0.6221 - val_accuracy: 0.7957\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.6167 - accuracy: 0.7889 - val_loss: 0.6291 - val_accuracy: 0.7865\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.6016 - accuracy: 0.7950 - val_loss: 0.5853 - val_accuracy: 0.8065\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5992 - accuracy: 0.7951 - val_loss: 0.5930 - val_accuracy: 0.8008\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.5869 - accuracy: 0.7971 - val_loss: 0.5883 - val_accuracy: 0.8057\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5854 - accuracy: 0.8018 - val_loss: 0.5786 - val_accuracy: 0.8096\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5814 - accuracy: 0.8020 - val_loss: 0.5996 - val_accuracy: 0.8031\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5791 - accuracy: 0.8018 - val_loss: 0.5895 - val_accuracy: 0.8085\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5695 - accuracy: 0.8039 - val_loss: 0.5833 - val_accuracy: 0.8104\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5676 - accuracy: 0.8055 - val_loss: 0.5847 - val_accuracy: 0.8076\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5646 - accuracy: 0.8059 - val_loss: 0.5631 - val_accuracy: 0.8123\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5581 - accuracy: 0.8105 - val_loss: 0.5841 - val_accuracy: 0.8080\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5516 - accuracy: 0.8108 - val_loss: 0.5723 - val_accuracy: 0.8108\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5534 - accuracy: 0.8103 - val_loss: 0.5832 - val_accuracy: 0.8054\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5499 - accuracy: 0.8127 - val_loss: 0.5874 - val_accuracy: 0.8107\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5518 - accuracy: 0.8128 - val_loss: 0.5856 - val_accuracy: 0.8100\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5433 - accuracy: 0.8155 - val_loss: 0.5494 - val_accuracy: 0.8150\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5391 - accuracy: 0.8163 - val_loss: 0.5591 - val_accuracy: 0.8125\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5309 - accuracy: 0.8171 - val_loss: 0.5763 - val_accuracy: 0.8097\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5325 - accuracy: 0.8182 - val_loss: 0.5786 - val_accuracy: 0.8090\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5274 - accuracy: 0.8207 - val_loss: 0.6008 - val_accuracy: 0.8023\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5323 - accuracy: 0.8187 - val_loss: 0.5786 - val_accuracy: 0.8062\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5265 - accuracy: 0.8214 - val_loss: 0.5905 - val_accuracy: 0.8119\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 13s 16ms/step - loss: 0.5277 - accuracy: 0.8202 - val_loss: 0.5669 - val_accuracy: 0.8133\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5212 - accuracy: 0.8239 - val_loss: 0.5719 - val_accuracy: 0.8103\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5194 - accuracy: 0.8246 - val_loss: 0.5623 - val_accuracy: 0.8225\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5207 - accuracy: 0.8227 - val_loss: 0.5620 - val_accuracy: 0.8210\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5067 - accuracy: 0.8279 - val_loss: 0.5953 - val_accuracy: 0.8096\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5146 - accuracy: 0.8265 - val_loss: 0.5423 - val_accuracy: 0.8205\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5053 - accuracy: 0.8287 - val_loss: 0.5481 - val_accuracy: 0.8246\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5010 - accuracy: 0.8287 - val_loss: 0.5500 - val_accuracy: 0.8273\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.5077 - accuracy: 0.8266 - val_loss: 0.5466 - val_accuracy: 0.8223\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5031 - accuracy: 0.8297 - val_loss: 0.5932 - val_accuracy: 0.8149\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5024 - accuracy: 0.8298 - val_loss: 0.5717 - val_accuracy: 0.8199\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.5066 - accuracy: 0.8295 - val_loss: 0.5516 - val_accuracy: 0.8160\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4968 - accuracy: 0.8302 - val_loss: 0.5606 - val_accuracy: 0.8265\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4863 - accuracy: 0.8351 - val_loss: 0.5538 - val_accuracy: 0.8229\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4928 - accuracy: 0.8331 - val_loss: 0.5586 - val_accuracy: 0.8173\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4943 - accuracy: 0.8319 - val_loss: 0.5919 - val_accuracy: 0.8149\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4891 - accuracy: 0.8315 - val_loss: 0.5788 - val_accuracy: 0.8179\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4920 - accuracy: 0.8336 - val_loss: 0.5841 - val_accuracy: 0.8181\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4900 - accuracy: 0.8332 - val_loss: 0.6051 - val_accuracy: 0.8145\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4841 - accuracy: 0.8336 - val_loss: 0.5809 - val_accuracy: 0.8235\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4872 - accuracy: 0.8350 - val_loss: 0.5577 - val_accuracy: 0.8210\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4783 - accuracy: 0.8361 - val_loss: 0.5607 - val_accuracy: 0.8268\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4812 - accuracy: 0.8362 - val_loss: 0.5551 - val_accuracy: 0.8229\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4787 - accuracy: 0.8373 - val_loss: 0.5707 - val_accuracy: 0.8206\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4800 - accuracy: 0.8372 - val_loss: 0.5745 - val_accuracy: 0.8211\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4763 - accuracy: 0.8376 - val_loss: 0.5536 - val_accuracy: 0.8182\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4795 - accuracy: 0.8364 - val_loss: 0.5926 - val_accuracy: 0.8113\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4691 - accuracy: 0.8397 - val_loss: 0.5731 - val_accuracy: 0.8195\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 12s 16ms/step - loss: 0.4752 - accuracy: 0.8363 - val_loss: 0.5788 - val_accuracy: 0.8235\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4768 - accuracy: 0.8373 - val_loss: 0.6148 - val_accuracy: 0.8118\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4715 - accuracy: 0.8383 - val_loss: 0.6015 - val_accuracy: 0.8274\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4720 - accuracy: 0.8387 - val_loss: 0.5757 - val_accuracy: 0.8231\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4664 - accuracy: 0.8415 - val_loss: 0.5800 - val_accuracy: 0.8167\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4691 - accuracy: 0.8395 - val_loss: 0.5695 - val_accuracy: 0.8269\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4632 - accuracy: 0.8419 - val_loss: 0.5630 - val_accuracy: 0.8282\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4622 - accuracy: 0.8426 - val_loss: 0.5787 - val_accuracy: 0.8204\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4641 - accuracy: 0.8423 - val_loss: 0.5435 - val_accuracy: 0.8273\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4621 - accuracy: 0.8442 - val_loss: 0.5744 - val_accuracy: 0.8198\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4612 - accuracy: 0.8432 - val_loss: 0.5530 - val_accuracy: 0.8277\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4615 - accuracy: 0.8441 - val_loss: 0.5732 - val_accuracy: 0.8265\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4570 - accuracy: 0.8440 - val_loss: 0.5922 - val_accuracy: 0.8171\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4623 - accuracy: 0.8432 - val_loss: 0.5605 - val_accuracy: 0.8234\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4658 - accuracy: 0.8434 - val_loss: 0.5790 - val_accuracy: 0.8238\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4590 - accuracy: 0.8436 - val_loss: 0.6217 - val_accuracy: 0.8088\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4607 - accuracy: 0.8439 - val_loss: 0.5618 - val_accuracy: 0.8189\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4656 - accuracy: 0.8438 - val_loss: 0.5673 - val_accuracy: 0.8295\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4606 - accuracy: 0.8416 - val_loss: 0.5837 - val_accuracy: 0.8194\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4587 - accuracy: 0.8448 - val_loss: 0.5611 - val_accuracy: 0.8225\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4616 - accuracy: 0.8436 - val_loss: 0.5654 - val_accuracy: 0.8243\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4567 - accuracy: 0.8446 - val_loss: 0.5588 - val_accuracy: 0.8282\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.4529 - accuracy: 0.8452 - val_loss: 0.6202 - val_accuracy: 0.8119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8680020ee0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', \n",
    "                    input_shape=(32, 32, 3)))\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.2))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.3))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((4, 4)))\n",
    "model_10.add(Dropout(0.4))\n",
    "model_10.add(Flatten())\n",
    "model_10.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model_10.add(Dropout(0.5))\n",
    "model_10.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Convert the model to a quantization aware model\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(model_10)\n",
    "\n",
    "quant_aware_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "quant_aware_model.summary()\n",
    "\n",
    "# Train and evaluate the quantization aware model\n",
    "quant_aware_model.fit(x_train,y_train, batch_size=64,epochs=100,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a952bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization aware training loss:  0.6202415823936462\n",
      "Quantization aware training accuracy:  0.8119000196456909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_12_layer_call_fn, conv2d_12_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, conv2d_13_layer_call_fn, conv2d_13_layer_call_and_return_conditional_losses while saving (showing 5 of 32). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpj0ynl284/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpj0ynl284/assets\n",
      "/home/themandalorian/anaconda3/envs/ml-on-mc/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-05-17 15:39:50.384161: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-05-17 15:39:50.384195: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-05-17 15:39:50.384343: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpj0ynl284\n",
      "2023-05-17 15:39:50.389846: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-17 15:39:50.389863: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpj0ynl284\n",
      "2023-05-17 15:39:50.412612: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-05-17 15:39:50.514293: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpj0ynl284\n",
      "2023-05-17 15:39:50.538872: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 154527 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_conv2d_12_input:0\n",
      "shape: [ 1 32 32  3]\n",
      "type: <class 'numpy.uint8'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.uint8'>\n",
      "Accuracy of quantized to int8 model is 81.17999999999999%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "quant_loss, quant_acc = quant_aware_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Quantization aware training loss: ', quant_loss)\n",
    "print('Quantization aware training accuracy: ', quant_acc)\n",
    "\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant_int8_qat = converter.convert()\n",
    "\n",
    "open(\"model_10_gpu_Base-CubeAI_qat_int8.tflite\", \"wb\").write(tflite_model_quant_int8_qat)\n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(\"model_10_gpu_Base-CubeAI_qat_int8.tflite\")\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "predictions = np.zeros((len(x_test),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(len(x_test)):\n",
    "    val_batch = x_test[i]\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()\n",
    "    \n",
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 10000\n",
    "print(\"Accuracy of quantized to int8 model is {}%\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd7bbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nstatic const unsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'static const unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data) :\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1bd3277",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_model_name = 'cifar10_ptq'\n",
    "# check if dir 'cfiles' exists, if not create it\n",
    "if not os.path.exists('cfiles'):\n",
    "    os.makedirs('cfiles')\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a8abea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c_model_name = 'cifar10_qat'\n",
    "# check if dir 'cfiles' exists, if not create it\n",
    "if not os.path.exists('cfiles'):\n",
    "    os.makedirs('cfiles')\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model_quant_int8_qat, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "798fd655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_28 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPoolin  (None, 16, 16, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_30 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPoolin  (None, 8, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_31 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_32 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 2, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 344,714\n",
      "Trainable params: 344,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.2598 - accuracy: 0.1511 - val_loss: 2.1110 - val_accuracy: 0.2282\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.0578 - accuracy: 0.2226 - val_loss: 1.9092 - val_accuracy: 0.3316\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.9138 - accuracy: 0.2729 - val_loss: 1.7877 - val_accuracy: 0.3510\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.8152 - accuracy: 0.3069 - val_loss: 1.6665 - val_accuracy: 0.3960\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.7507 - accuracy: 0.3385 - val_loss: 1.6072 - val_accuracy: 0.4071\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6982 - accuracy: 0.3596 - val_loss: 1.5637 - val_accuracy: 0.4317\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6518 - accuracy: 0.3744 - val_loss: 1.5221 - val_accuracy: 0.4471\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.6065 - accuracy: 0.3944 - val_loss: 1.4590 - val_accuracy: 0.4630\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5710 - accuracy: 0.4141 - val_loss: 1.4190 - val_accuracy: 0.4791\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5346 - accuracy: 0.4275 - val_loss: 1.3790 - val_accuracy: 0.4967\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.5033 - accuracy: 0.4431 - val_loss: 1.3434 - val_accuracy: 0.5175\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.4702 - accuracy: 0.4562 - val_loss: 1.3269 - val_accuracy: 0.5163\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.4364 - accuracy: 0.4678 - val_loss: 1.2871 - val_accuracy: 0.5355\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.4155 - accuracy: 0.4802 - val_loss: 1.2644 - val_accuracy: 0.5459\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.3890 - accuracy: 0.4931 - val_loss: 1.2214 - val_accuracy: 0.5593\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.3601 - accuracy: 0.5031 - val_loss: 1.2650 - val_accuracy: 0.5314\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.3338 - accuracy: 0.5153 - val_loss: 1.2105 - val_accuracy: 0.5562\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.3028 - accuracy: 0.5233 - val_loss: 1.1478 - val_accuracy: 0.5889\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.2872 - accuracy: 0.5323 - val_loss: 1.1509 - val_accuracy: 0.5900\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.2591 - accuracy: 0.5428 - val_loss: 1.1259 - val_accuracy: 0.5923\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.2398 - accuracy: 0.5480 - val_loss: 1.0950 - val_accuracy: 0.6050\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.2256 - accuracy: 0.5564 - val_loss: 1.1036 - val_accuracy: 0.5988\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1949 - accuracy: 0.5698 - val_loss: 1.0839 - val_accuracy: 0.6082\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1826 - accuracy: 0.5775 - val_loss: 1.0665 - val_accuracy: 0.6174\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1668 - accuracy: 0.5784 - val_loss: 1.0144 - val_accuracy: 0.6431\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1485 - accuracy: 0.5868 - val_loss: 0.9927 - val_accuracy: 0.6388\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1313 - accuracy: 0.5955 - val_loss: 0.9643 - val_accuracy: 0.6559\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1180 - accuracy: 0.5991 - val_loss: 0.9819 - val_accuracy: 0.6472\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.1051 - accuracy: 0.6072 - val_loss: 0.9437 - val_accuracy: 0.6589\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0905 - accuracy: 0.6121 - val_loss: 0.9652 - val_accuracy: 0.6539\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0755 - accuracy: 0.6164 - val_loss: 0.9965 - val_accuracy: 0.6429\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0664 - accuracy: 0.6206 - val_loss: 0.9296 - val_accuracy: 0.6643\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0529 - accuracy: 0.6249 - val_loss: 0.9075 - val_accuracy: 0.6758\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0402 - accuracy: 0.6320 - val_loss: 1.0071 - val_accuracy: 0.6430\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0315 - accuracy: 0.6341 - val_loss: 0.9014 - val_accuracy: 0.6788\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0180 - accuracy: 0.6380 - val_loss: 0.8876 - val_accuracy: 0.6837\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 1.0073 - accuracy: 0.6434 - val_loss: 0.8588 - val_accuracy: 0.6946\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9990 - accuracy: 0.6468 - val_loss: 0.8671 - val_accuracy: 0.6882\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9862 - accuracy: 0.6531 - val_loss: 0.8635 - val_accuracy: 0.6907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9831 - accuracy: 0.6514 - val_loss: 0.8508 - val_accuracy: 0.6974\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9706 - accuracy: 0.6586 - val_loss: 0.8727 - val_accuracy: 0.6862\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9620 - accuracy: 0.6616 - val_loss: 0.8951 - val_accuracy: 0.6815\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9530 - accuracy: 0.6637 - val_loss: 0.8391 - val_accuracy: 0.7012\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9463 - accuracy: 0.6676 - val_loss: 0.7983 - val_accuracy: 0.7199\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9361 - accuracy: 0.6692 - val_loss: 0.8355 - val_accuracy: 0.7019\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9313 - accuracy: 0.6715 - val_loss: 0.8037 - val_accuracy: 0.7122\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9199 - accuracy: 0.6768 - val_loss: 0.8478 - val_accuracy: 0.6977\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9173 - accuracy: 0.6802 - val_loss: 0.8326 - val_accuracy: 0.7042\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9041 - accuracy: 0.6826 - val_loss: 0.8509 - val_accuracy: 0.6990\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.9049 - accuracy: 0.6853 - val_loss: 0.8143 - val_accuracy: 0.7127\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8887 - accuracy: 0.6881 - val_loss: 0.7627 - val_accuracy: 0.7312\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8855 - accuracy: 0.6893 - val_loss: 0.7462 - val_accuracy: 0.7353\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8778 - accuracy: 0.6938 - val_loss: 0.8197 - val_accuracy: 0.7102\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8763 - accuracy: 0.6932 - val_loss: 0.7789 - val_accuracy: 0.7241\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8697 - accuracy: 0.6992 - val_loss: 0.7420 - val_accuracy: 0.7400\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8621 - accuracy: 0.6975 - val_loss: 0.7597 - val_accuracy: 0.7300\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8523 - accuracy: 0.7031 - val_loss: 0.7378 - val_accuracy: 0.7423\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8515 - accuracy: 0.7022 - val_loss: 0.7504 - val_accuracy: 0.7348\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8419 - accuracy: 0.7065 - val_loss: 0.7191 - val_accuracy: 0.7473\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8330 - accuracy: 0.7084 - val_loss: 0.7235 - val_accuracy: 0.7480\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8295 - accuracy: 0.7101 - val_loss: 0.7493 - val_accuracy: 0.7362\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8226 - accuracy: 0.7121 - val_loss: 0.7367 - val_accuracy: 0.7411\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8220 - accuracy: 0.7152 - val_loss: 0.7281 - val_accuracy: 0.7464\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8116 - accuracy: 0.7170 - val_loss: 0.6979 - val_accuracy: 0.7571\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8006 - accuracy: 0.7215 - val_loss: 0.7148 - val_accuracy: 0.7470\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.8011 - accuracy: 0.7215 - val_loss: 0.7531 - val_accuracy: 0.7380\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7938 - accuracy: 0.7236 - val_loss: 0.7003 - val_accuracy: 0.7568\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7905 - accuracy: 0.7246 - val_loss: 0.7135 - val_accuracy: 0.7495\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7858 - accuracy: 0.7271 - val_loss: 0.7096 - val_accuracy: 0.7531\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7795 - accuracy: 0.7291 - val_loss: 0.6884 - val_accuracy: 0.7586\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7764 - accuracy: 0.7309 - val_loss: 0.7305 - val_accuracy: 0.7443\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7675 - accuracy: 0.7340 - val_loss: 0.6894 - val_accuracy: 0.7593\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7653 - accuracy: 0.7349 - val_loss: 0.6701 - val_accuracy: 0.7690\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7652 - accuracy: 0.7341 - val_loss: 0.6823 - val_accuracy: 0.7620\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7599 - accuracy: 0.7383 - val_loss: 0.6820 - val_accuracy: 0.7609\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7577 - accuracy: 0.7357 - val_loss: 0.6575 - val_accuracy: 0.7754\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7466 - accuracy: 0.7406 - val_loss: 0.7067 - val_accuracy: 0.7560\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7427 - accuracy: 0.7432 - val_loss: 0.6924 - val_accuracy: 0.7619\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7368 - accuracy: 0.7454 - val_loss: 0.6666 - val_accuracy: 0.7689\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7305 - accuracy: 0.7473 - val_loss: 0.6487 - val_accuracy: 0.7745\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7308 - accuracy: 0.7465 - val_loss: 0.6556 - val_accuracy: 0.7777\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7300 - accuracy: 0.7474 - val_loss: 0.7013 - val_accuracy: 0.7606\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7247 - accuracy: 0.7492 - val_loss: 0.6352 - val_accuracy: 0.7834\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7163 - accuracy: 0.7514 - val_loss: 0.6557 - val_accuracy: 0.7750\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7172 - accuracy: 0.7510 - val_loss: 0.6640 - val_accuracy: 0.7732\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7144 - accuracy: 0.7542 - val_loss: 0.6495 - val_accuracy: 0.7768\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7083 - accuracy: 0.7559 - val_loss: 0.6311 - val_accuracy: 0.7851\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.7039 - accuracy: 0.7561 - val_loss: 0.6319 - val_accuracy: 0.7800\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6980 - accuracy: 0.7579 - val_loss: 0.6346 - val_accuracy: 0.7817\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6955 - accuracy: 0.7592 - val_loss: 0.6262 - val_accuracy: 0.7811\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6942 - accuracy: 0.7607 - val_loss: 0.6412 - val_accuracy: 0.7793\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6874 - accuracy: 0.7637 - val_loss: 0.6456 - val_accuracy: 0.7806\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6834 - accuracy: 0.7640 - val_loss: 0.6362 - val_accuracy: 0.7834\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6776 - accuracy: 0.7646 - val_loss: 0.6335 - val_accuracy: 0.7821\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6755 - accuracy: 0.7668 - val_loss: 0.6260 - val_accuracy: 0.7877\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6649 - accuracy: 0.7721 - val_loss: 0.6462 - val_accuracy: 0.7816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6744 - accuracy: 0.7678 - val_loss: 0.5881 - val_accuracy: 0.7992\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6699 - accuracy: 0.7691 - val_loss: 0.6171 - val_accuracy: 0.7860\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6609 - accuracy: 0.7699 - val_loss: 0.6067 - val_accuracy: 0.7961\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 0.6599 - accuracy: 0.7719 - val_loss: 0.6264 - val_accuracy: 0.7876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84a9d164c0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', \n",
    "                    input_shape=(32, 32, 3)))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.2))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.3))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((4, 4)))\n",
    "model_10.add(Dropout(0.4))\n",
    "model_10.add(Flatten())\n",
    "model_10.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model_10.add(Dropout(0.5))\n",
    "model_10.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_10.summary()\n",
    "\n",
    "# define the ModelCheckpoint callback to save the model after each epoch\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_10_gpu_Base-CubeAI_reduced.h5', save_freq='epoch')\n",
    "\n",
    "# compile model\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "model_10.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Model Fit\n",
    "model_10.fit(x_train,y_train,batch_size=64, epochs=100, validation_data=(x_test, y_test),callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "755e23e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.6264 - accuracy: 0.7876\n",
      "Test accuracy: 0.7875999808311462\n"
     ]
    }
   ],
   "source": [
    "# load the pre-saved model\n",
    "model_10 = load_model('model_10_gpu_Base-CubeAI_reduced.h5')\n",
    "\n",
    "# test the accuracy of the model on the test set\n",
    "loss, accuracy = model_10.evaluate(x_test, y_test)\n",
    "\n",
    "# print the accuracy of the model on the test set\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27eb02ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpujqp3jr7/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpujqp3jr7/assets\n",
      "/home/themandalorian/anaconda3/envs/ml-on-mc/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-05-17 16:26:17.724843: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-05-17 16:26:17.724866: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-05-17 16:26:17.725006: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpujqp3jr7\n",
      "2023-05-17 16:26:17.727111: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-17 16:26:17.727125: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpujqp3jr7\n",
      "2023-05-17 16:26:17.732583: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-05-17 16:26:17.771668: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpujqp3jr7\n",
      "2023-05-17 16:26:17.781594: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 56589 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_conv2d_28_input:0\n",
      "shape: [ 1 32 32  3]\n",
      "type: <class 'numpy.int8'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.int8'>\n",
      "Accuracy of reduced base quantized to int8 model is 78.79%\n"
     ]
    }
   ],
   "source": [
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_10)\n",
    "\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "tflite_model_name = 'model_10_gpu_Base-CubeAI_reduced-PTQ'\n",
    "        \n",
    "# Set the optimization flag.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Enforce full-int8 quantization\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "# Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name + '.tflite')\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "predictions = np.zeros((len(x_test),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(len(x_test)):\n",
    "    val_batch = x_test[i]\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()\n",
    "    \n",
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 10000\n",
    "print(\"Accuracy of reduced base quantized to int8 model is {}%\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da1a4554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_3 (QuantizeL  (None, 32, 32, 3)        3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_conv2d_33 (QuantizeWr  (None, 32, 32, 32)       963       \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_18 (Qua  (None, 16, 16, 32)       1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_dropout_24 (QuantizeW  (None, 16, 16, 32)       1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_conv2d_34 (QuantizeWr  (None, 16, 16, 64)       18627     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_35 (QuantizeWr  (None, 16, 16, 64)       37059     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_19 (Qua  (None, 8, 8, 64)         1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_dropout_25 (QuantizeW  (None, 8, 8, 64)         1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_conv2d_36 (QuantizeWr  (None, 8, 8, 128)        74115     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_37 (QuantizeWr  (None, 8, 8, 128)        147843    \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_20 (Qua  (None, 2, 2, 128)        1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_dropout_26 (QuantizeW  (None, 2, 2, 128)        1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_flatten_6 (QuantizeWr  (None, 512)              1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_12 (QuantizeWra  (None, 128)              65669     \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dropout_27 (QuantizeW  (None, 128)              1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_13 (QuantizeWra  (None, 10)               1295      \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 345,582\n",
      "Trainable params: 344,714\n",
      "Non-trainable params: 868\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 2.2396 - accuracy: 0.1598 - val_loss: 2.0537 - val_accuracy: 0.2488\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 2.0167 - accuracy: 0.2314 - val_loss: 1.8794 - val_accuracy: 0.3202\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.8773 - accuracy: 0.2823 - val_loss: 1.7389 - val_accuracy: 0.3512\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.7859 - accuracy: 0.3179 - val_loss: 1.6642 - val_accuracy: 0.3924\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.7160 - accuracy: 0.3491 - val_loss: 1.5948 - val_accuracy: 0.4189\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.6560 - accuracy: 0.3751 - val_loss: 1.5170 - val_accuracy: 0.4449\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.5999 - accuracy: 0.4024 - val_loss: 1.4618 - val_accuracy: 0.4763\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.5587 - accuracy: 0.4218 - val_loss: 1.4157 - val_accuracy: 0.4897\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 11s 15ms/step - loss: 1.5228 - accuracy: 0.4357 - val_loss: 1.4123 - val_accuracy: 0.4871\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.4858 - accuracy: 0.4494 - val_loss: 1.3376 - val_accuracy: 0.5159\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.4584 - accuracy: 0.4607 - val_loss: 1.3402 - val_accuracy: 0.5069\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.4330 - accuracy: 0.4744 - val_loss: 1.2701 - val_accuracy: 0.5440\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.4013 - accuracy: 0.4866 - val_loss: 1.2638 - val_accuracy: 0.5396\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.3778 - accuracy: 0.4929 - val_loss: 1.3167 - val_accuracy: 0.5232\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.3563 - accuracy: 0.5041 - val_loss: 1.2021 - val_accuracy: 0.5660\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.3364 - accuracy: 0.5117 - val_loss: 1.2581 - val_accuracy: 0.5556\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.3137 - accuracy: 0.5227 - val_loss: 1.1673 - val_accuracy: 0.5784\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.2897 - accuracy: 0.5322 - val_loss: 1.1825 - val_accuracy: 0.5753\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.2686 - accuracy: 0.5423 - val_loss: 1.1688 - val_accuracy: 0.5848\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.2490 - accuracy: 0.5470 - val_loss: 1.1291 - val_accuracy: 0.5984\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.2336 - accuracy: 0.5525 - val_loss: 1.0934 - val_accuracy: 0.6068\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.2120 - accuracy: 0.5622 - val_loss: 1.0519 - val_accuracy: 0.6213\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.1970 - accuracy: 0.5692 - val_loss: 1.0364 - val_accuracy: 0.6232\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 11s 15ms/step - loss: 1.1785 - accuracy: 0.5758 - val_loss: 1.0504 - val_accuracy: 0.6198\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.1598 - accuracy: 0.5825 - val_loss: 1.0250 - val_accuracy: 0.6331\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.1450 - accuracy: 0.5899 - val_loss: 0.9964 - val_accuracy: 0.6436\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.1270 - accuracy: 0.5972 - val_loss: 0.9734 - val_accuracy: 0.6509\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.1144 - accuracy: 0.6022 - val_loss: 0.9589 - val_accuracy: 0.6616\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.1033 - accuracy: 0.6083 - val_loss: 0.9797 - val_accuracy: 0.6545\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.0868 - accuracy: 0.6144 - val_loss: 1.0203 - val_accuracy: 0.6387\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 11s 15ms/step - loss: 1.0745 - accuracy: 0.6171 - val_loss: 1.0065 - val_accuracy: 0.6487\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.0646 - accuracy: 0.6200 - val_loss: 0.9429 - val_accuracy: 0.6629\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.0465 - accuracy: 0.6307 - val_loss: 0.9413 - val_accuracy: 0.6697\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.0368 - accuracy: 0.6334 - val_loss: 0.8756 - val_accuracy: 0.6929\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.0295 - accuracy: 0.6355 - val_loss: 0.9072 - val_accuracy: 0.6806\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.0128 - accuracy: 0.6451 - val_loss: 0.9187 - val_accuracy: 0.6730\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 1.0066 - accuracy: 0.6447 - val_loss: 0.8668 - val_accuracy: 0.7000\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.9915 - accuracy: 0.6527 - val_loss: 0.8929 - val_accuracy: 0.6876\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.9850 - accuracy: 0.6532 - val_loss: 0.8536 - val_accuracy: 0.7014\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.9759 - accuracy: 0.6560 - val_loss: 0.8854 - val_accuracy: 0.6861\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.9644 - accuracy: 0.6621 - val_loss: 0.8419 - val_accuracy: 0.7022\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.9651 - accuracy: 0.6624 - val_loss: 0.8284 - val_accuracy: 0.7103\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.9568 - accuracy: 0.6648 - val_loss: 0.8159 - val_accuracy: 0.7147\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 11s 15ms/step - loss: 0.9456 - accuracy: 0.6686 - val_loss: 0.8858 - val_accuracy: 0.6885\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.9367 - accuracy: 0.6704 - val_loss: 0.8319 - val_accuracy: 0.7083\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.9278 - accuracy: 0.6740 - val_loss: 0.8497 - val_accuracy: 0.6990\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.9176 - accuracy: 0.6770 - val_loss: 0.8489 - val_accuracy: 0.7029\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.9113 - accuracy: 0.6809 - val_loss: 0.7946 - val_accuracy: 0.7222\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.9062 - accuracy: 0.6817 - val_loss: 0.7896 - val_accuracy: 0.7216\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8993 - accuracy: 0.6851 - val_loss: 0.8334 - val_accuracy: 0.7038\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8877 - accuracy: 0.6902 - val_loss: 0.7574 - val_accuracy: 0.7364\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8865 - accuracy: 0.6922 - val_loss: 0.7606 - val_accuracy: 0.7309\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8780 - accuracy: 0.6931 - val_loss: 0.7685 - val_accuracy: 0.7316\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8733 - accuracy: 0.6931 - val_loss: 0.8366 - val_accuracy: 0.7077\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8614 - accuracy: 0.6990 - val_loss: 0.7461 - val_accuracy: 0.7409\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8623 - accuracy: 0.7001 - val_loss: 0.7709 - val_accuracy: 0.7309\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8540 - accuracy: 0.7023 - val_loss: 0.7445 - val_accuracy: 0.7422\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8459 - accuracy: 0.7043 - val_loss: 0.7464 - val_accuracy: 0.7372\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8423 - accuracy: 0.7047 - val_loss: 0.7627 - val_accuracy: 0.7344\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8359 - accuracy: 0.7067 - val_loss: 0.7484 - val_accuracy: 0.7405\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8325 - accuracy: 0.7098 - val_loss: 0.7319 - val_accuracy: 0.7480\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 11s 15ms/step - loss: 0.8270 - accuracy: 0.7114 - val_loss: 0.7095 - val_accuracy: 0.7523\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8267 - accuracy: 0.7115 - val_loss: 0.7139 - val_accuracy: 0.7512\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8183 - accuracy: 0.7161 - val_loss: 0.7134 - val_accuracy: 0.7525\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8086 - accuracy: 0.7184 - val_loss: 0.7551 - val_accuracy: 0.7396\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8041 - accuracy: 0.7189 - val_loss: 0.7155 - val_accuracy: 0.7504\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8007 - accuracy: 0.7224 - val_loss: 0.7214 - val_accuracy: 0.7492\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7949 - accuracy: 0.7237 - val_loss: 0.7292 - val_accuracy: 0.7497\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7929 - accuracy: 0.7247 - val_loss: 0.7021 - val_accuracy: 0.7552\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7860 - accuracy: 0.7252 - val_loss: 0.7218 - val_accuracy: 0.7516\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7804 - accuracy: 0.7289 - val_loss: 0.6917 - val_accuracy: 0.7580\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7680 - accuracy: 0.7299 - val_loss: 0.7075 - val_accuracy: 0.7540\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7712 - accuracy: 0.7303 - val_loss: 0.6827 - val_accuracy: 0.7616\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7672 - accuracy: 0.7335 - val_loss: 0.6716 - val_accuracy: 0.7700\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7583 - accuracy: 0.7374 - val_loss: 0.6780 - val_accuracy: 0.7642\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7588 - accuracy: 0.7367 - val_loss: 0.6793 - val_accuracy: 0.7637\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7532 - accuracy: 0.7358 - val_loss: 0.6766 - val_accuracy: 0.7655\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7437 - accuracy: 0.7420 - val_loss: 0.6540 - val_accuracy: 0.7729\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7456 - accuracy: 0.7410 - val_loss: 0.7030 - val_accuracy: 0.7542\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7369 - accuracy: 0.7466 - val_loss: 0.6705 - val_accuracy: 0.7682\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7344 - accuracy: 0.7440 - val_loss: 0.6722 - val_accuracy: 0.7697\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7292 - accuracy: 0.7467 - val_loss: 0.6481 - val_accuracy: 0.7752\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7291 - accuracy: 0.7477 - val_loss: 0.6964 - val_accuracy: 0.7616\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 11s 15ms/step - loss: 0.7252 - accuracy: 0.7478 - val_loss: 0.6561 - val_accuracy: 0.7737\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 12s 15ms/step - loss: 0.7187 - accuracy: 0.7499 - val_loss: 0.6531 - val_accuracy: 0.7731\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7152 - accuracy: 0.7510 - val_loss: 0.6549 - val_accuracy: 0.7720\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7088 - accuracy: 0.7547 - val_loss: 0.6524 - val_accuracy: 0.7760\n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7059 - accuracy: 0.7554 - val_loss: 0.6528 - val_accuracy: 0.7716\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.7052 - accuracy: 0.7544 - val_loss: 0.6227 - val_accuracy: 0.7843\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.6977 - accuracy: 0.7593 - val_loss: 0.6353 - val_accuracy: 0.7807\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.6954 - accuracy: 0.7595 - val_loss: 0.6187 - val_accuracy: 0.7841\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.6941 - accuracy: 0.7611 - val_loss: 0.6153 - val_accuracy: 0.7853\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.6918 - accuracy: 0.7613 - val_loss: 0.6347 - val_accuracy: 0.7773\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 11s 15ms/step - loss: 0.6896 - accuracy: 0.7618 - val_loss: 0.6413 - val_accuracy: 0.7763\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.6800 - accuracy: 0.7647 - val_loss: 0.6380 - val_accuracy: 0.7777\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.6816 - accuracy: 0.7637 - val_loss: 0.6255 - val_accuracy: 0.7871\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.6719 - accuracy: 0.7673 - val_loss: 0.6321 - val_accuracy: 0.7840\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.6703 - accuracy: 0.7665 - val_loss: 0.6084 - val_accuracy: 0.7910\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.6693 - accuracy: 0.7706 - val_loss: 0.6287 - val_accuracy: 0.7853\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.6638 - accuracy: 0.7702 - val_loss: 0.6120 - val_accuracy: 0.7871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84ae2c3160>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', \n",
    "                    input_shape=(32, 32, 3)))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.2))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.3))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((4, 4)))\n",
    "model_10.add(Dropout(0.4))\n",
    "model_10.add(Flatten())\n",
    "model_10.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model_10.add(Dropout(0.5))\n",
    "model_10.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Convert the model to a quantization aware model\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(model_10)\n",
    "\n",
    "quant_aware_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# define the ModelCheckpoint callback to save the model after each epoch\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_10_gpu_Base-CubeAI_reduced_QAT.h5', save_freq='epoch')\n",
    "\n",
    "quant_aware_model.summary()\n",
    "\n",
    "# Train and evaluate the quantization aware model\n",
    "quant_aware_model.fit(x_train,y_train, batch_size=64,epochs=100,validation_data=(x_test, y_test),\n",
    "                     callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb20dc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization aware training loss:  0.6596236228942871\n",
      "Quantization aware training accuracy:  0.7692000269889832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, dropout_32_layer_call_fn, dropout_32_layer_call_and_return_conditional_losses while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpmeh1heq3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpmeh1heq3/assets\n",
      "/home/themandalorian/anaconda3/envs/ml-on-mc/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-05-17 17:13:42.017819: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-05-17 17:13:42.017853: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-05-17 17:13:42.017993: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpmeh1heq3\n",
      "2023-05-17 17:13:42.022470: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-17 17:13:42.022489: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpmeh1heq3\n",
      "2023-05-17 17:13:42.035169: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-05-17 17:13:42.105851: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpmeh1heq3\n",
      "2023-05-17 17:13:42.124124: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 106130 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_conv2d_42_input:0\n",
      "shape: [ 1 32 32  3]\n",
      "type: <class 'numpy.uint8'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.uint8'>\n",
      "Accuracy of quantized to int8 model is 76.85%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "quant_loss, quant_acc = quant_aware_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Quantization aware training loss: ', quant_loss)\n",
    "print('Quantization aware training accuracy: ', quant_acc)\n",
    "\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant_int8_qat = converter.convert()\n",
    "\n",
    "open(\"model_10_gpu_Base-CubeAI_reduced_qat_int8.tflite\", \"wb\").write(tflite_model_quant_int8_qat)\n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(\"model_10_gpu_Base-CubeAI_reduced_qat_int8.tflite\")\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "predictions = np.zeros((len(x_test),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(len(x_test)):\n",
    "    val_batch = x_test[i]\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()\n",
    "    \n",
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 10000\n",
    "print(\"Accuracy of quantized to int8 model is {}%\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "406dcfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'cifar10_base_reduced_ptq'\n",
    "# check if dir 'cfiles' exists, if not create it\n",
    "if not os.path.exists('cfiles'):\n",
    "    os.makedirs('cfiles')\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "feb47ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'cifar10_reduced_qat'\n",
    "# check if dir 'cfiles' exists, if not create it\n",
    "if not os.path.exists('cfiles'):\n",
    "    os.makedirs('cfiles')\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model_quant_int8_qat, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30ff15be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_38 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_21 (MaxPoolin  (None, 16, 16, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_39 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_22 (MaxPoolin  (None, 8, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_40 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " conv2d_41 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_23 (MaxPoolin  (None, 2, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 307,786\n",
      "Trainable params: 307,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/themandalorian/anaconda3/envs/ml-on-mc/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "782/782 [==============================] - 4s 5ms/step - loss: 2.3276 - accuracy: 0.1224 - val_loss: 2.1935 - val_accuracy: 0.1866\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.1356 - accuracy: 0.1936 - val_loss: 2.0472 - val_accuracy: 0.2585\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.0183 - accuracy: 0.2382 - val_loss: 1.9070 - val_accuracy: 0.2970\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.8897 - accuracy: 0.2807 - val_loss: 1.7688 - val_accuracy: 0.3588\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.8004 - accuracy: 0.3170 - val_loss: 1.7162 - val_accuracy: 0.3806\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.7315 - accuracy: 0.3484 - val_loss: 1.6685 - val_accuracy: 0.3935\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6803 - accuracy: 0.3687 - val_loss: 1.5466 - val_accuracy: 0.4393\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6404 - accuracy: 0.3866 - val_loss: 1.5239 - val_accuracy: 0.4506\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6007 - accuracy: 0.4058 - val_loss: 1.4833 - val_accuracy: 0.4679\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5612 - accuracy: 0.4218 - val_loss: 1.4425 - val_accuracy: 0.4842\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5332 - accuracy: 0.4320 - val_loss: 1.3947 - val_accuracy: 0.4938\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4995 - accuracy: 0.4490 - val_loss: 1.3554 - val_accuracy: 0.5085\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4823 - accuracy: 0.4562 - val_loss: 1.3754 - val_accuracy: 0.5027\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4527 - accuracy: 0.4692 - val_loss: 1.3020 - val_accuracy: 0.5249\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4292 - accuracy: 0.4784 - val_loss: 1.2841 - val_accuracy: 0.5366\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4106 - accuracy: 0.4855 - val_loss: 1.2535 - val_accuracy: 0.5446\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3935 - accuracy: 0.4897 - val_loss: 1.2417 - val_accuracy: 0.5471\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3741 - accuracy: 0.4986 - val_loss: 1.2152 - val_accuracy: 0.5524\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3552 - accuracy: 0.5066 - val_loss: 1.2235 - val_accuracy: 0.5571\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3367 - accuracy: 0.5151 - val_loss: 1.2165 - val_accuracy: 0.5608\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3116 - accuracy: 0.5257 - val_loss: 1.1791 - val_accuracy: 0.5708\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2995 - accuracy: 0.5296 - val_loss: 1.1836 - val_accuracy: 0.5692\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2840 - accuracy: 0.5362 - val_loss: 1.1818 - val_accuracy: 0.5756\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2663 - accuracy: 0.5452 - val_loss: 1.1993 - val_accuracy: 0.5598\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2521 - accuracy: 0.5493 - val_loss: 1.1424 - val_accuracy: 0.5800\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2402 - accuracy: 0.5559 - val_loss: 1.1132 - val_accuracy: 0.5962\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2198 - accuracy: 0.5630 - val_loss: 1.0775 - val_accuracy: 0.6161\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2093 - accuracy: 0.5655 - val_loss: 1.0727 - val_accuracy: 0.6182\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1935 - accuracy: 0.5731 - val_loss: 1.0529 - val_accuracy: 0.6252\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1827 - accuracy: 0.5733 - val_loss: 1.0937 - val_accuracy: 0.6077\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1668 - accuracy: 0.5816 - val_loss: 1.0401 - val_accuracy: 0.6235\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1568 - accuracy: 0.5909 - val_loss: 1.0666 - val_accuracy: 0.6194\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1446 - accuracy: 0.5916 - val_loss: 1.0910 - val_accuracy: 0.6033\n",
      "Epoch 34/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1384 - accuracy: 0.5949 - val_loss: 1.0344 - val_accuracy: 0.6291\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1228 - accuracy: 0.6005 - val_loss: 1.0141 - val_accuracy: 0.6381\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1202 - accuracy: 0.6027 - val_loss: 1.0840 - val_accuracy: 0.6163\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1052 - accuracy: 0.6050 - val_loss: 1.0025 - val_accuracy: 0.6474\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0920 - accuracy: 0.6132 - val_loss: 0.9676 - val_accuracy: 0.6508\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0823 - accuracy: 0.6197 - val_loss: 0.9558 - val_accuracy: 0.6572\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0708 - accuracy: 0.6208 - val_loss: 0.9652 - val_accuracy: 0.6572\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0630 - accuracy: 0.6246 - val_loss: 0.9919 - val_accuracy: 0.6410\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0497 - accuracy: 0.6296 - val_loss: 1.0518 - val_accuracy: 0.6254\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0440 - accuracy: 0.6314 - val_loss: 0.9819 - val_accuracy: 0.6458\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0319 - accuracy: 0.6353 - val_loss: 0.9293 - val_accuracy: 0.6686\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0271 - accuracy: 0.6377 - val_loss: 0.9596 - val_accuracy: 0.6546\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0174 - accuracy: 0.6399 - val_loss: 0.9602 - val_accuracy: 0.6584\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0141 - accuracy: 0.6403 - val_loss: 0.9041 - val_accuracy: 0.6774\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9980 - accuracy: 0.6503 - val_loss: 0.8719 - val_accuracy: 0.6917\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9897 - accuracy: 0.6505 - val_loss: 0.9020 - val_accuracy: 0.6840\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9842 - accuracy: 0.6520 - val_loss: 0.8816 - val_accuracy: 0.6888\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9821 - accuracy: 0.6527 - val_loss: 0.9014 - val_accuracy: 0.6739\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9700 - accuracy: 0.6598 - val_loss: 0.8937 - val_accuracy: 0.6809\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9616 - accuracy: 0.6607 - val_loss: 0.8749 - val_accuracy: 0.6871\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9589 - accuracy: 0.6629 - val_loss: 0.8534 - val_accuracy: 0.6974\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9515 - accuracy: 0.6662 - val_loss: 0.8530 - val_accuracy: 0.6970\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9407 - accuracy: 0.6698 - val_loss: 0.8802 - val_accuracy: 0.6911\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9353 - accuracy: 0.6721 - val_loss: 0.8259 - val_accuracy: 0.7100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9336 - accuracy: 0.6728 - val_loss: 0.8105 - val_accuracy: 0.7146\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9252 - accuracy: 0.6785 - val_loss: 0.8076 - val_accuracy: 0.7160\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9157 - accuracy: 0.6781 - val_loss: 0.9021 - val_accuracy: 0.6864\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9127 - accuracy: 0.6816 - val_loss: 0.9223 - val_accuracy: 0.6768\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9069 - accuracy: 0.6833 - val_loss: 0.7912 - val_accuracy: 0.7223\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8983 - accuracy: 0.6863 - val_loss: 0.7956 - val_accuracy: 0.7188\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8950 - accuracy: 0.6858 - val_loss: 0.7905 - val_accuracy: 0.7207\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8892 - accuracy: 0.6898 - val_loss: 0.7857 - val_accuracy: 0.7210\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8850 - accuracy: 0.6913 - val_loss: 0.8215 - val_accuracy: 0.7133\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8793 - accuracy: 0.6945 - val_loss: 0.7946 - val_accuracy: 0.7207\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8773 - accuracy: 0.6942 - val_loss: 0.7890 - val_accuracy: 0.7218\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8694 - accuracy: 0.6976 - val_loss: 0.7768 - val_accuracy: 0.7274\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8608 - accuracy: 0.6993 - val_loss: 0.7319 - val_accuracy: 0.7459\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8556 - accuracy: 0.7016 - val_loss: 0.7444 - val_accuracy: 0.7407\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8549 - accuracy: 0.7028 - val_loss: 0.7654 - val_accuracy: 0.7341\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8467 - accuracy: 0.7049 - val_loss: 0.7594 - val_accuracy: 0.7356\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8448 - accuracy: 0.7057 - val_loss: 0.8014 - val_accuracy: 0.7193\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8376 - accuracy: 0.7085 - val_loss: 0.7247 - val_accuracy: 0.7490\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8362 - accuracy: 0.7081 - val_loss: 0.7237 - val_accuracy: 0.7473\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8265 - accuracy: 0.7151 - val_loss: 0.7095 - val_accuracy: 0.7527\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8206 - accuracy: 0.7166 - val_loss: 0.7507 - val_accuracy: 0.7399\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8123 - accuracy: 0.7185 - val_loss: 0.7272 - val_accuracy: 0.7473\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8123 - accuracy: 0.7179 - val_loss: 0.7221 - val_accuracy: 0.7508\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8095 - accuracy: 0.7187 - val_loss: 0.7067 - val_accuracy: 0.7511\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8044 - accuracy: 0.7211 - val_loss: 0.7415 - val_accuracy: 0.7435\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8017 - accuracy: 0.7216 - val_loss: 0.6927 - val_accuracy: 0.7582\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7936 - accuracy: 0.7256 - val_loss: 0.6912 - val_accuracy: 0.7600\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7929 - accuracy: 0.7264 - val_loss: 0.6963 - val_accuracy: 0.7546\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7904 - accuracy: 0.7264 - val_loss: 0.6962 - val_accuracy: 0.7561\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7840 - accuracy: 0.7280 - val_loss: 0.6995 - val_accuracy: 0.7546\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7826 - accuracy: 0.7291 - val_loss: 0.6806 - val_accuracy: 0.7651\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7748 - accuracy: 0.7319 - val_loss: 0.7775 - val_accuracy: 0.7262\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7707 - accuracy: 0.7327 - val_loss: 0.6990 - val_accuracy: 0.7562\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7690 - accuracy: 0.7319 - val_loss: 0.7041 - val_accuracy: 0.7530\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7596 - accuracy: 0.7373 - val_loss: 0.6888 - val_accuracy: 0.7598\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7618 - accuracy: 0.7378 - val_loss: 0.6650 - val_accuracy: 0.7674\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7597 - accuracy: 0.7371 - val_loss: 0.6648 - val_accuracy: 0.7701\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7599 - accuracy: 0.7393 - val_loss: 0.6806 - val_accuracy: 0.7627\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7469 - accuracy: 0.7388 - val_loss: 0.6877 - val_accuracy: 0.7623\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7529 - accuracy: 0.7395 - val_loss: 0.6481 - val_accuracy: 0.7770\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7421 - accuracy: 0.7433 - val_loss: 0.6535 - val_accuracy: 0.7720\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7437 - accuracy: 0.7415 - val_loss: 0.6988 - val_accuracy: 0.7583\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7342 - accuracy: 0.7450 - val_loss: 0.6462 - val_accuracy: 0.7749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f86800e18e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', \n",
    "                    input_shape=(32, 32, 3)))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.2))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.3))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((4, 4)))\n",
    "model_10.add(Dropout(0.4))\n",
    "model_10.add(Flatten())\n",
    "model_10.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model_10.add(Dropout(0.5))\n",
    "model_10.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_10.summary()\n",
    "\n",
    "# define the ModelCheckpoint callback to save the model after each epoch\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_10_gpu_Base-CubeAI_4Conv.h5', save_freq='epoch')\n",
    "\n",
    "# compile model\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "model_10.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Model Fit\n",
    "model_10.fit(x_train,y_train,batch_size=64, epochs=100, validation_data=(x_test, y_test),callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15cfe200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpuefmuyzg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpuefmuyzg/assets\n",
      "/home/themandalorian/anaconda3/envs/ml-on-mc/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-05-17 16:55:53.976100: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-05-17 16:55:53.976127: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-05-17 16:55:53.976269: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpuefmuyzg\n",
      "2023-05-17 16:55:53.978266: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-17 16:55:53.978284: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpuefmuyzg\n",
      "2023-05-17 16:55:53.983492: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-05-17 16:55:54.019385: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpuefmuyzg\n",
      "2023-05-17 16:55:54.028933: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 52662 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_conv2d_38_input:0\n",
      "shape: [ 1 32 32  3]\n",
      "type: <class 'numpy.int8'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.int8'>\n",
      "Accuracy of reduced base quantized to int8 model is 77.4%\n"
     ]
    }
   ],
   "source": [
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_10)\n",
    "\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "tflite_model_name = 'model_10_gpu_Base-CubeAI_4Conv-PTQ'\n",
    "        \n",
    "# Set the optimization flag.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Enforce full-int8 quantization\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "# Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name + '.tflite')\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "predictions = np.zeros((len(x_test),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(len(x_test)):\n",
    "    val_batch = x_test[i]\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()\n",
    "    \n",
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 10000\n",
    "print(\"Accuracy of reduced base quantized to int8 model is {}%\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dbcb1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_4 (QuantizeL  (None, 32, 32, 3)        3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_conv2d_42 (QuantizeWr  (None, 32, 32, 32)       963       \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_24 (Qua  (None, 16, 16, 32)       1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_dropout_32 (QuantizeW  (None, 16, 16, 32)       1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_conv2d_43 (QuantizeWr  (None, 16, 16, 64)       18627     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_25 (Qua  (None, 8, 8, 64)         1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_dropout_33 (QuantizeW  (None, 8, 8, 64)         1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_conv2d_44 (QuantizeWr  (None, 8, 8, 128)        74115     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_45 (QuantizeWr  (None, 8, 8, 128)        147843    \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_26 (Qua  (None, 2, 2, 128)        1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_dropout_34 (QuantizeW  (None, 2, 2, 128)        1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_flatten_8 (QuantizeWr  (None, 512)              1         \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_dense_16 (QuantizeWra  (None, 128)              65669     \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dropout_35 (QuantizeW  (None, 128)              1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_17 (QuantizeWra  (None, 10)               1295      \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 308,523\n",
      "Trainable params: 307,786\n",
      "Non-trainable params: 737\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "782/782 [==============================] - 11s 13ms/step - loss: 2.2973 - accuracy: 0.1457 - val_loss: 2.1565 - val_accuracy: 0.2010\n",
      "Epoch 2/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 2.0994 - accuracy: 0.2058 - val_loss: 2.0536 - val_accuracy: 0.2579\n",
      "Epoch 3/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.9905 - accuracy: 0.2511 - val_loss: 1.9007 - val_accuracy: 0.3324\n",
      "Epoch 4/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.8751 - accuracy: 0.2942 - val_loss: 1.7592 - val_accuracy: 0.3736\n",
      "Epoch 5/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.8041 - accuracy: 0.3188 - val_loss: 1.7691 - val_accuracy: 0.3660\n",
      "Epoch 6/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.7453 - accuracy: 0.3445 - val_loss: 1.6455 - val_accuracy: 0.4061\n",
      "Epoch 7/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.6944 - accuracy: 0.3648 - val_loss: 1.5601 - val_accuracy: 0.4362\n",
      "Epoch 8/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.6469 - accuracy: 0.3840 - val_loss: 1.5264 - val_accuracy: 0.4571\n",
      "Epoch 9/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.6122 - accuracy: 0.3978 - val_loss: 1.4925 - val_accuracy: 0.4645\n",
      "Epoch 10/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.5684 - accuracy: 0.4149 - val_loss: 1.4420 - val_accuracy: 0.4797\n",
      "Epoch 11/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.5458 - accuracy: 0.4289 - val_loss: 1.4030 - val_accuracy: 0.4937\n",
      "Epoch 12/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.5179 - accuracy: 0.4363 - val_loss: 1.3668 - val_accuracy: 0.5065\n",
      "Epoch 13/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.4908 - accuracy: 0.4521 - val_loss: 1.3674 - val_accuracy: 0.4969\n",
      "Epoch 14/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.4714 - accuracy: 0.4556 - val_loss: 1.3439 - val_accuracy: 0.5144\n",
      "Epoch 15/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.4451 - accuracy: 0.4695 - val_loss: 1.3019 - val_accuracy: 0.5243\n",
      "Epoch 16/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.4250 - accuracy: 0.4776 - val_loss: 1.2972 - val_accuracy: 0.5323\n",
      "Epoch 17/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.4057 - accuracy: 0.4844 - val_loss: 1.2686 - val_accuracy: 0.5388\n",
      "Epoch 18/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.3904 - accuracy: 0.4895 - val_loss: 1.2724 - val_accuracy: 0.5390\n",
      "Epoch 19/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.3713 - accuracy: 0.4997 - val_loss: 1.2862 - val_accuracy: 0.5300\n",
      "Epoch 20/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.3538 - accuracy: 0.5051 - val_loss: 1.2695 - val_accuracy: 0.5430\n",
      "Epoch 21/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.3371 - accuracy: 0.5155 - val_loss: 1.2447 - val_accuracy: 0.5517\n",
      "Epoch 22/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.3191 - accuracy: 0.5205 - val_loss: 1.2089 - val_accuracy: 0.5725\n",
      "Epoch 23/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.3087 - accuracy: 0.5268 - val_loss: 1.1589 - val_accuracy: 0.5797\n",
      "Epoch 24/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.2917 - accuracy: 0.5336 - val_loss: 1.1804 - val_accuracy: 0.5713\n",
      "Epoch 25/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.2722 - accuracy: 0.5394 - val_loss: 1.1455 - val_accuracy: 0.5981\n",
      "Epoch 26/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.2646 - accuracy: 0.5421 - val_loss: 1.1263 - val_accuracy: 0.6028\n",
      "Epoch 27/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.2475 - accuracy: 0.5500 - val_loss: 1.1791 - val_accuracy: 0.5677\n",
      "Epoch 28/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.2284 - accuracy: 0.5602 - val_loss: 1.1266 - val_accuracy: 0.6020\n",
      "Epoch 29/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.2168 - accuracy: 0.5635 - val_loss: 1.1353 - val_accuracy: 0.6016\n",
      "Epoch 30/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.2078 - accuracy: 0.5653 - val_loss: 1.1157 - val_accuracy: 0.6038\n",
      "Epoch 31/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.1879 - accuracy: 0.5739 - val_loss: 1.0593 - val_accuracy: 0.6262\n",
      "Epoch 32/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.1796 - accuracy: 0.5770 - val_loss: 1.1209 - val_accuracy: 0.6062\n",
      "Epoch 33/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.1605 - accuracy: 0.5832 - val_loss: 1.1333 - val_accuracy: 0.6048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.1498 - accuracy: 0.5899 - val_loss: 1.0357 - val_accuracy: 0.6377\n",
      "Epoch 35/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.1415 - accuracy: 0.5932 - val_loss: 1.0471 - val_accuracy: 0.6338\n",
      "Epoch 36/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.1304 - accuracy: 0.5967 - val_loss: 1.0293 - val_accuracy: 0.6400\n",
      "Epoch 37/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.1189 - accuracy: 0.6012 - val_loss: 1.0865 - val_accuracy: 0.6098\n",
      "Epoch 38/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.1104 - accuracy: 0.6040 - val_loss: 0.9947 - val_accuracy: 0.6459\n",
      "Epoch 39/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.0967 - accuracy: 0.6117 - val_loss: 0.9634 - val_accuracy: 0.6558\n",
      "Epoch 40/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.0884 - accuracy: 0.6124 - val_loss: 0.9859 - val_accuracy: 0.6479\n",
      "Epoch 41/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.0790 - accuracy: 0.6171 - val_loss: 0.9813 - val_accuracy: 0.6502\n",
      "Epoch 42/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.0731 - accuracy: 0.6218 - val_loss: 1.0066 - val_accuracy: 0.6408\n",
      "Epoch 43/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.0588 - accuracy: 0.6240 - val_loss: 0.9766 - val_accuracy: 0.6500\n",
      "Epoch 44/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.0539 - accuracy: 0.6255 - val_loss: 0.9803 - val_accuracy: 0.6509\n",
      "Epoch 45/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.0439 - accuracy: 0.6299 - val_loss: 0.9267 - val_accuracy: 0.6719\n",
      "Epoch 46/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.0374 - accuracy: 0.6355 - val_loss: 0.9054 - val_accuracy: 0.6816\n",
      "Epoch 47/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.0289 - accuracy: 0.6369 - val_loss: 0.9004 - val_accuracy: 0.6819\n",
      "Epoch 48/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.0238 - accuracy: 0.6389 - val_loss: 0.9042 - val_accuracy: 0.6815\n",
      "Epoch 49/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.0087 - accuracy: 0.6441 - val_loss: 0.9455 - val_accuracy: 0.6667\n",
      "Epoch 50/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.0057 - accuracy: 0.6476 - val_loss: 0.8829 - val_accuracy: 0.6880\n",
      "Epoch 51/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.0052 - accuracy: 0.6468 - val_loss: 0.9325 - val_accuracy: 0.6717\n",
      "Epoch 52/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.9946 - accuracy: 0.6489 - val_loss: 0.9554 - val_accuracy: 0.6690\n",
      "Epoch 53/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.9842 - accuracy: 0.6544 - val_loss: 0.8539 - val_accuracy: 0.7001\n",
      "Epoch 54/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9835 - accuracy: 0.6554 - val_loss: 0.8768 - val_accuracy: 0.6923\n",
      "Epoch 55/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.9727 - accuracy: 0.6612 - val_loss: 0.9087 - val_accuracy: 0.6823\n",
      "Epoch 56/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.9723 - accuracy: 0.6607 - val_loss: 0.8690 - val_accuracy: 0.6958\n",
      "Epoch 57/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9631 - accuracy: 0.6637 - val_loss: 0.8891 - val_accuracy: 0.6832\n",
      "Epoch 58/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9577 - accuracy: 0.6639 - val_loss: 0.8536 - val_accuracy: 0.7028\n",
      "Epoch 59/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9468 - accuracy: 0.6679 - val_loss: 0.8592 - val_accuracy: 0.6989\n",
      "Epoch 60/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9468 - accuracy: 0.6695 - val_loss: 0.8286 - val_accuracy: 0.7118\n",
      "Epoch 61/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9333 - accuracy: 0.6751 - val_loss: 0.8234 - val_accuracy: 0.7112\n",
      "Epoch 62/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9336 - accuracy: 0.6726 - val_loss: 0.8657 - val_accuracy: 0.7002\n",
      "Epoch 63/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9255 - accuracy: 0.6760 - val_loss: 0.8620 - val_accuracy: 0.6989\n",
      "Epoch 64/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9197 - accuracy: 0.6798 - val_loss: 0.7892 - val_accuracy: 0.7217\n",
      "Epoch 65/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.9176 - accuracy: 0.6795 - val_loss: 0.8163 - val_accuracy: 0.7133\n",
      "Epoch 66/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9092 - accuracy: 0.6842 - val_loss: 0.7829 - val_accuracy: 0.7270\n",
      "Epoch 67/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.9032 - accuracy: 0.6850 - val_loss: 0.8093 - val_accuracy: 0.7148\n",
      "Epoch 68/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.8964 - accuracy: 0.6856 - val_loss: 0.8087 - val_accuracy: 0.7178\n",
      "Epoch 69/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8917 - accuracy: 0.6878 - val_loss: 0.8168 - val_accuracy: 0.7147\n",
      "Epoch 70/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8854 - accuracy: 0.6900 - val_loss: 0.8210 - val_accuracy: 0.7130\n",
      "Epoch 71/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.8837 - accuracy: 0.6898 - val_loss: 0.7647 - val_accuracy: 0.7318\n",
      "Epoch 72/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8808 - accuracy: 0.6926 - val_loss: 0.8219 - val_accuracy: 0.7123\n",
      "Epoch 73/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8754 - accuracy: 0.6958 - val_loss: 0.7557 - val_accuracy: 0.7358\n",
      "Epoch 74/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.8677 - accuracy: 0.6992 - val_loss: 0.7659 - val_accuracy: 0.7354\n",
      "Epoch 75/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.8665 - accuracy: 0.6976 - val_loss: 0.7797 - val_accuracy: 0.7306\n",
      "Epoch 76/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8554 - accuracy: 0.7017 - val_loss: 0.7464 - val_accuracy: 0.7397\n",
      "Epoch 77/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8541 - accuracy: 0.7032 - val_loss: 0.7891 - val_accuracy: 0.7257\n",
      "Epoch 78/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.8511 - accuracy: 0.7056 - val_loss: 0.7547 - val_accuracy: 0.7386\n",
      "Epoch 79/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8501 - accuracy: 0.7057 - val_loss: 0.7792 - val_accuracy: 0.7285\n",
      "Epoch 80/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8412 - accuracy: 0.7073 - val_loss: 0.7350 - val_accuracy: 0.7437\n",
      "Epoch 81/100\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.8367 - accuracy: 0.7087 - val_loss: 0.7533 - val_accuracy: 0.7391\n",
      "Epoch 82/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8360 - accuracy: 0.7107 - val_loss: 0.7162 - val_accuracy: 0.7527\n",
      "Epoch 83/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8271 - accuracy: 0.7141 - val_loss: 0.7256 - val_accuracy: 0.7462\n",
      "Epoch 84/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8222 - accuracy: 0.7152 - val_loss: 0.7178 - val_accuracy: 0.7491\n",
      "Epoch 85/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8143 - accuracy: 0.7187 - val_loss: 0.7194 - val_accuracy: 0.7507\n",
      "Epoch 86/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8154 - accuracy: 0.7158 - val_loss: 0.7141 - val_accuracy: 0.7499\n",
      "Epoch 87/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8087 - accuracy: 0.7185 - val_loss: 0.7344 - val_accuracy: 0.7433\n",
      "Epoch 88/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8067 - accuracy: 0.7216 - val_loss: 0.7000 - val_accuracy: 0.7546\n",
      "Epoch 89/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.8036 - accuracy: 0.7194 - val_loss: 0.7169 - val_accuracy: 0.7515\n",
      "Epoch 90/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.7999 - accuracy: 0.7243 - val_loss: 0.7050 - val_accuracy: 0.7545\n",
      "Epoch 91/100\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.7955 - accuracy: 0.7234 - val_loss: 0.7045 - val_accuracy: 0.7557\n",
      "Epoch 92/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.7913 - accuracy: 0.7247 - val_loss: 0.7194 - val_accuracy: 0.7487\n",
      "Epoch 93/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.7850 - accuracy: 0.7285 - val_loss: 0.6732 - val_accuracy: 0.7634\n",
      "Epoch 94/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.7796 - accuracy: 0.7306 - val_loss: 0.6845 - val_accuracy: 0.7622\n",
      "Epoch 95/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.7797 - accuracy: 0.7312 - val_loss: 0.6926 - val_accuracy: 0.7569\n",
      "Epoch 96/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.7828 - accuracy: 0.7310 - val_loss: 0.6793 - val_accuracy: 0.7596\n",
      "Epoch 97/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.7748 - accuracy: 0.7320 - val_loss: 0.6905 - val_accuracy: 0.7592\n",
      "Epoch 98/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.7722 - accuracy: 0.7334 - val_loss: 0.7102 - val_accuracy: 0.7526\n",
      "Epoch 99/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.7678 - accuracy: 0.7336 - val_loss: 0.6603 - val_accuracy: 0.7699\n",
      "Epoch 100/100\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.7607 - accuracy: 0.7365 - val_loss: 0.6596 - val_accuracy: 0.7692\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84ae205ac0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', \n",
    "                    input_shape=(32, 32, 3)))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.2))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.3))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((4, 4)))\n",
    "model_10.add(Dropout(0.4))\n",
    "model_10.add(Flatten())\n",
    "model_10.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model_10.add(Dropout(0.5))\n",
    "model_10.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Convert the model to a quantization aware model\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(model_10)\n",
    "\n",
    "quant_aware_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# define the ModelCheckpoint callback to save the model after each epoch\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_10_gpu_Base-CubeAI_4Conv_QAT.h5', save_freq='epoch')\n",
    "\n",
    "quant_aware_model.summary()\n",
    "\n",
    "# Train and evaluate the quantization aware model\n",
    "quant_aware_model.fit(x_train,y_train, batch_size=64,epochs=100,validation_data=(x_test, y_test),\n",
    "                     callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "385d854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization aware training loss:  0.6596236228942871\n",
      "Quantization aware training accuracy:  0.7692000269889832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_42_layer_call_fn, conv2d_42_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, dropout_32_layer_call_fn, dropout_32_layer_call_and_return_conditional_losses while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp95_4bf4e/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp95_4bf4e/assets\n",
      "/home/themandalorian/anaconda3/envs/ml-on-mc/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-05-17 17:13:29.184972: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-05-17 17:13:29.185005: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-05-17 17:13:29.185144: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmp95_4bf4e\n",
      "2023-05-17 17:13:29.190065: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-17 17:13:29.190082: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmp95_4bf4e\n",
      "2023-05-17 17:13:29.205439: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-05-17 17:13:29.277525: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmp95_4bf4e\n",
      "2023-05-17 17:13:29.295897: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 110753 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_conv2d_42_input:0\n",
      "shape: [ 1 32 32  3]\n",
      "type: <class 'numpy.uint8'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.uint8'>\n",
      "Accuracy of quantized to int8 model is 76.85%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "quant_loss, quant_acc = quant_aware_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Quantization aware training loss: ', quant_loss)\n",
    "print('Quantization aware training accuracy: ', quant_acc)\n",
    "\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant_int8_qat = converter.convert()\n",
    "\n",
    "open(\"model_10_gpu_Base-CubeAI_4Conv_qat_int8.tflite\", \"wb\").write(tflite_model_quant_int8_qat)\n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(\"model_10_gpu_Base-CubeAI_4Conv_qat_int8.tflite\")\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "predictions = np.zeros((len(x_test),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(len(x_test)):\n",
    "    val_batch = x_test[i]\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()\n",
    "    \n",
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 10000\n",
    "print(\"Accuracy of quantized to int8 model is {}%\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6c785b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'cifar10_base_4conv_ptq'\n",
    "# check if dir 'cfiles' exists, if not create it\n",
    "if not os.path.exists('cfiles'):\n",
    "    os.makedirs('cfiles')\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ace95010",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'cifar10_base_4conv_qat'\n",
    "# check if dir 'cfiles' exists, if not create it\n",
    "if not os.path.exists('cfiles'):\n",
    "    os.makedirs('cfiles')\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model_quant_int8_qat, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fcdc1187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_49 (Conv2D)          (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_30 (MaxPoolin  (None, 16, 16, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_50 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_31 (MaxPoolin  (None, 8, 8, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_51 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_32 (MaxPoolin  (None, 2, 2, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 2, 2, 128)         0         \n",
      "                                                                 \n",
      " flatten_10 (Flatten)        (None, 512)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 128)               65664     \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,202\n",
      "Trainable params: 160,202\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/themandalorian/anaconda3/envs/ml-on-mc/lib/python3.8/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:111: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "782/782 [==============================] - 4s 4ms/step - loss: 2.3278 - accuracy: 0.1217 - val_loss: 2.2279 - val_accuracy: 0.1901\n",
      "Epoch 2/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.1597 - accuracy: 0.1832 - val_loss: 2.0796 - val_accuracy: 0.2470\n",
      "Epoch 3/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 2.0512 - accuracy: 0.2246 - val_loss: 1.9502 - val_accuracy: 0.3100\n",
      "Epoch 4/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.9420 - accuracy: 0.2638 - val_loss: 1.8353 - val_accuracy: 0.3580\n",
      "Epoch 5/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.8684 - accuracy: 0.2937 - val_loss: 1.7781 - val_accuracy: 0.3683\n",
      "Epoch 6/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.8183 - accuracy: 0.3095 - val_loss: 1.7377 - val_accuracy: 0.3816\n",
      "Epoch 7/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.7751 - accuracy: 0.3276 - val_loss: 1.7001 - val_accuracy: 0.3939\n",
      "Epoch 8/400\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 1.7386 - accuracy: 0.3440 - val_loss: 1.6575 - val_accuracy: 0.4104\n",
      "Epoch 9/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.7067 - accuracy: 0.3569 - val_loss: 1.6297 - val_accuracy: 0.4155\n",
      "Epoch 10/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6759 - accuracy: 0.3703 - val_loss: 1.5887 - val_accuracy: 0.4297\n",
      "Epoch 11/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6550 - accuracy: 0.3785 - val_loss: 1.5743 - val_accuracy: 0.4404\n",
      "Epoch 12/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.6281 - accuracy: 0.3903 - val_loss: 1.5344 - val_accuracy: 0.4483\n",
      "Epoch 13/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5996 - accuracy: 0.4049 - val_loss: 1.5076 - val_accuracy: 0.4636\n",
      "Epoch 14/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5794 - accuracy: 0.4130 - val_loss: 1.4893 - val_accuracy: 0.4739\n",
      "Epoch 15/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5621 - accuracy: 0.4211 - val_loss: 1.4719 - val_accuracy: 0.4737\n",
      "Epoch 16/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5429 - accuracy: 0.4287 - val_loss: 1.4493 - val_accuracy: 0.4826\n",
      "Epoch 17/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5212 - accuracy: 0.4379 - val_loss: 1.4362 - val_accuracy: 0.4952\n",
      "Epoch 18/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.5037 - accuracy: 0.4452 - val_loss: 1.4041 - val_accuracy: 0.5121\n",
      "Epoch 19/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4944 - accuracy: 0.4510 - val_loss: 1.3990 - val_accuracy: 0.4967\n",
      "Epoch 20/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4800 - accuracy: 0.4540 - val_loss: 1.3702 - val_accuracy: 0.5181\n",
      "Epoch 21/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4582 - accuracy: 0.4668 - val_loss: 1.3746 - val_accuracy: 0.5053\n",
      "Epoch 22/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4431 - accuracy: 0.4736 - val_loss: 1.3331 - val_accuracy: 0.5310\n",
      "Epoch 23/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4329 - accuracy: 0.4726 - val_loss: 1.3140 - val_accuracy: 0.5355\n",
      "Epoch 24/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4190 - accuracy: 0.4783 - val_loss: 1.3112 - val_accuracy: 0.5500\n",
      "Epoch 25/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.4044 - accuracy: 0.4851 - val_loss: 1.3057 - val_accuracy: 0.5381\n",
      "Epoch 26/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3917 - accuracy: 0.4933 - val_loss: 1.2645 - val_accuracy: 0.5598\n",
      "Epoch 27/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3773 - accuracy: 0.4958 - val_loss: 1.2670 - val_accuracy: 0.5576\n",
      "Epoch 28/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3676 - accuracy: 0.4989 - val_loss: 1.2560 - val_accuracy: 0.5632\n",
      "Epoch 29/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3562 - accuracy: 0.5064 - val_loss: 1.2665 - val_accuracy: 0.5499\n",
      "Epoch 30/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3406 - accuracy: 0.5108 - val_loss: 1.2212 - val_accuracy: 0.5795\n",
      "Epoch 31/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3416 - accuracy: 0.5093 - val_loss: 1.2130 - val_accuracy: 0.5691\n",
      "Epoch 32/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3276 - accuracy: 0.5176 - val_loss: 1.2090 - val_accuracy: 0.5777\n",
      "Epoch 33/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3184 - accuracy: 0.5197 - val_loss: 1.2366 - val_accuracy: 0.5649\n",
      "Epoch 34/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.3104 - accuracy: 0.5217 - val_loss: 1.1927 - val_accuracy: 0.5886\n",
      "Epoch 35/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2933 - accuracy: 0.5301 - val_loss: 1.2111 - val_accuracy: 0.5721\n",
      "Epoch 36/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2855 - accuracy: 0.5327 - val_loss: 1.1488 - val_accuracy: 0.5985\n",
      "Epoch 37/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2790 - accuracy: 0.5371 - val_loss: 1.1476 - val_accuracy: 0.6074\n",
      "Epoch 38/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2715 - accuracy: 0.5406 - val_loss: 1.1350 - val_accuracy: 0.6053\n",
      "Epoch 39/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2659 - accuracy: 0.5399 - val_loss: 1.1413 - val_accuracy: 0.6012\n",
      "Epoch 40/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2533 - accuracy: 0.5460 - val_loss: 1.1276 - val_accuracy: 0.6090\n",
      "Epoch 41/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2375 - accuracy: 0.5503 - val_loss: 1.1293 - val_accuracy: 0.6144\n",
      "Epoch 42/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2367 - accuracy: 0.5524 - val_loss: 1.1068 - val_accuracy: 0.6229\n",
      "Epoch 43/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2305 - accuracy: 0.5554 - val_loss: 1.0966 - val_accuracy: 0.6223\n",
      "Epoch 44/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2252 - accuracy: 0.5568 - val_loss: 1.1274 - val_accuracy: 0.6085\n",
      "Epoch 45/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2126 - accuracy: 0.5623 - val_loss: 1.0938 - val_accuracy: 0.6263\n",
      "Epoch 46/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.2071 - accuracy: 0.5666 - val_loss: 1.0754 - val_accuracy: 0.6274\n",
      "Epoch 47/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1980 - accuracy: 0.5692 - val_loss: 1.0617 - val_accuracy: 0.6352\n",
      "Epoch 48/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1895 - accuracy: 0.5695 - val_loss: 1.0823 - val_accuracy: 0.6243\n",
      "Epoch 49/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1852 - accuracy: 0.5730 - val_loss: 1.0443 - val_accuracy: 0.6355\n",
      "Epoch 50/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1751 - accuracy: 0.5790 - val_loss: 1.0409 - val_accuracy: 0.6463\n",
      "Epoch 51/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1709 - accuracy: 0.5801 - val_loss: 1.0182 - val_accuracy: 0.6475\n",
      "Epoch 52/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1597 - accuracy: 0.5814 - val_loss: 1.0330 - val_accuracy: 0.6469\n",
      "Epoch 53/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1546 - accuracy: 0.5885 - val_loss: 1.0146 - val_accuracy: 0.6431\n",
      "Epoch 54/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1522 - accuracy: 0.5852 - val_loss: 1.0078 - val_accuracy: 0.6548\n",
      "Epoch 55/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1455 - accuracy: 0.5907 - val_loss: 1.0120 - val_accuracy: 0.6473\n",
      "Epoch 56/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1365 - accuracy: 0.5964 - val_loss: 0.9854 - val_accuracy: 0.6618\n",
      "Epoch 57/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1267 - accuracy: 0.5979 - val_loss: 0.9995 - val_accuracy: 0.6510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1217 - accuracy: 0.6020 - val_loss: 0.9741 - val_accuracy: 0.6631\n",
      "Epoch 59/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1188 - accuracy: 0.5986 - val_loss: 0.9636 - val_accuracy: 0.6697\n",
      "Epoch 60/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1136 - accuracy: 0.6054 - val_loss: 0.9680 - val_accuracy: 0.6698\n",
      "Epoch 61/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.1115 - accuracy: 0.6007 - val_loss: 0.9728 - val_accuracy: 0.6630\n",
      "Epoch 62/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0993 - accuracy: 0.6089 - val_loss: 0.9634 - val_accuracy: 0.6633\n",
      "Epoch 63/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0936 - accuracy: 0.6099 - val_loss: 0.9906 - val_accuracy: 0.6493\n",
      "Epoch 64/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0884 - accuracy: 0.6156 - val_loss: 0.9494 - val_accuracy: 0.6651\n",
      "Epoch 65/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0862 - accuracy: 0.6127 - val_loss: 0.9540 - val_accuracy: 0.6738\n",
      "Epoch 66/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0827 - accuracy: 0.6141 - val_loss: 0.9701 - val_accuracy: 0.6595\n",
      "Epoch 67/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0756 - accuracy: 0.6209 - val_loss: 0.9160 - val_accuracy: 0.6839\n",
      "Epoch 68/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0708 - accuracy: 0.6175 - val_loss: 0.9408 - val_accuracy: 0.6668\n",
      "Epoch 69/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0633 - accuracy: 0.6223 - val_loss: 0.9317 - val_accuracy: 0.6702\n",
      "Epoch 70/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0616 - accuracy: 0.6229 - val_loss: 0.9311 - val_accuracy: 0.6746\n",
      "Epoch 71/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0630 - accuracy: 0.6225 - val_loss: 0.9290 - val_accuracy: 0.6701\n",
      "Epoch 72/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0514 - accuracy: 0.6270 - val_loss: 0.9173 - val_accuracy: 0.6773\n",
      "Epoch 73/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0525 - accuracy: 0.6273 - val_loss: 0.9201 - val_accuracy: 0.6802\n",
      "Epoch 74/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0365 - accuracy: 0.6295 - val_loss: 0.9413 - val_accuracy: 0.6665\n",
      "Epoch 75/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0425 - accuracy: 0.6335 - val_loss: 0.9045 - val_accuracy: 0.6851\n",
      "Epoch 76/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0352 - accuracy: 0.6350 - val_loss: 0.8949 - val_accuracy: 0.6848\n",
      "Epoch 77/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0316 - accuracy: 0.6354 - val_loss: 0.8951 - val_accuracy: 0.6896\n",
      "Epoch 78/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0235 - accuracy: 0.6357 - val_loss: 0.8981 - val_accuracy: 0.6876\n",
      "Epoch 79/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0180 - accuracy: 0.6417 - val_loss: 0.9175 - val_accuracy: 0.6798\n",
      "Epoch 80/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0182 - accuracy: 0.6391 - val_loss: 0.8951 - val_accuracy: 0.6834\n",
      "Epoch 81/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0125 - accuracy: 0.6429 - val_loss: 0.8963 - val_accuracy: 0.6835\n",
      "Epoch 82/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0043 - accuracy: 0.6454 - val_loss: 0.8714 - val_accuracy: 0.6979\n",
      "Epoch 83/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0046 - accuracy: 0.6457 - val_loss: 0.8742 - val_accuracy: 0.6938\n",
      "Epoch 84/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 1.0002 - accuracy: 0.6469 - val_loss: 0.9080 - val_accuracy: 0.6795\n",
      "Epoch 85/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9962 - accuracy: 0.6492 - val_loss: 0.8663 - val_accuracy: 0.6975\n",
      "Epoch 86/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9944 - accuracy: 0.6499 - val_loss: 0.8471 - val_accuracy: 0.7072\n",
      "Epoch 87/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9965 - accuracy: 0.6500 - val_loss: 0.8915 - val_accuracy: 0.6831\n",
      "Epoch 88/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9903 - accuracy: 0.6493 - val_loss: 0.8924 - val_accuracy: 0.6790\n",
      "Epoch 89/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9799 - accuracy: 0.6553 - val_loss: 0.8534 - val_accuracy: 0.7025\n",
      "Epoch 90/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9770 - accuracy: 0.6568 - val_loss: 0.8514 - val_accuracy: 0.6997\n",
      "Epoch 91/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9730 - accuracy: 0.6582 - val_loss: 0.8297 - val_accuracy: 0.7113\n",
      "Epoch 92/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9740 - accuracy: 0.6578 - val_loss: 0.8445 - val_accuracy: 0.7030\n",
      "Epoch 93/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9698 - accuracy: 0.6590 - val_loss: 0.8212 - val_accuracy: 0.7126\n",
      "Epoch 94/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9647 - accuracy: 0.6593 - val_loss: 0.8337 - val_accuracy: 0.7103\n",
      "Epoch 95/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9628 - accuracy: 0.6630 - val_loss: 0.8231 - val_accuracy: 0.7183\n",
      "Epoch 96/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9615 - accuracy: 0.6625 - val_loss: 0.8259 - val_accuracy: 0.7113\n",
      "Epoch 97/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9572 - accuracy: 0.6634 - val_loss: 0.8316 - val_accuracy: 0.7093\n",
      "Epoch 98/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9541 - accuracy: 0.6653 - val_loss: 0.8197 - val_accuracy: 0.7116\n",
      "Epoch 99/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9538 - accuracy: 0.6639 - val_loss: 0.8280 - val_accuracy: 0.7095\n",
      "Epoch 100/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9463 - accuracy: 0.6693 - val_loss: 0.8131 - val_accuracy: 0.7175\n",
      "Epoch 101/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9445 - accuracy: 0.6682 - val_loss: 0.8265 - val_accuracy: 0.7109\n",
      "Epoch 102/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9421 - accuracy: 0.6699 - val_loss: 0.8149 - val_accuracy: 0.7112\n",
      "Epoch 103/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9428 - accuracy: 0.6711 - val_loss: 0.8108 - val_accuracy: 0.7196\n",
      "Epoch 104/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9340 - accuracy: 0.6717 - val_loss: 0.7988 - val_accuracy: 0.7228\n",
      "Epoch 105/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9367 - accuracy: 0.6725 - val_loss: 0.8355 - val_accuracy: 0.7060\n",
      "Epoch 106/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9322 - accuracy: 0.6715 - val_loss: 0.7938 - val_accuracy: 0.7240\n",
      "Epoch 107/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9205 - accuracy: 0.6762 - val_loss: 0.7896 - val_accuracy: 0.7274\n",
      "Epoch 108/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9275 - accuracy: 0.6746 - val_loss: 0.7825 - val_accuracy: 0.7300\n",
      "Epoch 109/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9240 - accuracy: 0.6752 - val_loss: 0.8008 - val_accuracy: 0.7227\n",
      "Epoch 110/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9234 - accuracy: 0.6750 - val_loss: 0.7712 - val_accuracy: 0.7350\n",
      "Epoch 111/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9224 - accuracy: 0.6770 - val_loss: 0.7691 - val_accuracy: 0.7315\n",
      "Epoch 112/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9170 - accuracy: 0.6767 - val_loss: 0.7820 - val_accuracy: 0.7310\n",
      "Epoch 113/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9145 - accuracy: 0.6822 - val_loss: 0.7771 - val_accuracy: 0.7273\n",
      "Epoch 114/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9108 - accuracy: 0.6811 - val_loss: 0.7594 - val_accuracy: 0.7380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9062 - accuracy: 0.6831 - val_loss: 0.7687 - val_accuracy: 0.7341\n",
      "Epoch 116/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9085 - accuracy: 0.6825 - val_loss: 0.7606 - val_accuracy: 0.7384\n",
      "Epoch 117/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9096 - accuracy: 0.6803 - val_loss: 0.7780 - val_accuracy: 0.7316\n",
      "Epoch 118/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8988 - accuracy: 0.6868 - val_loss: 0.7822 - val_accuracy: 0.7282\n",
      "Epoch 119/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.9035 - accuracy: 0.6825 - val_loss: 0.7700 - val_accuracy: 0.7342\n",
      "Epoch 120/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8957 - accuracy: 0.6852 - val_loss: 0.7575 - val_accuracy: 0.7407\n",
      "Epoch 121/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8965 - accuracy: 0.6852 - val_loss: 0.7482 - val_accuracy: 0.7456\n",
      "Epoch 122/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8980 - accuracy: 0.6857 - val_loss: 0.7872 - val_accuracy: 0.7271\n",
      "Epoch 123/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8918 - accuracy: 0.6875 - val_loss: 0.7592 - val_accuracy: 0.7369\n",
      "Epoch 124/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8931 - accuracy: 0.6894 - val_loss: 0.7527 - val_accuracy: 0.7417\n",
      "Epoch 125/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8852 - accuracy: 0.6893 - val_loss: 0.7637 - val_accuracy: 0.7320\n",
      "Epoch 126/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8835 - accuracy: 0.6913 - val_loss: 0.7515 - val_accuracy: 0.7410\n",
      "Epoch 127/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8821 - accuracy: 0.6926 - val_loss: 0.7438 - val_accuracy: 0.7431\n",
      "Epoch 128/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8871 - accuracy: 0.6904 - val_loss: 0.7501 - val_accuracy: 0.7414\n",
      "Epoch 129/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8752 - accuracy: 0.6921 - val_loss: 0.7483 - val_accuracy: 0.7367\n",
      "Epoch 130/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8775 - accuracy: 0.6933 - val_loss: 0.7674 - val_accuracy: 0.7301\n",
      "Epoch 131/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8758 - accuracy: 0.6954 - val_loss: 0.7389 - val_accuracy: 0.7478\n",
      "Epoch 132/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8726 - accuracy: 0.6955 - val_loss: 0.7360 - val_accuracy: 0.7441\n",
      "Epoch 133/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8705 - accuracy: 0.6966 - val_loss: 0.7645 - val_accuracy: 0.7328\n",
      "Epoch 134/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8655 - accuracy: 0.6995 - val_loss: 0.7355 - val_accuracy: 0.7463\n",
      "Epoch 135/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8685 - accuracy: 0.6964 - val_loss: 0.7534 - val_accuracy: 0.7356\n",
      "Epoch 136/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8700 - accuracy: 0.6978 - val_loss: 0.7413 - val_accuracy: 0.7454\n",
      "Epoch 137/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8633 - accuracy: 0.6982 - val_loss: 0.7278 - val_accuracy: 0.7514\n",
      "Epoch 138/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8587 - accuracy: 0.7006 - val_loss: 0.7271 - val_accuracy: 0.7490\n",
      "Epoch 139/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8602 - accuracy: 0.7005 - val_loss: 0.7114 - val_accuracy: 0.7543\n",
      "Epoch 140/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8570 - accuracy: 0.7008 - val_loss: 0.7285 - val_accuracy: 0.7465\n",
      "Epoch 141/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8550 - accuracy: 0.7032 - val_loss: 0.7222 - val_accuracy: 0.7487\n",
      "Epoch 142/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8557 - accuracy: 0.7037 - val_loss: 0.7308 - val_accuracy: 0.7454\n",
      "Epoch 143/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8530 - accuracy: 0.7004 - val_loss: 0.7432 - val_accuracy: 0.7377\n",
      "Epoch 144/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8493 - accuracy: 0.7037 - val_loss: 0.7181 - val_accuracy: 0.7522\n",
      "Epoch 145/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8467 - accuracy: 0.7052 - val_loss: 0.7224 - val_accuracy: 0.7500\n",
      "Epoch 146/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8487 - accuracy: 0.7057 - val_loss: 0.7287 - val_accuracy: 0.7498\n",
      "Epoch 147/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8431 - accuracy: 0.7041 - val_loss: 0.7179 - val_accuracy: 0.7510\n",
      "Epoch 148/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8379 - accuracy: 0.7074 - val_loss: 0.7250 - val_accuracy: 0.7485\n",
      "Epoch 149/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8389 - accuracy: 0.7065 - val_loss: 0.7100 - val_accuracy: 0.7564\n",
      "Epoch 150/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8376 - accuracy: 0.7063 - val_loss: 0.7012 - val_accuracy: 0.7593\n",
      "Epoch 151/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8344 - accuracy: 0.7100 - val_loss: 0.7234 - val_accuracy: 0.7514\n",
      "Epoch 152/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8387 - accuracy: 0.7083 - val_loss: 0.7044 - val_accuracy: 0.7584\n",
      "Epoch 153/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8404 - accuracy: 0.7080 - val_loss: 0.6964 - val_accuracy: 0.7610\n",
      "Epoch 154/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8340 - accuracy: 0.7080 - val_loss: 0.6998 - val_accuracy: 0.7577\n",
      "Epoch 155/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8301 - accuracy: 0.7117 - val_loss: 0.7134 - val_accuracy: 0.7504\n",
      "Epoch 156/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8301 - accuracy: 0.7106 - val_loss: 0.7266 - val_accuracy: 0.7457\n",
      "Epoch 157/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8256 - accuracy: 0.7137 - val_loss: 0.7292 - val_accuracy: 0.7424\n",
      "Epoch 158/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8264 - accuracy: 0.7133 - val_loss: 0.7046 - val_accuracy: 0.7557\n",
      "Epoch 159/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8242 - accuracy: 0.7141 - val_loss: 0.7058 - val_accuracy: 0.7530\n",
      "Epoch 160/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8221 - accuracy: 0.7141 - val_loss: 0.7005 - val_accuracy: 0.7595\n",
      "Epoch 161/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8246 - accuracy: 0.7135 - val_loss: 0.6994 - val_accuracy: 0.7592\n",
      "Epoch 162/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8210 - accuracy: 0.7144 - val_loss: 0.7183 - val_accuracy: 0.7498\n",
      "Epoch 163/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8196 - accuracy: 0.7153 - val_loss: 0.6858 - val_accuracy: 0.7667\n",
      "Epoch 164/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8166 - accuracy: 0.7165 - val_loss: 0.7126 - val_accuracy: 0.7518\n",
      "Epoch 165/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8153 - accuracy: 0.7168 - val_loss: 0.6905 - val_accuracy: 0.7642\n",
      "Epoch 166/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8140 - accuracy: 0.7176 - val_loss: 0.6839 - val_accuracy: 0.7653\n",
      "Epoch 167/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8154 - accuracy: 0.7167 - val_loss: 0.6782 - val_accuracy: 0.7678\n",
      "Epoch 168/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8123 - accuracy: 0.7162 - val_loss: 0.6955 - val_accuracy: 0.7572\n",
      "Epoch 169/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8090 - accuracy: 0.7192 - val_loss: 0.6814 - val_accuracy: 0.7673\n",
      "Epoch 170/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8061 - accuracy: 0.7183 - val_loss: 0.6931 - val_accuracy: 0.7579\n",
      "Epoch 171/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8141 - accuracy: 0.7167 - val_loss: 0.7024 - val_accuracy: 0.7572\n",
      "Epoch 172/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8037 - accuracy: 0.7223 - val_loss: 0.6923 - val_accuracy: 0.7607\n",
      "Epoch 173/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8049 - accuracy: 0.7200 - val_loss: 0.6940 - val_accuracy: 0.7570\n",
      "Epoch 174/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8012 - accuracy: 0.7202 - val_loss: 0.7014 - val_accuracy: 0.7561\n",
      "Epoch 175/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8054 - accuracy: 0.7175 - val_loss: 0.6806 - val_accuracy: 0.7670\n",
      "Epoch 176/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7999 - accuracy: 0.7219 - val_loss: 0.6775 - val_accuracy: 0.7642\n",
      "Epoch 177/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8042 - accuracy: 0.7198 - val_loss: 0.6809 - val_accuracy: 0.7638\n",
      "Epoch 178/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.8021 - accuracy: 0.7196 - val_loss: 0.6957 - val_accuracy: 0.7567\n",
      "Epoch 179/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7985 - accuracy: 0.7218 - val_loss: 0.6885 - val_accuracy: 0.7610\n",
      "Epoch 180/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7965 - accuracy: 0.7236 - val_loss: 0.7056 - val_accuracy: 0.7553\n",
      "Epoch 181/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7953 - accuracy: 0.7228 - val_loss: 0.6706 - val_accuracy: 0.7693\n",
      "Epoch 182/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7906 - accuracy: 0.7260 - val_loss: 0.6875 - val_accuracy: 0.7607\n",
      "Epoch 183/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7860 - accuracy: 0.7260 - val_loss: 0.6733 - val_accuracy: 0.7690\n",
      "Epoch 184/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7911 - accuracy: 0.7253 - val_loss: 0.6972 - val_accuracy: 0.7596\n",
      "Epoch 185/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7877 - accuracy: 0.7272 - val_loss: 0.6619 - val_accuracy: 0.7732\n",
      "Epoch 186/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7882 - accuracy: 0.7271 - val_loss: 0.6556 - val_accuracy: 0.7733\n",
      "Epoch 187/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7877 - accuracy: 0.7249 - val_loss: 0.6725 - val_accuracy: 0.7659\n",
      "Epoch 188/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7895 - accuracy: 0.7237 - val_loss: 0.6588 - val_accuracy: 0.7712\n",
      "Epoch 189/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7843 - accuracy: 0.7268 - val_loss: 0.6610 - val_accuracy: 0.7726\n",
      "Epoch 190/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7828 - accuracy: 0.7291 - val_loss: 0.6628 - val_accuracy: 0.7697\n",
      "Epoch 191/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7835 - accuracy: 0.7277 - val_loss: 0.6624 - val_accuracy: 0.7694\n",
      "Epoch 192/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7824 - accuracy: 0.7287 - val_loss: 0.6699 - val_accuracy: 0.7697\n",
      "Epoch 193/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7790 - accuracy: 0.7287 - val_loss: 0.6592 - val_accuracy: 0.7724\n",
      "Epoch 194/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7793 - accuracy: 0.7284 - val_loss: 0.6830 - val_accuracy: 0.7594\n",
      "Epoch 195/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7792 - accuracy: 0.7279 - val_loss: 0.6507 - val_accuracy: 0.7792\n",
      "Epoch 196/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7781 - accuracy: 0.7300 - val_loss: 0.6514 - val_accuracy: 0.7757\n",
      "Epoch 197/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7729 - accuracy: 0.7323 - val_loss: 0.7231 - val_accuracy: 0.7480\n",
      "Epoch 198/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7786 - accuracy: 0.7303 - val_loss: 0.6418 - val_accuracy: 0.7800\n",
      "Epoch 199/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7776 - accuracy: 0.7299 - val_loss: 0.6967 - val_accuracy: 0.7557\n",
      "Epoch 200/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7735 - accuracy: 0.7304 - val_loss: 0.6785 - val_accuracy: 0.7645\n",
      "Epoch 201/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7726 - accuracy: 0.7314 - val_loss: 0.6593 - val_accuracy: 0.7703\n",
      "Epoch 202/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7645 - accuracy: 0.7339 - val_loss: 0.6440 - val_accuracy: 0.7788\n",
      "Epoch 203/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7669 - accuracy: 0.7329 - val_loss: 0.6398 - val_accuracy: 0.7812\n",
      "Epoch 204/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7766 - accuracy: 0.7302 - val_loss: 0.6439 - val_accuracy: 0.7806\n",
      "Epoch 205/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7670 - accuracy: 0.7350 - val_loss: 0.6469 - val_accuracy: 0.7766\n",
      "Epoch 206/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7679 - accuracy: 0.7332 - val_loss: 0.6391 - val_accuracy: 0.7809\n",
      "Epoch 207/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7644 - accuracy: 0.7343 - val_loss: 0.6678 - val_accuracy: 0.7723\n",
      "Epoch 208/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7604 - accuracy: 0.7347 - val_loss: 0.6362 - val_accuracy: 0.7803\n",
      "Epoch 209/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7648 - accuracy: 0.7331 - val_loss: 0.6425 - val_accuracy: 0.7779\n",
      "Epoch 210/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7672 - accuracy: 0.7331 - val_loss: 0.6599 - val_accuracy: 0.7682\n",
      "Epoch 211/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7601 - accuracy: 0.7353 - val_loss: 0.6485 - val_accuracy: 0.7763\n",
      "Epoch 212/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7606 - accuracy: 0.7358 - val_loss: 0.6393 - val_accuracy: 0.7790\n",
      "Epoch 213/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7558 - accuracy: 0.7381 - val_loss: 0.6419 - val_accuracy: 0.7793\n",
      "Epoch 214/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7624 - accuracy: 0.7367 - val_loss: 0.6508 - val_accuracy: 0.7766\n",
      "Epoch 215/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7606 - accuracy: 0.7369 - val_loss: 0.6533 - val_accuracy: 0.7728\n",
      "Epoch 216/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7539 - accuracy: 0.7364 - val_loss: 0.6374 - val_accuracy: 0.7780\n",
      "Epoch 217/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7594 - accuracy: 0.7348 - val_loss: 0.6373 - val_accuracy: 0.7821\n",
      "Epoch 218/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7576 - accuracy: 0.7383 - val_loss: 0.6459 - val_accuracy: 0.7756\n",
      "Epoch 219/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7520 - accuracy: 0.7385 - val_loss: 0.6265 - val_accuracy: 0.7851\n",
      "Epoch 220/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7554 - accuracy: 0.7380 - val_loss: 0.6397 - val_accuracy: 0.7786\n",
      "Epoch 221/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7471 - accuracy: 0.7411 - val_loss: 0.6408 - val_accuracy: 0.7790\n",
      "Epoch 222/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7527 - accuracy: 0.7385 - val_loss: 0.6652 - val_accuracy: 0.7650\n",
      "Epoch 223/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7460 - accuracy: 0.7412 - val_loss: 0.6453 - val_accuracy: 0.7757\n",
      "Epoch 224/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7478 - accuracy: 0.7407 - val_loss: 0.6301 - val_accuracy: 0.7830\n",
      "Epoch 225/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7510 - accuracy: 0.7403 - val_loss: 0.6215 - val_accuracy: 0.7871\n",
      "Epoch 226/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7473 - accuracy: 0.7405 - val_loss: 0.6450 - val_accuracy: 0.7753\n",
      "Epoch 227/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7433 - accuracy: 0.7424 - val_loss: 0.6406 - val_accuracy: 0.7767\n",
      "Epoch 228/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7414 - accuracy: 0.7431 - val_loss: 0.6596 - val_accuracy: 0.7677\n",
      "Epoch 229/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7461 - accuracy: 0.7397 - val_loss: 0.6313 - val_accuracy: 0.7798\n",
      "Epoch 230/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7450 - accuracy: 0.7420 - val_loss: 0.6432 - val_accuracy: 0.7761\n",
      "Epoch 231/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7473 - accuracy: 0.7407 - val_loss: 0.6330 - val_accuracy: 0.7811\n",
      "Epoch 232/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7423 - accuracy: 0.7426 - val_loss: 0.6234 - val_accuracy: 0.7844\n",
      "Epoch 233/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7424 - accuracy: 0.7421 - val_loss: 0.6428 - val_accuracy: 0.7770\n",
      "Epoch 234/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7412 - accuracy: 0.7419 - val_loss: 0.6152 - val_accuracy: 0.7871\n",
      "Epoch 235/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7358 - accuracy: 0.7452 - val_loss: 0.6280 - val_accuracy: 0.7813\n",
      "Epoch 236/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7422 - accuracy: 0.7431 - val_loss: 0.6347 - val_accuracy: 0.7812\n",
      "Epoch 237/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7386 - accuracy: 0.7434 - val_loss: 0.6300 - val_accuracy: 0.7828\n",
      "Epoch 238/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7419 - accuracy: 0.7433 - val_loss: 0.6453 - val_accuracy: 0.7765\n",
      "Epoch 239/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7338 - accuracy: 0.7442 - val_loss: 0.6234 - val_accuracy: 0.7832\n",
      "Epoch 240/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7356 - accuracy: 0.7416 - val_loss: 0.6111 - val_accuracy: 0.7912\n",
      "Epoch 241/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7379 - accuracy: 0.7428 - val_loss: 0.6306 - val_accuracy: 0.7830\n",
      "Epoch 242/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7344 - accuracy: 0.7436 - val_loss: 0.6149 - val_accuracy: 0.7879\n",
      "Epoch 243/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7373 - accuracy: 0.7445 - val_loss: 0.6158 - val_accuracy: 0.7867\n",
      "Epoch 244/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7359 - accuracy: 0.7456 - val_loss: 0.6273 - val_accuracy: 0.7830\n",
      "Epoch 245/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7261 - accuracy: 0.7479 - val_loss: 0.6219 - val_accuracy: 0.7845\n",
      "Epoch 246/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7332 - accuracy: 0.7433 - val_loss: 0.6248 - val_accuracy: 0.7813\n",
      "Epoch 247/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7262 - accuracy: 0.7486 - val_loss: 0.6137 - val_accuracy: 0.7862\n",
      "Epoch 248/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7311 - accuracy: 0.7442 - val_loss: 0.6089 - val_accuracy: 0.7876\n",
      "Epoch 249/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7304 - accuracy: 0.7460 - val_loss: 0.6093 - val_accuracy: 0.7880\n",
      "Epoch 250/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7291 - accuracy: 0.7466 - val_loss: 0.6109 - val_accuracy: 0.7881\n",
      "Epoch 251/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7342 - accuracy: 0.7459 - val_loss: 0.6313 - val_accuracy: 0.7807\n",
      "Epoch 252/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7255 - accuracy: 0.7471 - val_loss: 0.6331 - val_accuracy: 0.7782\n",
      "Epoch 253/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7219 - accuracy: 0.7490 - val_loss: 0.6133 - val_accuracy: 0.7884\n",
      "Epoch 254/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7226 - accuracy: 0.7476 - val_loss: 0.6365 - val_accuracy: 0.7773\n",
      "Epoch 255/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7267 - accuracy: 0.7487 - val_loss: 0.6137 - val_accuracy: 0.7873\n",
      "Epoch 256/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7227 - accuracy: 0.7478 - val_loss: 0.6219 - val_accuracy: 0.7854\n",
      "Epoch 257/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7228 - accuracy: 0.7480 - val_loss: 0.6268 - val_accuracy: 0.7826\n",
      "Epoch 258/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7165 - accuracy: 0.7529 - val_loss: 0.6044 - val_accuracy: 0.7897\n",
      "Epoch 259/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7239 - accuracy: 0.7489 - val_loss: 0.6098 - val_accuracy: 0.7894\n",
      "Epoch 260/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7222 - accuracy: 0.7473 - val_loss: 0.6132 - val_accuracy: 0.7883\n",
      "Epoch 261/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7192 - accuracy: 0.7502 - val_loss: 0.6142 - val_accuracy: 0.7877\n",
      "Epoch 262/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7150 - accuracy: 0.7528 - val_loss: 0.6147 - val_accuracy: 0.7887\n",
      "Epoch 263/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7155 - accuracy: 0.7519 - val_loss: 0.6338 - val_accuracy: 0.7808\n",
      "Epoch 264/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7143 - accuracy: 0.7524 - val_loss: 0.6139 - val_accuracy: 0.7875\n",
      "Epoch 265/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7188 - accuracy: 0.7517 - val_loss: 0.6295 - val_accuracy: 0.7831\n",
      "Epoch 266/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7149 - accuracy: 0.7510 - val_loss: 0.5942 - val_accuracy: 0.7951\n",
      "Epoch 267/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7120 - accuracy: 0.7526 - val_loss: 0.6080 - val_accuracy: 0.7901\n",
      "Epoch 268/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7138 - accuracy: 0.7513 - val_loss: 0.6194 - val_accuracy: 0.7841\n",
      "Epoch 269/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7138 - accuracy: 0.7522 - val_loss: 0.6151 - val_accuracy: 0.7838\n",
      "Epoch 270/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7152 - accuracy: 0.7507 - val_loss: 0.6028 - val_accuracy: 0.7943\n",
      "Epoch 271/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7164 - accuracy: 0.7518 - val_loss: 0.6036 - val_accuracy: 0.7921\n",
      "Epoch 272/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7135 - accuracy: 0.7527 - val_loss: 0.6305 - val_accuracy: 0.7809\n",
      "Epoch 273/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7118 - accuracy: 0.7527 - val_loss: 0.6306 - val_accuracy: 0.7812\n",
      "Epoch 274/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7130 - accuracy: 0.7549 - val_loss: 0.6283 - val_accuracy: 0.7807\n",
      "Epoch 275/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7110 - accuracy: 0.7518 - val_loss: 0.5928 - val_accuracy: 0.7935\n",
      "Epoch 276/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7121 - accuracy: 0.7523 - val_loss: 0.5979 - val_accuracy: 0.7953\n",
      "Epoch 277/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7117 - accuracy: 0.7526 - val_loss: 0.6482 - val_accuracy: 0.7740\n",
      "Epoch 278/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7021 - accuracy: 0.7572 - val_loss: 0.5990 - val_accuracy: 0.7934\n",
      "Epoch 279/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7083 - accuracy: 0.7552 - val_loss: 0.6074 - val_accuracy: 0.7887\n",
      "Epoch 280/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7073 - accuracy: 0.7539 - val_loss: 0.5987 - val_accuracy: 0.7945\n",
      "Epoch 281/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7075 - accuracy: 0.7541 - val_loss: 0.5867 - val_accuracy: 0.7960\n",
      "Epoch 282/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6989 - accuracy: 0.7587 - val_loss: 0.5976 - val_accuracy: 0.7916\n",
      "Epoch 283/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6998 - accuracy: 0.7560 - val_loss: 0.5927 - val_accuracy: 0.7933\n",
      "Epoch 284/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7016 - accuracy: 0.7564 - val_loss: 0.6031 - val_accuracy: 0.7889\n",
      "Epoch 285/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7020 - accuracy: 0.7567 - val_loss: 0.6080 - val_accuracy: 0.7893\n",
      "Epoch 286/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6994 - accuracy: 0.7554 - val_loss: 0.6191 - val_accuracy: 0.7829\n",
      "Epoch 287/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6922 - accuracy: 0.7594 - val_loss: 0.6210 - val_accuracy: 0.7871\n",
      "Epoch 288/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6994 - accuracy: 0.7568 - val_loss: 0.6242 - val_accuracy: 0.7833\n",
      "Epoch 289/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7034 - accuracy: 0.7563 - val_loss: 0.6044 - val_accuracy: 0.7879\n",
      "Epoch 290/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6989 - accuracy: 0.7584 - val_loss: 0.5925 - val_accuracy: 0.7932\n",
      "Epoch 291/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7001 - accuracy: 0.7555 - val_loss: 0.5978 - val_accuracy: 0.7916\n",
      "Epoch 292/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7006 - accuracy: 0.7572 - val_loss: 0.5936 - val_accuracy: 0.7936\n",
      "Epoch 293/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6961 - accuracy: 0.7589 - val_loss: 0.5879 - val_accuracy: 0.7953\n",
      "Epoch 294/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6986 - accuracy: 0.7572 - val_loss: 0.5933 - val_accuracy: 0.7923\n",
      "Epoch 295/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6994 - accuracy: 0.7566 - val_loss: 0.6112 - val_accuracy: 0.7841\n",
      "Epoch 296/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6987 - accuracy: 0.7579 - val_loss: 0.5828 - val_accuracy: 0.8020\n",
      "Epoch 297/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.7004 - accuracy: 0.7559 - val_loss: 0.6173 - val_accuracy: 0.7862\n",
      "Epoch 298/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6927 - accuracy: 0.7603 - val_loss: 0.5964 - val_accuracy: 0.7929\n",
      "Epoch 299/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6924 - accuracy: 0.7607 - val_loss: 0.5886 - val_accuracy: 0.7959\n",
      "Epoch 300/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6952 - accuracy: 0.7591 - val_loss: 0.5956 - val_accuracy: 0.7951\n",
      "Epoch 301/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6991 - accuracy: 0.7588 - val_loss: 0.6018 - val_accuracy: 0.7913\n",
      "Epoch 302/400\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.6967 - accuracy: 0.7586 - val_loss: 0.5976 - val_accuracy: 0.7910\n",
      "Epoch 303/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6863 - accuracy: 0.7606 - val_loss: 0.5981 - val_accuracy: 0.7929\n",
      "Epoch 304/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6968 - accuracy: 0.7572 - val_loss: 0.5935 - val_accuracy: 0.7939\n",
      "Epoch 305/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6913 - accuracy: 0.7605 - val_loss: 0.6004 - val_accuracy: 0.7942\n",
      "Epoch 306/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6948 - accuracy: 0.7606 - val_loss: 0.5922 - val_accuracy: 0.7959\n",
      "Epoch 307/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6904 - accuracy: 0.7611 - val_loss: 0.5922 - val_accuracy: 0.7928\n",
      "Epoch 308/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6866 - accuracy: 0.7624 - val_loss: 0.5886 - val_accuracy: 0.7928\n",
      "Epoch 309/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6868 - accuracy: 0.7614 - val_loss: 0.5932 - val_accuracy: 0.7932\n",
      "Epoch 310/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6935 - accuracy: 0.7591 - val_loss: 0.6108 - val_accuracy: 0.7894\n",
      "Epoch 311/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6880 - accuracy: 0.7620 - val_loss: 0.5901 - val_accuracy: 0.7968\n",
      "Epoch 312/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6877 - accuracy: 0.7621 - val_loss: 0.5887 - val_accuracy: 0.7954\n",
      "Epoch 313/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6841 - accuracy: 0.7625 - val_loss: 0.5993 - val_accuracy: 0.7908\n",
      "Epoch 314/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6910 - accuracy: 0.7615 - val_loss: 0.5923 - val_accuracy: 0.7955\n",
      "Epoch 315/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6874 - accuracy: 0.7627 - val_loss: 0.5764 - val_accuracy: 0.8028\n",
      "Epoch 316/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6847 - accuracy: 0.7631 - val_loss: 0.5730 - val_accuracy: 0.8035\n",
      "Epoch 317/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6923 - accuracy: 0.7589 - val_loss: 0.6060 - val_accuracy: 0.7878\n",
      "Epoch 318/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6861 - accuracy: 0.7619 - val_loss: 0.5889 - val_accuracy: 0.7970\n",
      "Epoch 319/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6860 - accuracy: 0.7635 - val_loss: 0.5836 - val_accuracy: 0.7987\n",
      "Epoch 320/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6805 - accuracy: 0.7644 - val_loss: 0.5727 - val_accuracy: 0.8028\n",
      "Epoch 321/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6865 - accuracy: 0.7587 - val_loss: 0.5753 - val_accuracy: 0.8003\n",
      "Epoch 322/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6808 - accuracy: 0.7631 - val_loss: 0.5861 - val_accuracy: 0.7968\n",
      "Epoch 323/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6766 - accuracy: 0.7646 - val_loss: 0.5785 - val_accuracy: 0.8001\n",
      "Epoch 324/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6799 - accuracy: 0.7639 - val_loss: 0.5946 - val_accuracy: 0.7977\n",
      "Epoch 325/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6812 - accuracy: 0.7657 - val_loss: 0.5722 - val_accuracy: 0.8038\n",
      "Epoch 326/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6829 - accuracy: 0.7615 - val_loss: 0.5843 - val_accuracy: 0.7983\n",
      "Epoch 327/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6809 - accuracy: 0.7637 - val_loss: 0.5951 - val_accuracy: 0.7938\n",
      "Epoch 328/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6786 - accuracy: 0.7661 - val_loss: 0.5942 - val_accuracy: 0.7942\n",
      "Epoch 329/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6822 - accuracy: 0.7628 - val_loss: 0.6132 - val_accuracy: 0.7854\n",
      "Epoch 330/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6811 - accuracy: 0.7648 - val_loss: 0.5938 - val_accuracy: 0.7938\n",
      "Epoch 331/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6792 - accuracy: 0.7638 - val_loss: 0.5770 - val_accuracy: 0.7982\n",
      "Epoch 332/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6808 - accuracy: 0.7636 - val_loss: 0.5846 - val_accuracy: 0.7961\n",
      "Epoch 333/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6772 - accuracy: 0.7648 - val_loss: 0.6023 - val_accuracy: 0.7903\n",
      "Epoch 334/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6779 - accuracy: 0.7637 - val_loss: 0.5718 - val_accuracy: 0.8022\n",
      "Epoch 335/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6820 - accuracy: 0.7640 - val_loss: 0.5738 - val_accuracy: 0.8010\n",
      "Epoch 336/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6718 - accuracy: 0.7682 - val_loss: 0.5782 - val_accuracy: 0.7998\n",
      "Epoch 337/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6767 - accuracy: 0.7643 - val_loss: 0.5677 - val_accuracy: 0.8041\n",
      "Epoch 338/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6702 - accuracy: 0.7660 - val_loss: 0.6154 - val_accuracy: 0.7863\n",
      "Epoch 339/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6766 - accuracy: 0.7646 - val_loss: 0.5856 - val_accuracy: 0.7996\n",
      "Epoch 340/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6760 - accuracy: 0.7656 - val_loss: 0.5705 - val_accuracy: 0.8045\n",
      "Epoch 341/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6796 - accuracy: 0.7649 - val_loss: 0.5883 - val_accuracy: 0.7971\n",
      "Epoch 342/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6723 - accuracy: 0.7659 - val_loss: 0.5764 - val_accuracy: 0.8024\n",
      "Epoch 343/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6737 - accuracy: 0.7668 - val_loss: 0.5888 - val_accuracy: 0.7965\n",
      "Epoch 344/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6744 - accuracy: 0.7677 - val_loss: 0.5615 - val_accuracy: 0.8041\n",
      "Epoch 345/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6713 - accuracy: 0.7687 - val_loss: 0.5680 - val_accuracy: 0.8029\n",
      "Epoch 346/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6742 - accuracy: 0.7667 - val_loss: 0.5726 - val_accuracy: 0.8027\n",
      "Epoch 347/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6632 - accuracy: 0.7701 - val_loss: 0.5844 - val_accuracy: 0.7970\n",
      "Epoch 348/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6739 - accuracy: 0.7683 - val_loss: 0.5681 - val_accuracy: 0.8021\n",
      "Epoch 349/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6701 - accuracy: 0.7667 - val_loss: 0.5614 - val_accuracy: 0.8065\n",
      "Epoch 350/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6719 - accuracy: 0.7670 - val_loss: 0.5762 - val_accuracy: 0.8033\n",
      "Epoch 351/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6729 - accuracy: 0.7661 - val_loss: 0.5840 - val_accuracy: 0.7963\n",
      "Epoch 352/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6666 - accuracy: 0.7704 - val_loss: 0.5860 - val_accuracy: 0.7935\n",
      "Epoch 353/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6713 - accuracy: 0.7660 - val_loss: 0.5743 - val_accuracy: 0.7983\n",
      "Epoch 354/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6663 - accuracy: 0.7672 - val_loss: 0.5861 - val_accuracy: 0.7963\n",
      "Epoch 355/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6713 - accuracy: 0.7665 - val_loss: 0.5588 - val_accuracy: 0.8059\n",
      "Epoch 356/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6650 - accuracy: 0.7673 - val_loss: 0.5802 - val_accuracy: 0.7997\n",
      "Epoch 357/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6634 - accuracy: 0.7707 - val_loss: 0.5622 - val_accuracy: 0.8071\n",
      "Epoch 358/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6711 - accuracy: 0.7662 - val_loss: 0.5684 - val_accuracy: 0.8048\n",
      "Epoch 359/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6654 - accuracy: 0.7695 - val_loss: 0.5836 - val_accuracy: 0.7981\n",
      "Epoch 360/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6681 - accuracy: 0.7677 - val_loss: 0.5796 - val_accuracy: 0.7976\n",
      "Epoch 361/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6676 - accuracy: 0.7691 - val_loss: 0.5825 - val_accuracy: 0.7996\n",
      "Epoch 362/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6627 - accuracy: 0.7691 - val_loss: 0.5819 - val_accuracy: 0.7996\n",
      "Epoch 363/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6629 - accuracy: 0.7689 - val_loss: 0.5620 - val_accuracy: 0.8081\n",
      "Epoch 364/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6674 - accuracy: 0.7708 - val_loss: 0.5686 - val_accuracy: 0.8042\n",
      "Epoch 365/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6605 - accuracy: 0.7701 - val_loss: 0.5972 - val_accuracy: 0.7928\n",
      "Epoch 366/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6660 - accuracy: 0.7681 - val_loss: 0.5840 - val_accuracy: 0.8005\n",
      "Epoch 367/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6666 - accuracy: 0.7693 - val_loss: 0.6002 - val_accuracy: 0.7928\n",
      "Epoch 368/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6619 - accuracy: 0.7695 - val_loss: 0.5854 - val_accuracy: 0.7957\n",
      "Epoch 369/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6598 - accuracy: 0.7718 - val_loss: 0.5578 - val_accuracy: 0.8052\n",
      "Epoch 370/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6676 - accuracy: 0.7671 - val_loss: 0.5893 - val_accuracy: 0.7944\n",
      "Epoch 371/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6580 - accuracy: 0.7709 - val_loss: 0.5678 - val_accuracy: 0.8045\n",
      "Epoch 372/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6656 - accuracy: 0.7700 - val_loss: 0.5715 - val_accuracy: 0.8026\n",
      "Epoch 373/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6582 - accuracy: 0.7729 - val_loss: 0.5613 - val_accuracy: 0.8060\n",
      "Epoch 374/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6586 - accuracy: 0.7695 - val_loss: 0.5862 - val_accuracy: 0.7969\n",
      "Epoch 375/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6599 - accuracy: 0.7710 - val_loss: 0.5646 - val_accuracy: 0.8046\n",
      "Epoch 376/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6608 - accuracy: 0.7716 - val_loss: 0.5705 - val_accuracy: 0.8035\n",
      "Epoch 377/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6638 - accuracy: 0.7704 - val_loss: 0.5742 - val_accuracy: 0.8026\n",
      "Epoch 378/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6611 - accuracy: 0.7714 - val_loss: 0.6022 - val_accuracy: 0.7905\n",
      "Epoch 379/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6554 - accuracy: 0.7723 - val_loss: 0.5606 - val_accuracy: 0.8070\n",
      "Epoch 380/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6562 - accuracy: 0.7729 - val_loss: 0.5564 - val_accuracy: 0.8085\n",
      "Epoch 381/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6583 - accuracy: 0.7690 - val_loss: 0.5680 - val_accuracy: 0.8049\n",
      "Epoch 382/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6549 - accuracy: 0.7732 - val_loss: 0.5643 - val_accuracy: 0.8029\n",
      "Epoch 383/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6538 - accuracy: 0.7742 - val_loss: 0.5771 - val_accuracy: 0.7991\n",
      "Epoch 384/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6524 - accuracy: 0.7751 - val_loss: 0.5664 - val_accuracy: 0.8030\n",
      "Epoch 385/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6557 - accuracy: 0.7707 - val_loss: 0.5739 - val_accuracy: 0.8028\n",
      "Epoch 386/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6563 - accuracy: 0.7714 - val_loss: 0.5705 - val_accuracy: 0.8041\n",
      "Epoch 387/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6508 - accuracy: 0.7739 - val_loss: 0.5558 - val_accuracy: 0.8080\n",
      "Epoch 388/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6533 - accuracy: 0.7727 - val_loss: 0.5861 - val_accuracy: 0.7959\n",
      "Epoch 389/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6535 - accuracy: 0.7720 - val_loss: 0.5800 - val_accuracy: 0.8006\n",
      "Epoch 390/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6496 - accuracy: 0.7732 - val_loss: 0.5735 - val_accuracy: 0.8035\n",
      "Epoch 391/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6518 - accuracy: 0.7732 - val_loss: 0.5572 - val_accuracy: 0.8065\n",
      "Epoch 392/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6585 - accuracy: 0.7722 - val_loss: 0.5609 - val_accuracy: 0.8057\n",
      "Epoch 393/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6496 - accuracy: 0.7737 - val_loss: 0.5532 - val_accuracy: 0.8105\n",
      "Epoch 394/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6578 - accuracy: 0.7717 - val_loss: 0.5696 - val_accuracy: 0.8034\n",
      "Epoch 395/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6515 - accuracy: 0.7727 - val_loss: 0.5593 - val_accuracy: 0.8063\n",
      "Epoch 396/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6579 - accuracy: 0.7726 - val_loss: 0.5647 - val_accuracy: 0.8065\n",
      "Epoch 397/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6459 - accuracy: 0.7749 - val_loss: 0.5677 - val_accuracy: 0.8038\n",
      "Epoch 398/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6472 - accuracy: 0.7762 - val_loss: 0.5585 - val_accuracy: 0.8076\n",
      "Epoch 399/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6477 - accuracy: 0.7746 - val_loss: 0.5691 - val_accuracy: 0.8024\n",
      "Epoch 400/400\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.6479 - accuracy: 0.7747 - val_loss: 0.5617 - val_accuracy: 0.8058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84a95faf40>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', \n",
    "                    input_shape=(32, 32, 3)))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.2))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.3))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((4, 4)))\n",
    "model_10.add(Dropout(0.4))\n",
    "model_10.add(Flatten())\n",
    "model_10.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model_10.add(Dropout(0.5))\n",
    "model_10.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model_10.summary()\n",
    "\n",
    "# define the ModelCheckpoint callback to save the model after each epoch\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_10_gpu_Base-CubeAI_3Conv.h5', save_freq='epoch')\n",
    "\n",
    "# compile model\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "model_10.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# Model Fit\n",
    "model_10.fit(x_train,y_train,batch_size=64, epochs=400, validation_data=(x_test, y_test),callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "514b4599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptj_jkdf_/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmptj_jkdf_/assets\n",
      "/home/themandalorian/anaconda3/envs/ml-on-mc/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-05-17 17:40:20.765482: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-05-17 17:40:20.765512: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-05-17 17:40:20.765665: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmptj_jkdf_\n",
      "2023-05-17 17:40:20.770867: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-17 17:40:20.770886: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmptj_jkdf_\n",
      "2023-05-17 17:40:20.788132: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-05-17 17:40:20.852408: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmptj_jkdf_\n",
      "2023-05-17 17:40:20.876846: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 111181 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_conv2d_49_input:0\n",
      "shape: [ 1 32 32  3]\n",
      "type: <class 'numpy.int8'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.int8'>\n",
      "Accuracy of reduced base quantized to int8 model is 80.44%\n"
     ]
    }
   ],
   "source": [
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_10)\n",
    "\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "tflite_model_name = 'model_10_gpu_Base-CubeAI_3Conv-PTQ'\n",
    "        \n",
    "# Set the optimization flag.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Enforce full-int8 quantization\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "# Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name + '.tflite')\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "predictions = np.zeros((len(x_test),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(len(x_test)):\n",
    "    val_batch = x_test[i]\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()\n",
    "    \n",
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 10000\n",
    "print(\"Accuracy of reduced base quantized to int8 model is {}%\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca6f42c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'cifar10_base_3conv_ptq'\n",
    "# check if dir 'cfiles' exists, if not create it\n",
    "if not os.path.exists('cfiles'):\n",
    "    os.makedirs('cfiles')\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7ddb3243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer_8 (QuantizeL  (None, 32, 32, 3)        3         \n",
      " ayer)                                                           \n",
      "                                                                 \n",
      " quant_conv2d_61 (QuantizeWr  (None, 32, 32, 32)       963       \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_42 (Qua  (None, 16, 16, 32)       1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_dropout_56 (QuantizeW  (None, 16, 16, 32)       1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_conv2d_62 (QuantizeWr  (None, 16, 16, 64)       18627     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_43 (Qua  (None, 8, 8, 64)         1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_dropout_57 (QuantizeW  (None, 8, 8, 64)         1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_conv2d_63 (QuantizeWr  (None, 8, 8, 128)        74115     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_44 (Qua  (None, 2, 2, 128)        1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_dropout_58 (QuantizeW  (None, 2, 2, 128)        1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_flatten_14 (QuantizeW  (None, 512)              1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_28 (QuantizeWra  (None, 128)              65669     \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dropout_59 (QuantizeW  (None, 128)              1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_29 (QuantizeWra  (None, 10)               1295      \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,680\n",
      "Trainable params: 160,202\n",
      "Non-trainable params: 478\n",
      "_________________________________________________________________\n",
      "Epoch 1/400\n",
      "782/782 [==============================] - 12s 13ms/step - loss: 2.3326 - accuracy: 0.1318 - val_loss: 2.2038 - val_accuracy: 0.2019\n",
      "Epoch 2/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 2.1377 - accuracy: 0.1951 - val_loss: 2.0637 - val_accuracy: 0.2628\n",
      "Epoch 3/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 2.0206 - accuracy: 0.2392 - val_loss: 1.9241 - val_accuracy: 0.3221\n",
      "Epoch 4/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.9096 - accuracy: 0.2784 - val_loss: 1.8388 - val_accuracy: 0.3296\n",
      "Epoch 5/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.8355 - accuracy: 0.3049 - val_loss: 1.7771 - val_accuracy: 0.3762\n",
      "Epoch 6/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.7868 - accuracy: 0.3232 - val_loss: 1.7025 - val_accuracy: 0.3989\n",
      "Epoch 7/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.7479 - accuracy: 0.3397 - val_loss: 1.6861 - val_accuracy: 0.3961\n",
      "Epoch 8/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.7118 - accuracy: 0.3524 - val_loss: 1.6470 - val_accuracy: 0.4216\n",
      "Epoch 9/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.6789 - accuracy: 0.3685 - val_loss: 1.6252 - val_accuracy: 0.4358\n",
      "Epoch 10/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.6480 - accuracy: 0.3827 - val_loss: 1.5572 - val_accuracy: 0.4532\n",
      "Epoch 11/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.6218 - accuracy: 0.3943 - val_loss: 1.5371 - val_accuracy: 0.4651\n",
      "Epoch 12/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.5955 - accuracy: 0.4060 - val_loss: 1.5196 - val_accuracy: 0.4759\n",
      "Epoch 13/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5715 - accuracy: 0.4165 - val_loss: 1.4952 - val_accuracy: 0.4792\n",
      "Epoch 14/400\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.5474 - accuracy: 0.4235 - val_loss: 1.4669 - val_accuracy: 0.4918\n",
      "Epoch 15/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.5270 - accuracy: 0.4323 - val_loss: 1.4521 - val_accuracy: 0.4896\n",
      "Epoch 16/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.5114 - accuracy: 0.4419 - val_loss: 1.4381 - val_accuracy: 0.4965\n",
      "Epoch 17/400\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.4976 - accuracy: 0.4457 - val_loss: 1.4059 - val_accuracy: 0.5077\n",
      "Epoch 18/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4766 - accuracy: 0.4563 - val_loss: 1.4030 - val_accuracy: 0.5129\n",
      "Epoch 19/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.4598 - accuracy: 0.4619 - val_loss: 1.3908 - val_accuracy: 0.5141\n",
      "Epoch 20/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4444 - accuracy: 0.4679 - val_loss: 1.3430 - val_accuracy: 0.5225\n",
      "Epoch 21/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.4288 - accuracy: 0.4743 - val_loss: 1.3426 - val_accuracy: 0.5333\n",
      "Epoch 22/400\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.4181 - accuracy: 0.4797 - val_loss: 1.3301 - val_accuracy: 0.5377\n",
      "Epoch 23/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.3993 - accuracy: 0.4868 - val_loss: 1.3084 - val_accuracy: 0.5394\n",
      "Epoch 24/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.3815 - accuracy: 0.4945 - val_loss: 1.2827 - val_accuracy: 0.5596\n",
      "Epoch 25/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.3714 - accuracy: 0.4978 - val_loss: 1.2936 - val_accuracy: 0.5508\n",
      "Epoch 26/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.3624 - accuracy: 0.5032 - val_loss: 1.2736 - val_accuracy: 0.5576\n",
      "Epoch 27/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.3482 - accuracy: 0.5086 - val_loss: 1.2351 - val_accuracy: 0.5798\n",
      "Epoch 28/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.3351 - accuracy: 0.5128 - val_loss: 1.2562 - val_accuracy: 0.5550\n",
      "Epoch 29/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.3224 - accuracy: 0.5209 - val_loss: 1.2078 - val_accuracy: 0.5915\n",
      "Epoch 30/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.3120 - accuracy: 0.5231 - val_loss: 1.2005 - val_accuracy: 0.5924\n",
      "Epoch 31/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.3044 - accuracy: 0.5269 - val_loss: 1.1856 - val_accuracy: 0.5945\n",
      "Epoch 32/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.2883 - accuracy: 0.5301 - val_loss: 1.2258 - val_accuracy: 0.5773\n",
      "Epoch 33/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.2803 - accuracy: 0.5372 - val_loss: 1.1923 - val_accuracy: 0.5776\n",
      "Epoch 34/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.2740 - accuracy: 0.5406 - val_loss: 1.1692 - val_accuracy: 0.6002\n",
      "Epoch 35/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 9s 12ms/step - loss: 1.2621 - accuracy: 0.5440 - val_loss: 1.1612 - val_accuracy: 0.6095\n",
      "Epoch 36/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.2489 - accuracy: 0.5489 - val_loss: 1.1636 - val_accuracy: 0.5932\n",
      "Epoch 37/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.2448 - accuracy: 0.5492 - val_loss: 1.1526 - val_accuracy: 0.5989\n",
      "Epoch 38/400\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.2350 - accuracy: 0.5545 - val_loss: 1.1312 - val_accuracy: 0.6096\n",
      "Epoch 39/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.2275 - accuracy: 0.5579 - val_loss: 1.1239 - val_accuracy: 0.6158\n",
      "Epoch 40/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.2220 - accuracy: 0.5629 - val_loss: 1.0980 - val_accuracy: 0.6270\n",
      "Epoch 41/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.2126 - accuracy: 0.5626 - val_loss: 1.1197 - val_accuracy: 0.6186\n",
      "Epoch 42/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.2095 - accuracy: 0.5656 - val_loss: 1.0807 - val_accuracy: 0.6307\n",
      "Epoch 43/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.1963 - accuracy: 0.5693 - val_loss: 1.0726 - val_accuracy: 0.6354\n",
      "Epoch 44/400\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.1917 - accuracy: 0.5729 - val_loss: 1.1202 - val_accuracy: 0.6047\n",
      "Epoch 45/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.1820 - accuracy: 0.5729 - val_loss: 1.1052 - val_accuracy: 0.6126\n",
      "Epoch 46/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.1804 - accuracy: 0.5754 - val_loss: 1.0490 - val_accuracy: 0.6406\n",
      "Epoch 47/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.1653 - accuracy: 0.5839 - val_loss: 1.0968 - val_accuracy: 0.6150\n",
      "Epoch 48/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.1603 - accuracy: 0.5846 - val_loss: 1.0577 - val_accuracy: 0.6328\n",
      "Epoch 49/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.1547 - accuracy: 0.5883 - val_loss: 1.0416 - val_accuracy: 0.6365\n",
      "Epoch 50/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.1544 - accuracy: 0.5860 - val_loss: 1.1105 - val_accuracy: 0.6120\n",
      "Epoch 51/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.1427 - accuracy: 0.5936 - val_loss: 1.0307 - val_accuracy: 0.6451\n",
      "Epoch 52/400\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.1392 - accuracy: 0.5936 - val_loss: 1.0223 - val_accuracy: 0.6466\n",
      "Epoch 53/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.1316 - accuracy: 0.5946 - val_loss: 1.0212 - val_accuracy: 0.6406\n",
      "Epoch 54/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.1281 - accuracy: 0.5981 - val_loss: 1.0181 - val_accuracy: 0.6410\n",
      "Epoch 55/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.1232 - accuracy: 0.5989 - val_loss: 0.9883 - val_accuracy: 0.6633\n",
      "Epoch 56/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.1168 - accuracy: 0.6005 - val_loss: 1.0310 - val_accuracy: 0.6408\n",
      "Epoch 57/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.1116 - accuracy: 0.6017 - val_loss: 0.9979 - val_accuracy: 0.6524\n",
      "Epoch 58/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.1032 - accuracy: 0.6073 - val_loss: 1.0122 - val_accuracy: 0.6427\n",
      "Epoch 59/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0958 - accuracy: 0.6088 - val_loss: 0.9832 - val_accuracy: 0.6555\n",
      "Epoch 60/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0944 - accuracy: 0.6102 - val_loss: 0.9729 - val_accuracy: 0.6581\n",
      "Epoch 61/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0876 - accuracy: 0.6120 - val_loss: 0.9816 - val_accuracy: 0.6617\n",
      "Epoch 62/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0848 - accuracy: 0.6130 - val_loss: 0.9725 - val_accuracy: 0.6583\n",
      "Epoch 63/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0782 - accuracy: 0.6181 - val_loss: 0.9477 - val_accuracy: 0.6712\n",
      "Epoch 64/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0755 - accuracy: 0.6183 - val_loss: 0.9432 - val_accuracy: 0.6664\n",
      "Epoch 65/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0751 - accuracy: 0.6198 - val_loss: 0.9686 - val_accuracy: 0.6550\n",
      "Epoch 66/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0628 - accuracy: 0.6209 - val_loss: 0.9265 - val_accuracy: 0.6765\n",
      "Epoch 67/400\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.0574 - accuracy: 0.6249 - val_loss: 0.9258 - val_accuracy: 0.6727\n",
      "Epoch 68/400\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.0574 - accuracy: 0.6228 - val_loss: 0.9199 - val_accuracy: 0.6806\n",
      "Epoch 69/400\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 1.0557 - accuracy: 0.6275 - val_loss: 0.9686 - val_accuracy: 0.6602\n",
      "Epoch 70/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0477 - accuracy: 0.6278 - val_loss: 0.9089 - val_accuracy: 0.6883\n",
      "Epoch 71/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0402 - accuracy: 0.6320 - val_loss: 0.9161 - val_accuracy: 0.6771\n",
      "Epoch 72/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0378 - accuracy: 0.6318 - val_loss: 0.9496 - val_accuracy: 0.6673\n",
      "Epoch 73/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.0325 - accuracy: 0.6322 - val_loss: 0.8968 - val_accuracy: 0.6932\n",
      "Epoch 74/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.0264 - accuracy: 0.6349 - val_loss: 0.8958 - val_accuracy: 0.6916\n",
      "Epoch 75/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0253 - accuracy: 0.6369 - val_loss: 0.9030 - val_accuracy: 0.6908\n",
      "Epoch 76/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.0237 - accuracy: 0.6346 - val_loss: 0.9075 - val_accuracy: 0.6847\n",
      "Epoch 77/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.0229 - accuracy: 0.6377 - val_loss: 0.9942 - val_accuracy: 0.6410\n",
      "Epoch 78/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0157 - accuracy: 0.6401 - val_loss: 0.9332 - val_accuracy: 0.6709\n",
      "Epoch 79/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0179 - accuracy: 0.6377 - val_loss: 0.9028 - val_accuracy: 0.6889\n",
      "Epoch 80/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0051 - accuracy: 0.6460 - val_loss: 0.9331 - val_accuracy: 0.6741\n",
      "Epoch 81/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 1.0066 - accuracy: 0.6427 - val_loss: 0.8847 - val_accuracy: 0.6956\n",
      "Epoch 82/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 1.0032 - accuracy: 0.6461 - val_loss: 0.9455 - val_accuracy: 0.6628\n",
      "Epoch 83/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9989 - accuracy: 0.6490 - val_loss: 0.8808 - val_accuracy: 0.7004\n",
      "Epoch 84/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.9958 - accuracy: 0.6480 - val_loss: 0.8878 - val_accuracy: 0.6892\n",
      "Epoch 85/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9934 - accuracy: 0.6485 - val_loss: 0.8478 - val_accuracy: 0.7046\n",
      "Epoch 86/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9852 - accuracy: 0.6511 - val_loss: 0.8697 - val_accuracy: 0.6985\n",
      "Epoch 87/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.9806 - accuracy: 0.6535 - val_loss: 0.8729 - val_accuracy: 0.6971\n",
      "Epoch 88/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9812 - accuracy: 0.6545 - val_loss: 0.8933 - val_accuracy: 0.6797\n",
      "Epoch 89/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9810 - accuracy: 0.6560 - val_loss: 0.8595 - val_accuracy: 0.7006\n",
      "Epoch 90/400\n",
      "782/782 [==============================] - 10s 12ms/step - loss: 0.9772 - accuracy: 0.6548 - val_loss: 0.8384 - val_accuracy: 0.7106\n",
      "Epoch 91/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9728 - accuracy: 0.6578 - val_loss: 0.8339 - val_accuracy: 0.7102\n",
      "Epoch 92/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9697 - accuracy: 0.6575 - val_loss: 0.8652 - val_accuracy: 0.7005\n",
      "Epoch 93/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9657 - accuracy: 0.6574 - val_loss: 0.8385 - val_accuracy: 0.7105\n",
      "Epoch 94/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9607 - accuracy: 0.6626 - val_loss: 0.8579 - val_accuracy: 0.6984\n",
      "Epoch 95/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9598 - accuracy: 0.6619 - val_loss: 0.8605 - val_accuracy: 0.7007\n",
      "Epoch 96/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9537 - accuracy: 0.6649 - val_loss: 0.8396 - val_accuracy: 0.7102\n",
      "Epoch 97/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9542 - accuracy: 0.6633 - val_loss: 0.8135 - val_accuracy: 0.7185\n",
      "Epoch 98/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9452 - accuracy: 0.6677 - val_loss: 0.8562 - val_accuracy: 0.6980\n",
      "Epoch 99/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9497 - accuracy: 0.6675 - val_loss: 0.8120 - val_accuracy: 0.7200\n",
      "Epoch 100/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9445 - accuracy: 0.6680 - val_loss: 0.8214 - val_accuracy: 0.7125\n",
      "Epoch 101/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9413 - accuracy: 0.6683 - val_loss: 0.8089 - val_accuracy: 0.7208\n",
      "Epoch 102/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9370 - accuracy: 0.6706 - val_loss: 0.8123 - val_accuracy: 0.7179\n",
      "Epoch 103/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9341 - accuracy: 0.6726 - val_loss: 0.8495 - val_accuracy: 0.6967\n",
      "Epoch 104/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9343 - accuracy: 0.6714 - val_loss: 0.8182 - val_accuracy: 0.7117\n",
      "Epoch 105/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9286 - accuracy: 0.6735 - val_loss: 0.8032 - val_accuracy: 0.7207\n",
      "Epoch 106/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9261 - accuracy: 0.6740 - val_loss: 0.8139 - val_accuracy: 0.7122\n",
      "Epoch 107/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9277 - accuracy: 0.6739 - val_loss: 0.8233 - val_accuracy: 0.7148\n",
      "Epoch 108/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9176 - accuracy: 0.6783 - val_loss: 0.7979 - val_accuracy: 0.7200\n",
      "Epoch 109/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9210 - accuracy: 0.6770 - val_loss: 0.8236 - val_accuracy: 0.7104\n",
      "Epoch 110/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9191 - accuracy: 0.6768 - val_loss: 0.7830 - val_accuracy: 0.7301\n",
      "Epoch 111/400\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.9214 - accuracy: 0.6765 - val_loss: 0.7846 - val_accuracy: 0.7269\n",
      "Epoch 112/400\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.9140 - accuracy: 0.6778 - val_loss: 0.7804 - val_accuracy: 0.7284\n",
      "Epoch 113/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9121 - accuracy: 0.6793 - val_loss: 0.7865 - val_accuracy: 0.7279\n",
      "Epoch 114/400\n",
      "782/782 [==============================] - 7s 10ms/step - loss: 0.9106 - accuracy: 0.6800 - val_loss: 0.7767 - val_accuracy: 0.7354\n",
      "Epoch 115/400\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.9047 - accuracy: 0.6818 - val_loss: 0.7768 - val_accuracy: 0.7263\n",
      "Epoch 116/400\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.9091 - accuracy: 0.6800 - val_loss: 0.7629 - val_accuracy: 0.7369\n",
      "Epoch 117/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.9054 - accuracy: 0.6822 - val_loss: 0.7782 - val_accuracy: 0.7285\n",
      "Epoch 118/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8984 - accuracy: 0.6836 - val_loss: 0.8040 - val_accuracy: 0.7191\n",
      "Epoch 119/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8956 - accuracy: 0.6878 - val_loss: 0.7815 - val_accuracy: 0.7252\n",
      "Epoch 120/400\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.8924 - accuracy: 0.6868 - val_loss: 0.7714 - val_accuracy: 0.7311\n",
      "Epoch 121/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8939 - accuracy: 0.6851 - val_loss: 0.7622 - val_accuracy: 0.7369\n",
      "Epoch 122/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8890 - accuracy: 0.6884 - val_loss: 0.7865 - val_accuracy: 0.7201\n",
      "Epoch 123/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8900 - accuracy: 0.6881 - val_loss: 0.8062 - val_accuracy: 0.7155\n",
      "Epoch 124/400\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.8874 - accuracy: 0.6887 - val_loss: 0.7607 - val_accuracy: 0.7356\n",
      "Epoch 125/400\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.8851 - accuracy: 0.6907 - val_loss: 0.7594 - val_accuracy: 0.7341\n",
      "Epoch 126/400\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.8790 - accuracy: 0.6908 - val_loss: 0.7913 - val_accuracy: 0.7280\n",
      "Epoch 127/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8797 - accuracy: 0.6910 - val_loss: 0.7972 - val_accuracy: 0.7188\n",
      "Epoch 128/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8832 - accuracy: 0.6922 - val_loss: 0.7558 - val_accuracy: 0.7346\n",
      "Epoch 129/400\n",
      "782/782 [==============================] - 11s 14ms/step - loss: 0.8791 - accuracy: 0.6919 - val_loss: 0.7439 - val_accuracy: 0.7386\n",
      "Epoch 130/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.8724 - accuracy: 0.6933 - val_loss: 0.7423 - val_accuracy: 0.7420\n",
      "Epoch 131/400\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.8696 - accuracy: 0.6956 - val_loss: 0.7785 - val_accuracy: 0.7320\n",
      "Epoch 132/400\n",
      "782/782 [==============================] - 7s 10ms/step - loss: 0.8699 - accuracy: 0.6963 - val_loss: 0.7605 - val_accuracy: 0.7348\n",
      "Epoch 133/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8699 - accuracy: 0.6963 - val_loss: 0.7658 - val_accuracy: 0.7310\n",
      "Epoch 134/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8682 - accuracy: 0.6973 - val_loss: 0.7451 - val_accuracy: 0.7365\n",
      "Epoch 135/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8658 - accuracy: 0.6954 - val_loss: 0.7669 - val_accuracy: 0.7274\n",
      "Epoch 136/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8660 - accuracy: 0.6966 - val_loss: 0.7482 - val_accuracy: 0.7359\n",
      "Epoch 137/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8622 - accuracy: 0.6985 - val_loss: 0.7284 - val_accuracy: 0.7447\n",
      "Epoch 138/400\n",
      "782/782 [==============================] - 10s 13ms/step - loss: 0.8606 - accuracy: 0.6990 - val_loss: 0.7510 - val_accuracy: 0.7373\n",
      "Epoch 139/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8564 - accuracy: 0.6988 - val_loss: 0.7987 - val_accuracy: 0.7163\n",
      "Epoch 140/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8558 - accuracy: 0.7013 - val_loss: 0.7576 - val_accuracy: 0.7365\n",
      "Epoch 141/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.8581 - accuracy: 0.6989 - val_loss: 0.7366 - val_accuracy: 0.7389\n",
      "Epoch 142/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8542 - accuracy: 0.7032 - val_loss: 0.7528 - val_accuracy: 0.7343\n",
      "Epoch 143/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8513 - accuracy: 0.7015 - val_loss: 0.7386 - val_accuracy: 0.7403\n",
      "Epoch 144/400\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.8467 - accuracy: 0.7044 - val_loss: 0.7351 - val_accuracy: 0.7438\n",
      "Epoch 145/400\n",
      "782/782 [==============================] - 7s 9ms/step - loss: 0.8460 - accuracy: 0.7039 - val_loss: 0.7187 - val_accuracy: 0.7479\n",
      "Epoch 146/400\n",
      "782/782 [==============================] - 9s 12ms/step - loss: 0.8439 - accuracy: 0.7058 - val_loss: 0.7181 - val_accuracy: 0.7479\n",
      "Epoch 147/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8442 - accuracy: 0.7054 - val_loss: 0.7236 - val_accuracy: 0.7471\n",
      "Epoch 148/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.8410 - accuracy: 0.7061 - val_loss: 0.7207 - val_accuracy: 0.7479\n",
      "Epoch 149/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8377 - accuracy: 0.7082 - val_loss: 0.7016 - val_accuracy: 0.7551\n",
      "Epoch 150/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8393 - accuracy: 0.7040 - val_loss: 0.7349 - val_accuracy: 0.7409\n",
      "Epoch 151/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8362 - accuracy: 0.7058 - val_loss: 0.6947 - val_accuracy: 0.7586\n",
      "Epoch 152/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8326 - accuracy: 0.7090 - val_loss: 0.7466 - val_accuracy: 0.7360\n",
      "Epoch 153/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8368 - accuracy: 0.7065 - val_loss: 0.7144 - val_accuracy: 0.7478\n",
      "Epoch 154/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.8324 - accuracy: 0.7099 - val_loss: 0.7368 - val_accuracy: 0.7395\n",
      "Epoch 155/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8323 - accuracy: 0.7100 - val_loss: 0.7069 - val_accuracy: 0.7559\n",
      "Epoch 156/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8331 - accuracy: 0.7097 - val_loss: 0.7086 - val_accuracy: 0.7506\n",
      "Epoch 157/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8294 - accuracy: 0.7114 - val_loss: 0.6905 - val_accuracy: 0.7642\n",
      "Epoch 158/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8276 - accuracy: 0.7124 - val_loss: 0.7212 - val_accuracy: 0.7472\n",
      "Epoch 159/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8243 - accuracy: 0.7128 - val_loss: 0.7061 - val_accuracy: 0.7537\n",
      "Epoch 160/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.8255 - accuracy: 0.7113 - val_loss: 0.7779 - val_accuracy: 0.7261\n",
      "Epoch 161/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8236 - accuracy: 0.7127 - val_loss: 0.7109 - val_accuracy: 0.7515\n",
      "Epoch 162/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8191 - accuracy: 0.7131 - val_loss: 0.6979 - val_accuracy: 0.7559\n",
      "Epoch 163/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8210 - accuracy: 0.7151 - val_loss: 0.6935 - val_accuracy: 0.7579\n",
      "Epoch 164/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8184 - accuracy: 0.7150 - val_loss: 0.6735 - val_accuracy: 0.7679\n",
      "Epoch 165/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.8152 - accuracy: 0.7165 - val_loss: 0.6999 - val_accuracy: 0.7558\n",
      "Epoch 166/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.8123 - accuracy: 0.7168 - val_loss: 0.6894 - val_accuracy: 0.7622\n",
      "Epoch 167/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.8132 - accuracy: 0.7159 - val_loss: 0.6867 - val_accuracy: 0.7630\n",
      "Epoch 168/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8128 - accuracy: 0.7157 - val_loss: 0.6710 - val_accuracy: 0.7674\n",
      "Epoch 169/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8068 - accuracy: 0.7194 - val_loss: 0.6911 - val_accuracy: 0.7617\n",
      "Epoch 170/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8139 - accuracy: 0.7184 - val_loss: 0.6799 - val_accuracy: 0.7633\n",
      "Epoch 171/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8129 - accuracy: 0.7164 - val_loss: 0.7157 - val_accuracy: 0.7496\n",
      "Epoch 172/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.8078 - accuracy: 0.7196 - val_loss: 0.7070 - val_accuracy: 0.7510\n",
      "Epoch 173/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8075 - accuracy: 0.7190 - val_loss: 0.6926 - val_accuracy: 0.7545\n",
      "Epoch 174/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8032 - accuracy: 0.7193 - val_loss: 0.7028 - val_accuracy: 0.7491\n",
      "Epoch 175/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7991 - accuracy: 0.7218 - val_loss: 0.6943 - val_accuracy: 0.7557\n",
      "Epoch 176/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8023 - accuracy: 0.7217 - val_loss: 0.6866 - val_accuracy: 0.7609\n",
      "Epoch 177/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.8010 - accuracy: 0.7201 - val_loss: 0.6870 - val_accuracy: 0.7603\n",
      "Epoch 178/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.7995 - accuracy: 0.7198 - val_loss: 0.7182 - val_accuracy: 0.7490\n",
      "Epoch 179/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7924 - accuracy: 0.7229 - val_loss: 0.6963 - val_accuracy: 0.7572\n",
      "Epoch 180/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7983 - accuracy: 0.7207 - val_loss: 0.6849 - val_accuracy: 0.7617\n",
      "Epoch 181/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7921 - accuracy: 0.7251 - val_loss: 0.6745 - val_accuracy: 0.7666\n",
      "Epoch 182/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7937 - accuracy: 0.7238 - val_loss: 0.6999 - val_accuracy: 0.7541\n",
      "Epoch 183/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7937 - accuracy: 0.7227 - val_loss: 0.6576 - val_accuracy: 0.7728\n",
      "Epoch 184/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.7914 - accuracy: 0.7239 - val_loss: 0.6907 - val_accuracy: 0.7548\n",
      "Epoch 185/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7864 - accuracy: 0.7247 - val_loss: 0.7037 - val_accuracy: 0.7521\n",
      "Epoch 186/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7898 - accuracy: 0.7265 - val_loss: 0.6760 - val_accuracy: 0.7612\n",
      "Epoch 187/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.7927 - accuracy: 0.7241 - val_loss: 0.6642 - val_accuracy: 0.7673\n",
      "Epoch 188/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7812 - accuracy: 0.7295 - val_loss: 0.6773 - val_accuracy: 0.7655\n",
      "Epoch 189/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.7816 - accuracy: 0.7275 - val_loss: 0.6796 - val_accuracy: 0.7604\n",
      "Epoch 190/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.7829 - accuracy: 0.7268 - val_loss: 0.6646 - val_accuracy: 0.7670\n",
      "Epoch 191/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7833 - accuracy: 0.7267 - val_loss: 0.6605 - val_accuracy: 0.7690\n",
      "Epoch 192/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7811 - accuracy: 0.7291 - val_loss: 0.6690 - val_accuracy: 0.7667\n",
      "Epoch 193/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7768 - accuracy: 0.7288 - val_loss: 0.6707 - val_accuracy: 0.7672\n",
      "Epoch 194/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7768 - accuracy: 0.7315 - val_loss: 0.6705 - val_accuracy: 0.7694\n",
      "Epoch 195/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7746 - accuracy: 0.7300 - val_loss: 0.6838 - val_accuracy: 0.7598\n",
      "Epoch 196/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.7758 - accuracy: 0.7311 - val_loss: 0.6774 - val_accuracy: 0.7629\n",
      "Epoch 197/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7744 - accuracy: 0.7316 - val_loss: 0.6615 - val_accuracy: 0.7671\n",
      "Epoch 198/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7686 - accuracy: 0.7311 - val_loss: 0.6447 - val_accuracy: 0.7753\n",
      "Epoch 199/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7688 - accuracy: 0.7304 - val_loss: 0.6475 - val_accuracy: 0.7751\n",
      "Epoch 200/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7714 - accuracy: 0.7306 - val_loss: 0.6633 - val_accuracy: 0.7678\n",
      "Epoch 201/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7703 - accuracy: 0.7345 - val_loss: 0.6488 - val_accuracy: 0.7775\n",
      "Epoch 202/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.7603 - accuracy: 0.7341 - val_loss: 0.6538 - val_accuracy: 0.7713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 203/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7684 - accuracy: 0.7310 - val_loss: 0.6639 - val_accuracy: 0.7690\n",
      "Epoch 204/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7673 - accuracy: 0.7334 - val_loss: 0.6429 - val_accuracy: 0.7778\n",
      "Epoch 205/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7668 - accuracy: 0.7337 - val_loss: 0.6405 - val_accuracy: 0.7781\n",
      "Epoch 206/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7627 - accuracy: 0.7339 - val_loss: 0.6603 - val_accuracy: 0.7710\n",
      "Epoch 207/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7646 - accuracy: 0.7320 - val_loss: 0.6685 - val_accuracy: 0.7648\n",
      "Epoch 208/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.7625 - accuracy: 0.7355 - val_loss: 0.6664 - val_accuracy: 0.7723\n",
      "Epoch 209/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7579 - accuracy: 0.7358 - val_loss: 0.6363 - val_accuracy: 0.7819\n",
      "Epoch 210/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7611 - accuracy: 0.7353 - val_loss: 0.6524 - val_accuracy: 0.7741\n",
      "Epoch 211/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7582 - accuracy: 0.7364 - val_loss: 0.6288 - val_accuracy: 0.7837\n",
      "Epoch 212/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7580 - accuracy: 0.7355 - val_loss: 0.6680 - val_accuracy: 0.7690\n",
      "Epoch 213/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7577 - accuracy: 0.7342 - val_loss: 0.6431 - val_accuracy: 0.7789\n",
      "Epoch 214/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7562 - accuracy: 0.7369 - val_loss: 0.6535 - val_accuracy: 0.7710\n",
      "Epoch 215/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7557 - accuracy: 0.7376 - val_loss: 0.6473 - val_accuracy: 0.7746\n",
      "Epoch 216/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7535 - accuracy: 0.7389 - val_loss: 0.6328 - val_accuracy: 0.7824\n",
      "Epoch 217/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7497 - accuracy: 0.7386 - val_loss: 0.6390 - val_accuracy: 0.7803\n",
      "Epoch 218/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7508 - accuracy: 0.7380 - val_loss: 0.6354 - val_accuracy: 0.7824\n",
      "Epoch 219/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7511 - accuracy: 0.7392 - val_loss: 0.6477 - val_accuracy: 0.7744\n",
      "Epoch 220/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.7534 - accuracy: 0.7400 - val_loss: 0.6384 - val_accuracy: 0.7804\n",
      "Epoch 221/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7467 - accuracy: 0.7415 - val_loss: 0.6403 - val_accuracy: 0.7790\n",
      "Epoch 222/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7444 - accuracy: 0.7411 - val_loss: 0.6401 - val_accuracy: 0.7773\n",
      "Epoch 223/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7501 - accuracy: 0.7374 - val_loss: 0.6384 - val_accuracy: 0.7777\n",
      "Epoch 224/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7476 - accuracy: 0.7389 - val_loss: 0.6626 - val_accuracy: 0.7695\n",
      "Epoch 225/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.7498 - accuracy: 0.7374 - val_loss: 0.6652 - val_accuracy: 0.7680\n",
      "Epoch 226/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.7426 - accuracy: 0.7417 - val_loss: 0.6268 - val_accuracy: 0.7821\n",
      "Epoch 227/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7481 - accuracy: 0.7406 - val_loss: 0.6347 - val_accuracy: 0.7834\n",
      "Epoch 228/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7389 - accuracy: 0.7412 - val_loss: 0.6482 - val_accuracy: 0.7752\n",
      "Epoch 229/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7435 - accuracy: 0.7409 - val_loss: 0.6309 - val_accuracy: 0.7817\n",
      "Epoch 230/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7404 - accuracy: 0.7414 - val_loss: 0.6263 - val_accuracy: 0.7828\n",
      "Epoch 231/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.7430 - accuracy: 0.7423 - val_loss: 0.6435 - val_accuracy: 0.7753\n",
      "Epoch 232/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.7335 - accuracy: 0.7466 - val_loss: 0.6220 - val_accuracy: 0.7846\n",
      "Epoch 233/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7383 - accuracy: 0.7434 - val_loss: 0.6526 - val_accuracy: 0.7721\n",
      "Epoch 234/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7417 - accuracy: 0.7422 - val_loss: 0.6307 - val_accuracy: 0.7810\n",
      "Epoch 235/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7388 - accuracy: 0.7447 - val_loss: 0.6264 - val_accuracy: 0.7869\n",
      "Epoch 236/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7370 - accuracy: 0.7445 - val_loss: 0.6693 - val_accuracy: 0.7650\n",
      "Epoch 237/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7378 - accuracy: 0.7429 - val_loss: 0.6482 - val_accuracy: 0.7762\n",
      "Epoch 238/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.7335 - accuracy: 0.7434 - val_loss: 0.6352 - val_accuracy: 0.7792\n",
      "Epoch 239/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7323 - accuracy: 0.7442 - val_loss: 0.6203 - val_accuracy: 0.7854\n",
      "Epoch 240/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7306 - accuracy: 0.7438 - val_loss: 0.6356 - val_accuracy: 0.7798\n",
      "Epoch 241/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7303 - accuracy: 0.7440 - val_loss: 0.6697 - val_accuracy: 0.7679\n",
      "Epoch 242/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7290 - accuracy: 0.7464 - val_loss: 0.6120 - val_accuracy: 0.7880\n",
      "Epoch 243/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7298 - accuracy: 0.7455 - val_loss: 0.6297 - val_accuracy: 0.7791\n",
      "Epoch 244/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.7310 - accuracy: 0.7461 - val_loss: 0.6329 - val_accuracy: 0.7806\n",
      "Epoch 245/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7296 - accuracy: 0.7492 - val_loss: 0.6334 - val_accuracy: 0.7807\n",
      "Epoch 246/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7231 - accuracy: 0.7480 - val_loss: 0.6178 - val_accuracy: 0.7843\n",
      "Epoch 247/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7306 - accuracy: 0.7470 - val_loss: 0.6385 - val_accuracy: 0.7768\n",
      "Epoch 248/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7222 - accuracy: 0.7477 - val_loss: 0.6385 - val_accuracy: 0.7799\n",
      "Epoch 249/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7283 - accuracy: 0.7483 - val_loss: 0.6372 - val_accuracy: 0.7793\n",
      "Epoch 250/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.7212 - accuracy: 0.7495 - val_loss: 0.6063 - val_accuracy: 0.7893\n",
      "Epoch 251/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7332 - accuracy: 0.7446 - val_loss: 0.6442 - val_accuracy: 0.7759\n",
      "Epoch 252/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7186 - accuracy: 0.7499 - val_loss: 0.6084 - val_accuracy: 0.7904\n",
      "Epoch 253/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7241 - accuracy: 0.7488 - val_loss: 0.6305 - val_accuracy: 0.7819\n",
      "Epoch 254/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7185 - accuracy: 0.7486 - val_loss: 0.6403 - val_accuracy: 0.7772\n",
      "Epoch 255/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7213 - accuracy: 0.7499 - val_loss: 0.6275 - val_accuracy: 0.7815\n",
      "Epoch 256/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.7154 - accuracy: 0.7502 - val_loss: 0.6112 - val_accuracy: 0.7850\n",
      "Epoch 257/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7232 - accuracy: 0.7493 - val_loss: 0.6007 - val_accuracy: 0.7920\n",
      "Epoch 258/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7155 - accuracy: 0.7502 - val_loss: 0.6234 - val_accuracy: 0.7841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 259/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7135 - accuracy: 0.7510 - val_loss: 0.6455 - val_accuracy: 0.7764\n",
      "Epoch 260/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7180 - accuracy: 0.7518 - val_loss: 0.6325 - val_accuracy: 0.7802\n",
      "Epoch 261/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7161 - accuracy: 0.7534 - val_loss: 0.6103 - val_accuracy: 0.7893\n",
      "Epoch 262/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.7147 - accuracy: 0.7514 - val_loss: 0.6145 - val_accuracy: 0.7855\n",
      "Epoch 263/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7130 - accuracy: 0.7533 - val_loss: 0.6250 - val_accuracy: 0.7825\n",
      "Epoch 264/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7095 - accuracy: 0.7544 - val_loss: 0.6270 - val_accuracy: 0.7800\n",
      "Epoch 265/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7095 - accuracy: 0.7515 - val_loss: 0.6151 - val_accuracy: 0.7874\n",
      "Epoch 266/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7092 - accuracy: 0.7530 - val_loss: 0.6153 - val_accuracy: 0.7846\n",
      "Epoch 267/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7123 - accuracy: 0.7505 - val_loss: 0.6450 - val_accuracy: 0.7745\n",
      "Epoch 268/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.7068 - accuracy: 0.7556 - val_loss: 0.6007 - val_accuracy: 0.7901\n",
      "Epoch 269/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7091 - accuracy: 0.7522 - val_loss: 0.6087 - val_accuracy: 0.7881\n",
      "Epoch 270/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7120 - accuracy: 0.7549 - val_loss: 0.6034 - val_accuracy: 0.7888\n",
      "Epoch 271/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7126 - accuracy: 0.7542 - val_loss: 0.6013 - val_accuracy: 0.7910\n",
      "Epoch 272/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7065 - accuracy: 0.7535 - val_loss: 0.5981 - val_accuracy: 0.7933\n",
      "Epoch 273/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7127 - accuracy: 0.7517 - val_loss: 0.6294 - val_accuracy: 0.7789\n",
      "Epoch 274/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.7090 - accuracy: 0.7524 - val_loss: 0.5979 - val_accuracy: 0.7930\n",
      "Epoch 275/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7024 - accuracy: 0.7533 - val_loss: 0.6140 - val_accuracy: 0.7888\n",
      "Epoch 276/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6995 - accuracy: 0.7568 - val_loss: 0.5938 - val_accuracy: 0.7928\n",
      "Epoch 277/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7031 - accuracy: 0.7536 - val_loss: 0.6577 - val_accuracy: 0.7681\n",
      "Epoch 278/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7013 - accuracy: 0.7562 - val_loss: 0.5949 - val_accuracy: 0.7925\n",
      "Epoch 279/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7000 - accuracy: 0.7556 - val_loss: 0.6497 - val_accuracy: 0.7723\n",
      "Epoch 280/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.7021 - accuracy: 0.7539 - val_loss: 0.6139 - val_accuracy: 0.7841\n",
      "Epoch 281/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7006 - accuracy: 0.7558 - val_loss: 0.6588 - val_accuracy: 0.7694\n",
      "Epoch 282/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7025 - accuracy: 0.7565 - val_loss: 0.5954 - val_accuracy: 0.7911\n",
      "Epoch 283/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6997 - accuracy: 0.7593 - val_loss: 0.5999 - val_accuracy: 0.7933\n",
      "Epoch 284/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6984 - accuracy: 0.7567 - val_loss: 0.6228 - val_accuracy: 0.7825\n",
      "Epoch 285/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6967 - accuracy: 0.7585 - val_loss: 0.6077 - val_accuracy: 0.7841\n",
      "Epoch 286/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.6962 - accuracy: 0.7577 - val_loss: 0.6148 - val_accuracy: 0.7841\n",
      "Epoch 287/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7014 - accuracy: 0.7574 - val_loss: 0.6074 - val_accuracy: 0.7873\n",
      "Epoch 288/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.7017 - accuracy: 0.7565 - val_loss: 0.6011 - val_accuracy: 0.7896\n",
      "Epoch 289/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6950 - accuracy: 0.7579 - val_loss: 0.6132 - val_accuracy: 0.7867\n",
      "Epoch 290/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6974 - accuracy: 0.7566 - val_loss: 0.6321 - val_accuracy: 0.7780\n",
      "Epoch 291/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6976 - accuracy: 0.7580 - val_loss: 0.5992 - val_accuracy: 0.7893\n",
      "Epoch 292/400\n",
      "782/782 [==============================] - 14s 18ms/step - loss: 0.6937 - accuracy: 0.7606 - val_loss: 0.6009 - val_accuracy: 0.7895\n",
      "Epoch 293/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6957 - accuracy: 0.7598 - val_loss: 0.5856 - val_accuracy: 0.7969\n",
      "Epoch 294/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6962 - accuracy: 0.7569 - val_loss: 0.6156 - val_accuracy: 0.7846\n",
      "Epoch 295/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.6889 - accuracy: 0.7601 - val_loss: 0.6046 - val_accuracy: 0.7886\n",
      "Epoch 296/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6959 - accuracy: 0.7582 - val_loss: 0.5915 - val_accuracy: 0.7927\n",
      "Epoch 297/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.6866 - accuracy: 0.7625 - val_loss: 0.5837 - val_accuracy: 0.7973\n",
      "Epoch 298/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.6915 - accuracy: 0.7606 - val_loss: 0.6068 - val_accuracy: 0.7849\n",
      "Epoch 299/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6888 - accuracy: 0.7618 - val_loss: 0.6340 - val_accuracy: 0.7817\n",
      "Epoch 300/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6913 - accuracy: 0.7606 - val_loss: 0.6038 - val_accuracy: 0.7867\n",
      "Epoch 301/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6862 - accuracy: 0.7611 - val_loss: 0.5917 - val_accuracy: 0.7949\n",
      "Epoch 302/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6868 - accuracy: 0.7603 - val_loss: 0.6036 - val_accuracy: 0.7897\n",
      "Epoch 303/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6868 - accuracy: 0.7616 - val_loss: 0.5890 - val_accuracy: 0.7940\n",
      "Epoch 304/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6858 - accuracy: 0.7636 - val_loss: 0.6070 - val_accuracy: 0.7885\n",
      "Epoch 305/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6886 - accuracy: 0.7607 - val_loss: 0.5931 - val_accuracy: 0.7937\n",
      "Epoch 306/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6853 - accuracy: 0.7616 - val_loss: 0.6030 - val_accuracy: 0.7894\n",
      "Epoch 307/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6879 - accuracy: 0.7614 - val_loss: 0.6048 - val_accuracy: 0.7889\n",
      "Epoch 308/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6846 - accuracy: 0.7631 - val_loss: 0.5891 - val_accuracy: 0.7923\n",
      "Epoch 309/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6814 - accuracy: 0.7628 - val_loss: 0.6075 - val_accuracy: 0.7886\n",
      "Epoch 310/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6884 - accuracy: 0.7625 - val_loss: 0.6173 - val_accuracy: 0.7857\n",
      "Epoch 311/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6833 - accuracy: 0.7628 - val_loss: 0.5936 - val_accuracy: 0.7947\n",
      "Epoch 312/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6771 - accuracy: 0.7656 - val_loss: 0.6184 - val_accuracy: 0.7846\n",
      "Epoch 313/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6870 - accuracy: 0.7616 - val_loss: 0.5892 - val_accuracy: 0.7951\n",
      "Epoch 314/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6817 - accuracy: 0.7636 - val_loss: 0.6183 - val_accuracy: 0.7815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 315/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6773 - accuracy: 0.7656 - val_loss: 0.5804 - val_accuracy: 0.7980\n",
      "Epoch 316/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6827 - accuracy: 0.7620 - val_loss: 0.6237 - val_accuracy: 0.7821\n",
      "Epoch 317/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6807 - accuracy: 0.7623 - val_loss: 0.6052 - val_accuracy: 0.7876\n",
      "Epoch 318/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6800 - accuracy: 0.7645 - val_loss: 0.6034 - val_accuracy: 0.7892\n",
      "Epoch 319/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6753 - accuracy: 0.7663 - val_loss: 0.5787 - val_accuracy: 0.7968\n",
      "Epoch 320/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6762 - accuracy: 0.7645 - val_loss: 0.5803 - val_accuracy: 0.7974\n",
      "Epoch 321/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6757 - accuracy: 0.7655 - val_loss: 0.5891 - val_accuracy: 0.7944\n",
      "Epoch 322/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6789 - accuracy: 0.7629 - val_loss: 0.5860 - val_accuracy: 0.7937\n",
      "Epoch 323/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6755 - accuracy: 0.7662 - val_loss: 0.5790 - val_accuracy: 0.7966\n",
      "Epoch 324/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6774 - accuracy: 0.7660 - val_loss: 0.5927 - val_accuracy: 0.7917\n",
      "Epoch 325/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6718 - accuracy: 0.7688 - val_loss: 0.5958 - val_accuracy: 0.7904\n",
      "Epoch 326/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6771 - accuracy: 0.7645 - val_loss: 0.5946 - val_accuracy: 0.7937\n",
      "Epoch 327/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6720 - accuracy: 0.7660 - val_loss: 0.6067 - val_accuracy: 0.7884\n",
      "Epoch 328/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6693 - accuracy: 0.7662 - val_loss: 0.5975 - val_accuracy: 0.7899\n",
      "Epoch 329/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6762 - accuracy: 0.7658 - val_loss: 0.6143 - val_accuracy: 0.7853\n",
      "Epoch 330/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6693 - accuracy: 0.7685 - val_loss: 0.6036 - val_accuracy: 0.7900\n",
      "Epoch 331/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6741 - accuracy: 0.7663 - val_loss: 0.5941 - val_accuracy: 0.7912\n",
      "Epoch 332/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6681 - accuracy: 0.7670 - val_loss: 0.6011 - val_accuracy: 0.7915\n",
      "Epoch 333/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6725 - accuracy: 0.7686 - val_loss: 0.6027 - val_accuracy: 0.7879\n",
      "Epoch 334/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6682 - accuracy: 0.7656 - val_loss: 0.5739 - val_accuracy: 0.8010\n",
      "Epoch 335/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6692 - accuracy: 0.7660 - val_loss: 0.6322 - val_accuracy: 0.7795\n",
      "Epoch 336/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6653 - accuracy: 0.7689 - val_loss: 0.5880 - val_accuracy: 0.7956\n",
      "Epoch 337/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6726 - accuracy: 0.7669 - val_loss: 0.5789 - val_accuracy: 0.8005\n",
      "Epoch 338/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6670 - accuracy: 0.7680 - val_loss: 0.5777 - val_accuracy: 0.8000\n",
      "Epoch 339/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6695 - accuracy: 0.7685 - val_loss: 0.5846 - val_accuracy: 0.7961\n",
      "Epoch 340/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6635 - accuracy: 0.7695 - val_loss: 0.5657 - val_accuracy: 0.8039\n",
      "Epoch 341/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.6687 - accuracy: 0.7665 - val_loss: 0.5721 - val_accuracy: 0.7995\n",
      "Epoch 342/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6690 - accuracy: 0.7683 - val_loss: 0.5770 - val_accuracy: 0.7998\n",
      "Epoch 343/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6688 - accuracy: 0.7672 - val_loss: 0.5997 - val_accuracy: 0.7904\n",
      "Epoch 344/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6678 - accuracy: 0.7671 - val_loss: 0.5872 - val_accuracy: 0.7945\n",
      "Epoch 345/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6662 - accuracy: 0.7699 - val_loss: 0.5888 - val_accuracy: 0.7938\n",
      "Epoch 346/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6670 - accuracy: 0.7672 - val_loss: 0.5758 - val_accuracy: 0.7994\n",
      "Epoch 347/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.6596 - accuracy: 0.7708 - val_loss: 0.6054 - val_accuracy: 0.7883\n",
      "Epoch 348/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.6674 - accuracy: 0.7662 - val_loss: 0.5768 - val_accuracy: 0.7989\n",
      "Epoch 349/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6622 - accuracy: 0.7687 - val_loss: 0.5735 - val_accuracy: 0.7995\n",
      "Epoch 350/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6604 - accuracy: 0.7700 - val_loss: 0.6022 - val_accuracy: 0.7913\n",
      "Epoch 351/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6624 - accuracy: 0.7711 - val_loss: 0.5997 - val_accuracy: 0.7910\n",
      "Epoch 352/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6583 - accuracy: 0.7695 - val_loss: 0.5891 - val_accuracy: 0.7942\n",
      "Epoch 353/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6586 - accuracy: 0.7720 - val_loss: 0.5715 - val_accuracy: 0.8018\n",
      "Epoch 354/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6576 - accuracy: 0.7713 - val_loss: 0.6000 - val_accuracy: 0.7912\n",
      "Epoch 355/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6617 - accuracy: 0.7691 - val_loss: 0.5733 - val_accuracy: 0.7968\n",
      "Epoch 356/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6600 - accuracy: 0.7710 - val_loss: 0.5871 - val_accuracy: 0.7940\n",
      "Epoch 357/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6656 - accuracy: 0.7683 - val_loss: 0.5736 - val_accuracy: 0.8006\n",
      "Epoch 358/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6534 - accuracy: 0.7742 - val_loss: 0.5867 - val_accuracy: 0.7960\n",
      "Epoch 359/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6609 - accuracy: 0.7682 - val_loss: 0.5724 - val_accuracy: 0.7994\n",
      "Epoch 360/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6636 - accuracy: 0.7701 - val_loss: 0.5829 - val_accuracy: 0.7977\n",
      "Epoch 361/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6553 - accuracy: 0.7721 - val_loss: 0.5762 - val_accuracy: 0.7991\n",
      "Epoch 362/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6576 - accuracy: 0.7707 - val_loss: 0.5621 - val_accuracy: 0.8033\n",
      "Epoch 363/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6572 - accuracy: 0.7712 - val_loss: 0.6036 - val_accuracy: 0.7884\n",
      "Epoch 364/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6606 - accuracy: 0.7710 - val_loss: 0.5911 - val_accuracy: 0.7932\n",
      "Epoch 365/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6552 - accuracy: 0.7727 - val_loss: 0.5621 - val_accuracy: 0.8018\n",
      "Epoch 366/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6551 - accuracy: 0.7731 - val_loss: 0.5513 - val_accuracy: 0.8082\n",
      "Epoch 367/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6588 - accuracy: 0.7719 - val_loss: 0.5926 - val_accuracy: 0.7904\n",
      "Epoch 368/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6586 - accuracy: 0.7713 - val_loss: 0.6002 - val_accuracy: 0.7888\n",
      "Epoch 369/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6569 - accuracy: 0.7724 - val_loss: 0.5908 - val_accuracy: 0.7942\n",
      "Epoch 370/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6588 - accuracy: 0.7703 - val_loss: 0.5892 - val_accuracy: 0.7945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6549 - accuracy: 0.7732 - val_loss: 0.5719 - val_accuracy: 0.7989\n",
      "Epoch 372/400\n",
      "782/782 [==============================] - 14s 17ms/step - loss: 0.6578 - accuracy: 0.7714 - val_loss: 0.5695 - val_accuracy: 0.7987\n",
      "Epoch 373/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6505 - accuracy: 0.7735 - val_loss: 0.5628 - val_accuracy: 0.8039\n",
      "Epoch 374/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6495 - accuracy: 0.7763 - val_loss: 0.5726 - val_accuracy: 0.7972\n",
      "Epoch 375/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6524 - accuracy: 0.7739 - val_loss: 0.5557 - val_accuracy: 0.8051\n",
      "Epoch 376/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6535 - accuracy: 0.7750 - val_loss: 0.5891 - val_accuracy: 0.7903\n",
      "Epoch 377/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6547 - accuracy: 0.7722 - val_loss: 0.5644 - val_accuracy: 0.8016\n",
      "Epoch 378/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6519 - accuracy: 0.7725 - val_loss: 0.5776 - val_accuracy: 0.7969\n",
      "Epoch 379/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6527 - accuracy: 0.7726 - val_loss: 0.5717 - val_accuracy: 0.7990\n",
      "Epoch 380/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6532 - accuracy: 0.7713 - val_loss: 0.5564 - val_accuracy: 0.8054\n",
      "Epoch 381/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6453 - accuracy: 0.7751 - val_loss: 0.5681 - val_accuracy: 0.8034\n",
      "Epoch 382/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6516 - accuracy: 0.7741 - val_loss: 0.5955 - val_accuracy: 0.7909\n",
      "Epoch 383/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6492 - accuracy: 0.7741 - val_loss: 0.5795 - val_accuracy: 0.8013\n",
      "Epoch 384/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6483 - accuracy: 0.7741 - val_loss: 0.5735 - val_accuracy: 0.8011\n",
      "Epoch 385/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6499 - accuracy: 0.7743 - val_loss: 0.5693 - val_accuracy: 0.8033\n",
      "Epoch 386/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6541 - accuracy: 0.7719 - val_loss: 0.5875 - val_accuracy: 0.7937\n",
      "Epoch 387/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6462 - accuracy: 0.7773 - val_loss: 0.5745 - val_accuracy: 0.8007\n",
      "Epoch 388/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6461 - accuracy: 0.7763 - val_loss: 0.5673 - val_accuracy: 0.8023\n",
      "Epoch 389/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6471 - accuracy: 0.7759 - val_loss: 0.5620 - val_accuracy: 0.8036\n",
      "Epoch 390/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6424 - accuracy: 0.7780 - val_loss: 0.5738 - val_accuracy: 0.7982\n",
      "Epoch 391/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6486 - accuracy: 0.7758 - val_loss: 0.5543 - val_accuracy: 0.8086\n",
      "Epoch 392/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6499 - accuracy: 0.7744 - val_loss: 0.5646 - val_accuracy: 0.8033\n",
      "Epoch 393/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6451 - accuracy: 0.7776 - val_loss: 0.5593 - val_accuracy: 0.8028\n",
      "Epoch 394/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6443 - accuracy: 0.7758 - val_loss: 0.5870 - val_accuracy: 0.7961\n",
      "Epoch 395/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6439 - accuracy: 0.7769 - val_loss: 0.5652 - val_accuracy: 0.8020\n",
      "Epoch 396/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6456 - accuracy: 0.7746 - val_loss: 0.5743 - val_accuracy: 0.8005\n",
      "Epoch 397/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6494 - accuracy: 0.7720 - val_loss: 0.5727 - val_accuracy: 0.8017\n",
      "Epoch 398/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6416 - accuracy: 0.7797 - val_loss: 0.5673 - val_accuracy: 0.8020\n",
      "Epoch 399/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6502 - accuracy: 0.7731 - val_loss: 0.5699 - val_accuracy: 0.7989\n",
      "Epoch 400/400\n",
      "782/782 [==============================] - 13s 17ms/step - loss: 0.6467 - accuracy: 0.7747 - val_loss: 0.5772 - val_accuracy: 0.7973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f84a9792490>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', \n",
    "                    input_shape=(32, 32, 3)))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.2))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.3))\n",
    "model_10.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((4, 4)))\n",
    "model_10.add(Dropout(0.4))\n",
    "model_10.add(Flatten())\n",
    "model_10.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model_10.add(Dropout(0.5))\n",
    "model_10.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# Convert the model to a quantization aware model\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(model_10)\n",
    "\n",
    "quant_aware_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# define the ModelCheckpoint callback to save the model after each epoch\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_10_gpu_Base-CubeAI_3Conv_QAT.h5', save_freq='epoch')\n",
    "\n",
    "quant_aware_model.summary()\n",
    "\n",
    "# Train and evaluate the quantization aware model\n",
    "quant_aware_model.fit(x_train,y_train, batch_size=64,epochs=400,validation_data=(x_test, y_test),\n",
    "                     callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5b1c72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization aware training loss:  0.577202320098877\n",
      "Quantization aware training accuracy:  0.7972999811172485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as conv2d_61_layer_call_fn, conv2d_61_layer_call_and_return_conditional_losses, _jit_compiled_convolution_op, dropout_56_layer_call_fn, dropout_56_layer_call_and_return_conditional_losses while saving (showing 5 of 23). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpi9v_dq7q/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpi9v_dq7q/assets\n",
      "/home/themandalorian/anaconda3/envs/ml-on-mc/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:766: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-05-17 23:09:30.438235: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-05-17 23:09:30.438265: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-05-17 23:09:30.438414: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpi9v_dq7q\n",
      "2023-05-17 23:09:30.445365: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-17 23:09:30.445386: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpi9v_dq7q\n",
      "2023-05-17 23:09:30.464158: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-05-17 23:09:30.534725: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpi9v_dq7q\n",
      "2023-05-17 23:09:30.554172: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 115757 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: UINT8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_conv2d_61_input:0\n",
      "shape: [ 1 32 32  3]\n",
      "type: <class 'numpy.uint8'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.uint8'>\n",
      "Accuracy of quantized to int8 model is 79.67999999999999%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model on the test set\n",
    "quant_loss, quant_acc = quant_aware_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Quantization aware training loss: ', quant_loss)\n",
    "print('Quantization aware training accuracy: ', quant_acc)\n",
    "\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant_int8_qat = converter.convert()\n",
    "\n",
    "open(\"model_10_gpu_Base-CubeAI_3Conv_qat_int8.tflite\", \"wb\").write(tflite_model_quant_int8_qat)\n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(\"model_10_gpu_Base-CubeAI_3Conv_qat_int8.tflite\")\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "predictions = np.zeros((len(x_test),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(len(x_test)):\n",
    "    val_batch = x_test[i]\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()\n",
    "    \n",
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 10000\n",
    "print(\"Accuracy of quantized to int8 model is {}%\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c4eb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'cifar10_base_3conv_qat'\n",
    "# check if dir 'cfiles' exists, if not create it\n",
    "if not os.path.exists('cfiles'):\n",
    "    os.makedirs('cfiles')\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model_quant_int8_qat, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17439b98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
