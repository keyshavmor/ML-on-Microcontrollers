{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "076d49b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-29 18:48:55.927016: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-29 18:48:57.224232: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-29 18:48:57.224283: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-29 18:48:57.224288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, Conv2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import timeit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from tensorflow.keras.models import load_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6961c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CIFAR-10\n",
      "Number of features: (32, 32, 3)\n",
      "Number of classes: 10\n",
      "Number of training samples: 50000\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "data_path = 'cifar-10-batches-py/'\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for i in range(1, 6):\n",
    "    batch = unpickle(data_path + 'data_batch_' + str(i))\n",
    "    train_data.append(batch[b'data'])\n",
    "    train_labels += batch[b'labels']\n",
    "    \n",
    "    \n",
    "train_data = np.concatenate(train_data)\n",
    "train_data = train_data.reshape((50000, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "test_data = unpickle(data_path + 'test_batch')[b'data']\n",
    "test_data = test_data.reshape((10000, 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "train_labels = np.array(train_labels)\n",
    "test_labels = np.array(unpickle(data_path + 'test_batch')[b'labels'])\n",
    "\n",
    "# Print the number of features, classes, and training samples for CIFAR-10\n",
    "print(\"CIFAR-10\")\n",
    "print(\"Number of features:\", train_data.shape[1:])\n",
    "print(\"Number of classes:\", len(np.unique(train_labels)))\n",
    "print(\"Number of training samples:\", train_data.shape[0])\n",
    "\n",
    "# Shuffle your dataset\n",
    "train_data, train_labels = shuffle(train_data, train_labels, random_state=42)\n",
    "test_data, test_labels = shuffle(test_data, test_labels, random_state=42)\n",
    "\n",
    "# Normalize pixel values\n",
    "x_train = train_data.astype('float32') / 255.0\n",
    "x_test = test_data.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to categorical format\n",
    "y_train = to_categorical(train_labels, num_classes=10)\n",
    "y_test = to_categorical(test_labels, num_classes=10)\n",
    "\n",
    "# create data generator\n",
    "datagen = ImageDataGenerator(zoom_range=[0.9,1.1],\n",
    "                             rotation_range=30,\n",
    "                             brightness_range=[0.9,1.1],\n",
    "                             width_shift_range=0.3, \n",
    "                             height_shift_range=0.3,\n",
    "                             vertical_flip=True,\n",
    "                             horizontal_flip=True)\n",
    "# prepare iterator\n",
    "it_train = datagen.flow(x_train, y_train, batch_size=64)\n",
    "# fit model\n",
    "steps = int(x_train.shape[0] / 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0433baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function: Convert some hex value into an array for C programming\n",
    "def hex_to_c_array(hex_data, var_name):\n",
    "\n",
    "    c_str = ''\n",
    "\n",
    "    # Create header guard\n",
    "    c_str += '#ifndef ' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define ' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    # Add array length at top of file\n",
    "    c_str += '\\nstatic const unsigned int ' + var_name + '_len = ' + str(len(hex_data)) + ';\\n'\n",
    "\n",
    "    # Declare C variable\n",
    "    c_str += 'static const unsigned char ' + var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data) :\n",
    "\n",
    "        # Construct string from hex\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        # Add formatting so each line stays within 80 characters\n",
    "        if (i + 1) < len(hex_data):\n",
    "            hex_str += ','\n",
    "        if (i + 1) % 12 == 0:\n",
    "            hex_str += '\\n '\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "    # Add closing brace\n",
    "    c_str += '\\n ' + format(' '.join(hex_array)) + '\\n};\\n\\n'\n",
    "\n",
    "    # Close out header guard\n",
    "    c_str += '#endif //' + var_name.upper() + '_H'\n",
    "\n",
    "    return c_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3244371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_52 (Conv2D)          (None, 32, 32, 16)        448       \n",
      "                                                                 \n",
      " conv2d_53 (Conv2D)          (None, 32, 32, 16)        2320      \n",
      "                                                                 \n",
      " max_pooling2d_31 (MaxPoolin  (None, 16, 16, 16)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " conv2d_54 (Conv2D)          (None, 16, 16, 32)        4640      \n",
      "                                                                 \n",
      " conv2d_55 (Conv2D)          (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_32 (MaxPoolin  (None, 8, 8, 32)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " conv2d_56 (Conv2D)          (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " conv2d_57 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_33 (MaxPoolin  (None, 4, 4, 64)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " flatten_9 (Flatten)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 128)               131200    \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 204,570\n",
      "Trainable params: 204,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 2.0715 - accuracy: 0.2100 - val_loss: 1.7391 - val_accuracy: 0.3426\n",
      "Epoch 2/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.7190 - accuracy: 0.3555 - val_loss: 1.4996 - val_accuracy: 0.4561\n",
      "Epoch 3/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.5515 - accuracy: 0.4280 - val_loss: 1.4509 - val_accuracy: 0.4625\n",
      "Epoch 4/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.4420 - accuracy: 0.4745 - val_loss: 1.3109 - val_accuracy: 0.5328\n",
      "Epoch 5/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.3558 - accuracy: 0.5084 - val_loss: 1.2083 - val_accuracy: 0.5666\n",
      "Epoch 6/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.3033 - accuracy: 0.5332 - val_loss: 1.1425 - val_accuracy: 0.5903\n",
      "Epoch 7/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.2386 - accuracy: 0.5588 - val_loss: 1.1244 - val_accuracy: 0.6047\n",
      "Epoch 8/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.1938 - accuracy: 0.5755 - val_loss: 1.1048 - val_accuracy: 0.6144\n",
      "Epoch 9/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 1.1487 - accuracy: 0.5896 - val_loss: 1.0100 - val_accuracy: 0.6451\n",
      "Epoch 10/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.1317 - accuracy: 0.6007 - val_loss: 0.9824 - val_accuracy: 0.6523\n",
      "Epoch 11/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.0935 - accuracy: 0.6144 - val_loss: 0.9729 - val_accuracy: 0.6637\n",
      "Epoch 12/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 1.0703 - accuracy: 0.6257 - val_loss: 0.9428 - val_accuracy: 0.6671\n",
      "Epoch 13/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 1.0483 - accuracy: 0.6321 - val_loss: 0.9104 - val_accuracy: 0.6799\n",
      "Epoch 14/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.0285 - accuracy: 0.6390 - val_loss: 0.8685 - val_accuracy: 0.6966\n",
      "Epoch 15/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.0039 - accuracy: 0.6486 - val_loss: 0.8687 - val_accuracy: 0.6940\n",
      "Epoch 16/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 1.0015 - accuracy: 0.6528 - val_loss: 0.8254 - val_accuracy: 0.7090\n",
      "Epoch 17/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.9795 - accuracy: 0.6580 - val_loss: 0.8795 - val_accuracy: 0.6895\n",
      "Epoch 18/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.9659 - accuracy: 0.6617 - val_loss: 0.8840 - val_accuracy: 0.6860\n",
      "Epoch 19/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.9573 - accuracy: 0.6668 - val_loss: 0.8180 - val_accuracy: 0.7157\n",
      "Epoch 20/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.9447 - accuracy: 0.6699 - val_loss: 0.8494 - val_accuracy: 0.7045\n",
      "Epoch 21/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.9377 - accuracy: 0.6723 - val_loss: 0.8257 - val_accuracy: 0.7105\n",
      "Epoch 22/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.9240 - accuracy: 0.6792 - val_loss: 0.7882 - val_accuracy: 0.7259\n",
      "Epoch 23/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.9119 - accuracy: 0.6818 - val_loss: 0.8276 - val_accuracy: 0.7083\n",
      "Epoch 24/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.9061 - accuracy: 0.6861 - val_loss: 0.7971 - val_accuracy: 0.7226\n",
      "Epoch 25/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.8901 - accuracy: 0.6887 - val_loss: 0.8598 - val_accuracy: 0.6970\n",
      "Epoch 26/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8890 - accuracy: 0.6889 - val_loss: 0.7728 - val_accuracy: 0.7275\n",
      "Epoch 27/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8801 - accuracy: 0.6967 - val_loss: 0.7454 - val_accuracy: 0.7435\n",
      "Epoch 28/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8833 - accuracy: 0.6941 - val_loss: 0.7810 - val_accuracy: 0.7281\n",
      "Epoch 29/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8710 - accuracy: 0.6990 - val_loss: 0.7816 - val_accuracy: 0.7266\n",
      "Epoch 30/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8644 - accuracy: 0.7003 - val_loss: 0.7566 - val_accuracy: 0.7346\n",
      "Epoch 31/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8694 - accuracy: 0.7000 - val_loss: 0.7966 - val_accuracy: 0.7279\n",
      "Epoch 32/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8650 - accuracy: 0.7003 - val_loss: 0.7891 - val_accuracy: 0.7251\n",
      "Epoch 33/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8447 - accuracy: 0.7075 - val_loss: 0.7717 - val_accuracy: 0.7383\n",
      "Epoch 34/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8482 - accuracy: 0.7057 - val_loss: 0.7414 - val_accuracy: 0.7479\n",
      "Epoch 35/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8462 - accuracy: 0.7058 - val_loss: 0.7563 - val_accuracy: 0.7404\n",
      "Epoch 36/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8361 - accuracy: 0.7101 - val_loss: 0.7690 - val_accuracy: 0.7355\n",
      "Epoch 37/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8316 - accuracy: 0.7122 - val_loss: 0.7515 - val_accuracy: 0.7437\n",
      "Epoch 38/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8300 - accuracy: 0.7151 - val_loss: 0.7295 - val_accuracy: 0.7493\n",
      "Epoch 39/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8220 - accuracy: 0.7163 - val_loss: 0.7367 - val_accuracy: 0.7440\n",
      "Epoch 40/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8180 - accuracy: 0.7176 - val_loss: 0.7119 - val_accuracy: 0.7556\n",
      "Epoch 41/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8250 - accuracy: 0.7142 - val_loss: 0.7451 - val_accuracy: 0.7365\n",
      "Epoch 42/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8140 - accuracy: 0.7212 - val_loss: 0.7520 - val_accuracy: 0.7415\n",
      "Epoch 43/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8118 - accuracy: 0.7215 - val_loss: 0.7698 - val_accuracy: 0.7340\n",
      "Epoch 44/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8077 - accuracy: 0.7198 - val_loss: 0.7343 - val_accuracy: 0.7452\n",
      "Epoch 45/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8089 - accuracy: 0.7220 - val_loss: 0.7172 - val_accuracy: 0.7563\n",
      "Epoch 46/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8111 - accuracy: 0.7171 - val_loss: 0.7313 - val_accuracy: 0.7533\n",
      "Epoch 47/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8072 - accuracy: 0.7218 - val_loss: 0.6964 - val_accuracy: 0.7644\n",
      "Epoch 48/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8076 - accuracy: 0.7207 - val_loss: 0.7140 - val_accuracy: 0.7545\n",
      "Epoch 49/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8015 - accuracy: 0.7248 - val_loss: 0.7237 - val_accuracy: 0.7494\n",
      "Epoch 50/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8001 - accuracy: 0.7230 - val_loss: 0.6905 - val_accuracy: 0.7694\n",
      "Epoch 51/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7932 - accuracy: 0.7273 - val_loss: 0.6956 - val_accuracy: 0.7615\n",
      "Epoch 52/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7955 - accuracy: 0.7275 - val_loss: 0.6915 - val_accuracy: 0.7639\n",
      "Epoch 53/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.8002 - accuracy: 0.7256 - val_loss: 0.6971 - val_accuracy: 0.7637\n",
      "Epoch 54/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7928 - accuracy: 0.7257 - val_loss: 0.6911 - val_accuracy: 0.7651\n",
      "Epoch 55/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7909 - accuracy: 0.7292 - val_loss: 0.6912 - val_accuracy: 0.7655\n",
      "Epoch 56/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7892 - accuracy: 0.7282 - val_loss: 0.7072 - val_accuracy: 0.7567\n",
      "Epoch 57/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7888 - accuracy: 0.7286 - val_loss: 0.7409 - val_accuracy: 0.7481\n",
      "Epoch 58/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7900 - accuracy: 0.7285 - val_loss: 0.6905 - val_accuracy: 0.7622\n",
      "Epoch 59/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7798 - accuracy: 0.7326 - val_loss: 0.6672 - val_accuracy: 0.7730\n",
      "Epoch 60/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7866 - accuracy: 0.7303 - val_loss: 0.7311 - val_accuracy: 0.7523\n",
      "Epoch 61/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7894 - accuracy: 0.7277 - val_loss: 0.7034 - val_accuracy: 0.7634\n",
      "Epoch 62/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7741 - accuracy: 0.7325 - val_loss: 0.6829 - val_accuracy: 0.7715\n",
      "Epoch 63/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7869 - accuracy: 0.7293 - val_loss: 0.7055 - val_accuracy: 0.7606\n",
      "Epoch 64/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7787 - accuracy: 0.7328 - val_loss: 0.6841 - val_accuracy: 0.7710\n",
      "Epoch 65/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7738 - accuracy: 0.7339 - val_loss: 0.7618 - val_accuracy: 0.7422\n",
      "Epoch 66/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7720 - accuracy: 0.7342 - val_loss: 0.6744 - val_accuracy: 0.7731\n",
      "Epoch 67/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7875 - accuracy: 0.7295 - val_loss: 0.7207 - val_accuracy: 0.7556\n",
      "Epoch 68/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7780 - accuracy: 0.7324 - val_loss: 0.7271 - val_accuracy: 0.7595\n",
      "Epoch 69/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7782 - accuracy: 0.7333 - val_loss: 0.6721 - val_accuracy: 0.7721\n",
      "Epoch 70/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7755 - accuracy: 0.7337 - val_loss: 0.6791 - val_accuracy: 0.7775\n",
      "Epoch 71/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7767 - accuracy: 0.7364 - val_loss: 0.6752 - val_accuracy: 0.7720\n",
      "Epoch 72/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7651 - accuracy: 0.7358 - val_loss: 0.6672 - val_accuracy: 0.7711\n",
      "Epoch 73/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7753 - accuracy: 0.7326 - val_loss: 0.7096 - val_accuracy: 0.7600\n",
      "Epoch 74/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7724 - accuracy: 0.7343 - val_loss: 0.6764 - val_accuracy: 0.7742\n",
      "Epoch 75/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7715 - accuracy: 0.7333 - val_loss: 0.7260 - val_accuracy: 0.7531\n",
      "Epoch 76/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7728 - accuracy: 0.7345 - val_loss: 0.7292 - val_accuracy: 0.7556\n",
      "Epoch 77/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7642 - accuracy: 0.7407 - val_loss: 0.6683 - val_accuracy: 0.7750\n",
      "Epoch 78/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7693 - accuracy: 0.7352 - val_loss: 0.6701 - val_accuracy: 0.7727\n",
      "Epoch 79/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7561 - accuracy: 0.7388 - val_loss: 0.6669 - val_accuracy: 0.7749\n",
      "Epoch 80/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7724 - accuracy: 0.7350 - val_loss: 0.6888 - val_accuracy: 0.7668\n",
      "Epoch 81/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7672 - accuracy: 0.7387 - val_loss: 0.6608 - val_accuracy: 0.7811\n",
      "Epoch 82/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7638 - accuracy: 0.7380 - val_loss: 0.6803 - val_accuracy: 0.7772\n",
      "Epoch 83/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7703 - accuracy: 0.7385 - val_loss: 0.6927 - val_accuracy: 0.7714\n",
      "Epoch 84/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7657 - accuracy: 0.7392 - val_loss: 0.7127 - val_accuracy: 0.7609\n",
      "Epoch 85/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7771 - accuracy: 0.7321 - val_loss: 0.7078 - val_accuracy: 0.7604\n",
      "Epoch 86/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7574 - accuracy: 0.7415 - val_loss: 0.6881 - val_accuracy: 0.7657\n",
      "Epoch 87/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7677 - accuracy: 0.7352 - val_loss: 0.6816 - val_accuracy: 0.7705\n",
      "Epoch 88/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7619 - accuracy: 0.7376 - val_loss: 0.7089 - val_accuracy: 0.7622\n",
      "Epoch 89/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7669 - accuracy: 0.7368 - val_loss: 0.6907 - val_accuracy: 0.7708\n",
      "Epoch 90/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7654 - accuracy: 0.7397 - val_loss: 0.6709 - val_accuracy: 0.7743\n",
      "Epoch 91/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7714 - accuracy: 0.7335 - val_loss: 0.6699 - val_accuracy: 0.7733\n",
      "Epoch 92/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7552 - accuracy: 0.7436 - val_loss: 0.7021 - val_accuracy: 0.7684\n",
      "Epoch 93/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7675 - accuracy: 0.7374 - val_loss: 0.6601 - val_accuracy: 0.7792\n",
      "Epoch 94/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7699 - accuracy: 0.7396 - val_loss: 0.6834 - val_accuracy: 0.7689\n",
      "Epoch 95/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7592 - accuracy: 0.7402 - val_loss: 0.7098 - val_accuracy: 0.7661\n",
      "Epoch 96/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7454 - accuracy: 0.7464 - val_loss: 0.6586 - val_accuracy: 0.7804\n",
      "Epoch 97/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7594 - accuracy: 0.7398 - val_loss: 0.6670 - val_accuracy: 0.7777\n",
      "Epoch 98/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7524 - accuracy: 0.7434 - val_loss: 0.6632 - val_accuracy: 0.7767\n",
      "Epoch 99/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7556 - accuracy: 0.7419 - val_loss: 0.6816 - val_accuracy: 0.7744\n",
      "Epoch 100/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7633 - accuracy: 0.7395 - val_loss: 0.7147 - val_accuracy: 0.7611\n",
      "Epoch 101/400\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.7561 - accuracy: 0.7413 - val_loss: 0.6597 - val_accuracy: 0.7808\n",
      "Epoch 102/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7524 - accuracy: 0.7441 - val_loss: 0.7020 - val_accuracy: 0.7641\n",
      "Epoch 103/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7587 - accuracy: 0.7401 - val_loss: 0.6771 - val_accuracy: 0.7724\n",
      "Epoch 104/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7514 - accuracy: 0.7425 - val_loss: 0.6642 - val_accuracy: 0.7794\n",
      "Epoch 105/400\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.7514 - accuracy: 0.7412 - val_loss: 0.6710 - val_accuracy: 0.7780\n",
      "Epoch 106/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7451 - accuracy: 0.7437 - val_loss: 0.6682 - val_accuracy: 0.7719\n",
      "Epoch 107/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7576 - accuracy: 0.7419 - val_loss: 0.7010 - val_accuracy: 0.7619\n",
      "Epoch 108/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7603 - accuracy: 0.7387 - val_loss: 0.6513 - val_accuracy: 0.7794\n",
      "Epoch 109/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7579 - accuracy: 0.7403 - val_loss: 0.6736 - val_accuracy: 0.7694\n",
      "Epoch 110/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7556 - accuracy: 0.7420 - val_loss: 0.6962 - val_accuracy: 0.7617\n",
      "Epoch 111/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7603 - accuracy: 0.7402 - val_loss: 0.6785 - val_accuracy: 0.7706\n",
      "Epoch 112/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7523 - accuracy: 0.7421 - val_loss: 0.6823 - val_accuracy: 0.7711\n",
      "Epoch 113/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7474 - accuracy: 0.7444 - val_loss: 0.7163 - val_accuracy: 0.7569\n",
      "Epoch 114/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7537 - accuracy: 0.7411 - val_loss: 0.6975 - val_accuracy: 0.7688\n",
      "Epoch 115/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7655 - accuracy: 0.7396 - val_loss: 0.6927 - val_accuracy: 0.7690\n",
      "Epoch 116/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7626 - accuracy: 0.7389 - val_loss: 0.6727 - val_accuracy: 0.7749\n",
      "Epoch 117/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7586 - accuracy: 0.7382 - val_loss: 0.6678 - val_accuracy: 0.7758\n",
      "Epoch 118/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7502 - accuracy: 0.7427 - val_loss: 0.6599 - val_accuracy: 0.7829\n",
      "Epoch 119/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7522 - accuracy: 0.7447 - val_loss: 0.6774 - val_accuracy: 0.7773\n",
      "Epoch 120/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7524 - accuracy: 0.7449 - val_loss: 0.6836 - val_accuracy: 0.7712\n",
      "Epoch 121/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7478 - accuracy: 0.7440 - val_loss: 0.7303 - val_accuracy: 0.7497\n",
      "Epoch 122/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7503 - accuracy: 0.7426 - val_loss: 0.6728 - val_accuracy: 0.7739\n",
      "Epoch 123/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7500 - accuracy: 0.7459 - val_loss: 0.6618 - val_accuracy: 0.7778\n",
      "Epoch 124/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7576 - accuracy: 0.7444 - val_loss: 0.6887 - val_accuracy: 0.7652\n",
      "Epoch 125/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7558 - accuracy: 0.7433 - val_loss: 0.6544 - val_accuracy: 0.7763\n",
      "Epoch 126/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7504 - accuracy: 0.7414 - val_loss: 0.6634 - val_accuracy: 0.7784\n",
      "Epoch 127/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7489 - accuracy: 0.7457 - val_loss: 0.6624 - val_accuracy: 0.7780\n",
      "Epoch 128/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7518 - accuracy: 0.7402 - val_loss: 0.6920 - val_accuracy: 0.7717\n",
      "Epoch 129/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7465 - accuracy: 0.7437 - val_loss: 0.6993 - val_accuracy: 0.7692\n",
      "Epoch 130/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7524 - accuracy: 0.7419 - val_loss: 0.7096 - val_accuracy: 0.7657\n",
      "Epoch 131/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7487 - accuracy: 0.7459 - val_loss: 0.7145 - val_accuracy: 0.7608\n",
      "Epoch 132/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7524 - accuracy: 0.7440 - val_loss: 0.6918 - val_accuracy: 0.7620\n",
      "Epoch 133/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7535 - accuracy: 0.7458 - val_loss: 0.6664 - val_accuracy: 0.7798\n",
      "Epoch 134/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7524 - accuracy: 0.7415 - val_loss: 0.6788 - val_accuracy: 0.7722\n",
      "Epoch 135/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7513 - accuracy: 0.7459 - val_loss: 0.6966 - val_accuracy: 0.7681\n",
      "Epoch 136/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7532 - accuracy: 0.7444 - val_loss: 0.6651 - val_accuracy: 0.7803\n",
      "Epoch 137/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7479 - accuracy: 0.7460 - val_loss: 0.7490 - val_accuracy: 0.7465\n",
      "Epoch 138/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7493 - accuracy: 0.7450 - val_loss: 0.7028 - val_accuracy: 0.7626\n",
      "Epoch 139/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7472 - accuracy: 0.7447 - val_loss: 0.6999 - val_accuracy: 0.7710\n",
      "Epoch 140/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7492 - accuracy: 0.7437 - val_loss: 0.6627 - val_accuracy: 0.7728\n",
      "Epoch 141/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7528 - accuracy: 0.7425 - val_loss: 0.6867 - val_accuracy: 0.7826\n",
      "Epoch 142/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7469 - accuracy: 0.7447 - val_loss: 0.7118 - val_accuracy: 0.7623\n",
      "Epoch 143/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7408 - accuracy: 0.7470 - val_loss: 0.6662 - val_accuracy: 0.7753\n",
      "Epoch 144/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7473 - accuracy: 0.7455 - val_loss: 0.7026 - val_accuracy: 0.7703\n",
      "Epoch 145/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7449 - accuracy: 0.7467 - val_loss: 0.6917 - val_accuracy: 0.7657\n",
      "Epoch 146/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7453 - accuracy: 0.7453 - val_loss: 0.7025 - val_accuracy: 0.7605\n",
      "Epoch 147/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7528 - accuracy: 0.7414 - val_loss: 0.6887 - val_accuracy: 0.7668\n",
      "Epoch 148/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7520 - accuracy: 0.7462 - val_loss: 0.6930 - val_accuracy: 0.7749\n",
      "Epoch 149/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7526 - accuracy: 0.7443 - val_loss: 0.6798 - val_accuracy: 0.7758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 150/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7433 - accuracy: 0.7445 - val_loss: 0.6844 - val_accuracy: 0.7674\n",
      "Epoch 151/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7398 - accuracy: 0.7468 - val_loss: 0.6613 - val_accuracy: 0.7849\n",
      "Epoch 152/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7517 - accuracy: 0.7477 - val_loss: 0.7072 - val_accuracy: 0.7641\n",
      "Epoch 153/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7504 - accuracy: 0.7438 - val_loss: 0.6897 - val_accuracy: 0.7652\n",
      "Epoch 154/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7574 - accuracy: 0.7416 - val_loss: 0.6816 - val_accuracy: 0.7764\n",
      "Epoch 155/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7503 - accuracy: 0.7442 - val_loss: 0.7245 - val_accuracy: 0.7517\n",
      "Epoch 156/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7463 - accuracy: 0.7435 - val_loss: 0.6623 - val_accuracy: 0.7794\n",
      "Epoch 157/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7403 - accuracy: 0.7485 - val_loss: 0.6692 - val_accuracy: 0.7773\n",
      "Epoch 158/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7481 - accuracy: 0.7435 - val_loss: 0.6725 - val_accuracy: 0.7766\n",
      "Epoch 159/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7402 - accuracy: 0.7435 - val_loss: 0.6890 - val_accuracy: 0.7739\n",
      "Epoch 160/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7481 - accuracy: 0.7451 - val_loss: 0.6638 - val_accuracy: 0.7819\n",
      "Epoch 161/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7449 - accuracy: 0.7453 - val_loss: 0.7335 - val_accuracy: 0.7533\n",
      "Epoch 162/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7425 - accuracy: 0.7473 - val_loss: 0.6832 - val_accuracy: 0.7785\n",
      "Epoch 163/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7478 - accuracy: 0.7469 - val_loss: 0.7046 - val_accuracy: 0.7667\n",
      "Epoch 164/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7398 - accuracy: 0.7472 - val_loss: 0.6656 - val_accuracy: 0.7754\n",
      "Epoch 165/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7505 - accuracy: 0.7437 - val_loss: 0.7577 - val_accuracy: 0.7437\n",
      "Epoch 166/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7490 - accuracy: 0.7451 - val_loss: 0.6782 - val_accuracy: 0.7700\n",
      "Epoch 167/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7508 - accuracy: 0.7451 - val_loss: 0.6878 - val_accuracy: 0.7712\n",
      "Epoch 168/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7540 - accuracy: 0.7420 - val_loss: 0.6761 - val_accuracy: 0.7771\n",
      "Epoch 169/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7482 - accuracy: 0.7442 - val_loss: 0.6898 - val_accuracy: 0.7694\n",
      "Epoch 170/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7526 - accuracy: 0.7438 - val_loss: 0.6397 - val_accuracy: 0.7851\n",
      "Epoch 171/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7417 - accuracy: 0.7460 - val_loss: 0.6730 - val_accuracy: 0.7724\n",
      "Epoch 172/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7500 - accuracy: 0.7443 - val_loss: 0.6652 - val_accuracy: 0.7770\n",
      "Epoch 173/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7473 - accuracy: 0.7461 - val_loss: 0.6821 - val_accuracy: 0.7682\n",
      "Epoch 174/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7466 - accuracy: 0.7448 - val_loss: 0.6579 - val_accuracy: 0.7809\n",
      "Epoch 175/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7408 - accuracy: 0.7473 - val_loss: 0.6663 - val_accuracy: 0.7745\n",
      "Epoch 176/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7519 - accuracy: 0.7439 - val_loss: 0.7055 - val_accuracy: 0.7623\n",
      "Epoch 177/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7513 - accuracy: 0.7444 - val_loss: 0.6912 - val_accuracy: 0.7705\n",
      "Epoch 178/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7534 - accuracy: 0.7429 - val_loss: 0.6924 - val_accuracy: 0.7669\n",
      "Epoch 179/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7527 - accuracy: 0.7440 - val_loss: 0.7039 - val_accuracy: 0.7666\n",
      "Epoch 180/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7442 - accuracy: 0.7460 - val_loss: 0.7036 - val_accuracy: 0.7633\n",
      "Epoch 181/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7418 - accuracy: 0.7457 - val_loss: 0.6515 - val_accuracy: 0.7800\n",
      "Epoch 182/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7507 - accuracy: 0.7465 - val_loss: 0.6472 - val_accuracy: 0.7847\n",
      "Epoch 183/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7431 - accuracy: 0.7479 - val_loss: 0.6669 - val_accuracy: 0.7793\n",
      "Epoch 184/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7391 - accuracy: 0.7498 - val_loss: 0.6826 - val_accuracy: 0.7718\n",
      "Epoch 185/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7445 - accuracy: 0.7466 - val_loss: 0.6658 - val_accuracy: 0.7750\n",
      "Epoch 186/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7439 - accuracy: 0.7480 - val_loss: 0.6656 - val_accuracy: 0.7722\n",
      "Epoch 187/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7452 - accuracy: 0.7480 - val_loss: 0.6632 - val_accuracy: 0.7805\n",
      "Epoch 188/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7406 - accuracy: 0.7495 - val_loss: 0.6566 - val_accuracy: 0.7817\n",
      "Epoch 189/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7424 - accuracy: 0.7474 - val_loss: 0.6605 - val_accuracy: 0.7827\n",
      "Epoch 190/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7468 - accuracy: 0.7478 - val_loss: 0.6779 - val_accuracy: 0.7735\n",
      "Epoch 191/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7481 - accuracy: 0.7436 - val_loss: 0.6771 - val_accuracy: 0.7715\n",
      "Epoch 192/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7409 - accuracy: 0.7474 - val_loss: 0.6741 - val_accuracy: 0.7815\n",
      "Epoch 193/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7512 - accuracy: 0.7448 - val_loss: 0.6956 - val_accuracy: 0.7655\n",
      "Epoch 194/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7442 - accuracy: 0.7448 - val_loss: 0.6890 - val_accuracy: 0.7675\n",
      "Epoch 195/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7480 - accuracy: 0.7454 - val_loss: 0.6832 - val_accuracy: 0.7694\n",
      "Epoch 196/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7440 - accuracy: 0.7467 - val_loss: 0.6912 - val_accuracy: 0.7669\n",
      "Epoch 197/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7458 - accuracy: 0.7454 - val_loss: 0.6837 - val_accuracy: 0.7705\n",
      "Epoch 198/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7466 - accuracy: 0.7459 - val_loss: 0.6752 - val_accuracy: 0.7773\n",
      "Epoch 199/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7401 - accuracy: 0.7458 - val_loss: 0.6621 - val_accuracy: 0.7837\n",
      "Epoch 200/400\n",
      "782/782 [==============================] - 25s 32ms/step - loss: 0.7521 - accuracy: 0.7462 - val_loss: 0.6731 - val_accuracy: 0.7735\n",
      "Epoch 201/400\n",
      "782/782 [==============================] - 25s 33ms/step - loss: 0.7563 - accuracy: 0.7430 - val_loss: 0.6745 - val_accuracy: 0.7727\n",
      "Epoch 202/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7484 - accuracy: 0.7460 - val_loss: 0.6589 - val_accuracy: 0.7769\n",
      "Epoch 203/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7530 - accuracy: 0.7437 - val_loss: 0.7003 - val_accuracy: 0.7722\n",
      "Epoch 204/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7564 - accuracy: 0.7427 - val_loss: 0.6600 - val_accuracy: 0.7806\n",
      "Epoch 205/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7416 - accuracy: 0.7472 - val_loss: 0.6782 - val_accuracy: 0.7731\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 206/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7378 - accuracy: 0.7487 - val_loss: 0.6701 - val_accuracy: 0.7780\n",
      "Epoch 207/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7586 - accuracy: 0.7401 - val_loss: 0.7092 - val_accuracy: 0.7687\n",
      "Epoch 208/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7490 - accuracy: 0.7458 - val_loss: 0.6423 - val_accuracy: 0.7862\n",
      "Epoch 209/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7503 - accuracy: 0.7425 - val_loss: 0.6921 - val_accuracy: 0.7672\n",
      "Epoch 210/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7384 - accuracy: 0.7495 - val_loss: 0.6607 - val_accuracy: 0.7805\n",
      "Epoch 211/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7428 - accuracy: 0.7471 - val_loss: 0.6638 - val_accuracy: 0.7782\n",
      "Epoch 212/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7425 - accuracy: 0.7468 - val_loss: 0.7157 - val_accuracy: 0.7648\n",
      "Epoch 213/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7467 - accuracy: 0.7469 - val_loss: 0.6871 - val_accuracy: 0.7717\n",
      "Epoch 214/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7396 - accuracy: 0.7496 - val_loss: 0.6887 - val_accuracy: 0.7694\n",
      "Epoch 215/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7374 - accuracy: 0.7488 - val_loss: 0.6640 - val_accuracy: 0.7753\n",
      "Epoch 216/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7476 - accuracy: 0.7489 - val_loss: 0.7133 - val_accuracy: 0.7594\n",
      "Epoch 217/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7482 - accuracy: 0.7445 - val_loss: 0.6615 - val_accuracy: 0.7806\n",
      "Epoch 218/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7379 - accuracy: 0.7483 - val_loss: 0.6733 - val_accuracy: 0.7722\n",
      "Epoch 219/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7465 - accuracy: 0.7465 - val_loss: 0.6631 - val_accuracy: 0.7792\n",
      "Epoch 220/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7417 - accuracy: 0.7489 - val_loss: 0.6667 - val_accuracy: 0.7825\n",
      "Epoch 221/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7437 - accuracy: 0.7464 - val_loss: 0.6478 - val_accuracy: 0.7822\n",
      "Epoch 222/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7521 - accuracy: 0.7455 - val_loss: 0.6576 - val_accuracy: 0.7782\n",
      "Epoch 223/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7471 - accuracy: 0.7447 - val_loss: 0.6625 - val_accuracy: 0.7747\n",
      "Epoch 224/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7358 - accuracy: 0.7507 - val_loss: 0.6841 - val_accuracy: 0.7659\n",
      "Epoch 225/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7465 - accuracy: 0.7480 - val_loss: 0.6695 - val_accuracy: 0.7802\n",
      "Epoch 226/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7430 - accuracy: 0.7486 - val_loss: 0.6820 - val_accuracy: 0.7674\n",
      "Epoch 227/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7512 - accuracy: 0.7430 - val_loss: 0.6892 - val_accuracy: 0.7730\n",
      "Epoch 228/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7472 - accuracy: 0.7468 - val_loss: 0.6676 - val_accuracy: 0.7734\n",
      "Epoch 229/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7402 - accuracy: 0.7480 - val_loss: 0.6962 - val_accuracy: 0.7694\n",
      "Epoch 230/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7463 - accuracy: 0.7451 - val_loss: 0.6800 - val_accuracy: 0.7771\n",
      "Epoch 231/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7396 - accuracy: 0.7487 - val_loss: 0.6498 - val_accuracy: 0.7853\n",
      "Epoch 232/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7405 - accuracy: 0.7481 - val_loss: 0.6834 - val_accuracy: 0.7720\n",
      "Epoch 233/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7460 - accuracy: 0.7480 - val_loss: 0.6718 - val_accuracy: 0.7740\n",
      "Epoch 234/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7471 - accuracy: 0.7459 - val_loss: 0.6666 - val_accuracy: 0.7801\n",
      "Epoch 235/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7502 - accuracy: 0.7461 - val_loss: 0.7005 - val_accuracy: 0.7694\n",
      "Epoch 236/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7559 - accuracy: 0.7444 - val_loss: 0.6606 - val_accuracy: 0.7804\n",
      "Epoch 237/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7525 - accuracy: 0.7456 - val_loss: 0.6829 - val_accuracy: 0.7705\n",
      "Epoch 238/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7450 - accuracy: 0.7459 - val_loss: 0.6895 - val_accuracy: 0.7720\n",
      "Epoch 239/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7544 - accuracy: 0.7451 - val_loss: 0.6687 - val_accuracy: 0.7740\n",
      "Epoch 240/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7451 - accuracy: 0.7463 - val_loss: 0.6675 - val_accuracy: 0.7796\n",
      "Epoch 241/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7355 - accuracy: 0.7485 - val_loss: 0.6590 - val_accuracy: 0.7769\n",
      "Epoch 242/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7486 - accuracy: 0.7467 - val_loss: 0.6780 - val_accuracy: 0.7721\n",
      "Epoch 243/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7501 - accuracy: 0.7461 - val_loss: 0.6993 - val_accuracy: 0.7692\n",
      "Epoch 244/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7492 - accuracy: 0.7451 - val_loss: 0.7276 - val_accuracy: 0.7547\n",
      "Epoch 245/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7419 - accuracy: 0.7467 - val_loss: 0.6857 - val_accuracy: 0.7715\n",
      "Epoch 246/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7492 - accuracy: 0.7454 - val_loss: 0.6543 - val_accuracy: 0.7829\n",
      "Epoch 247/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7436 - accuracy: 0.7464 - val_loss: 0.6441 - val_accuracy: 0.7855\n",
      "Epoch 248/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7421 - accuracy: 0.7473 - val_loss: 0.6731 - val_accuracy: 0.7814\n",
      "Epoch 249/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7499 - accuracy: 0.7461 - val_loss: 0.6852 - val_accuracy: 0.7672\n",
      "Epoch 250/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7462 - accuracy: 0.7462 - val_loss: 0.6684 - val_accuracy: 0.7791\n",
      "Epoch 251/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7487 - accuracy: 0.7442 - val_loss: 0.7078 - val_accuracy: 0.7702\n",
      "Epoch 252/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7473 - accuracy: 0.7476 - val_loss: 0.6824 - val_accuracy: 0.7715\n",
      "Epoch 253/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7443 - accuracy: 0.7486 - val_loss: 0.6540 - val_accuracy: 0.7833\n",
      "Epoch 254/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7410 - accuracy: 0.7479 - val_loss: 0.6811 - val_accuracy: 0.7772\n",
      "Epoch 255/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7457 - accuracy: 0.7474 - val_loss: 0.6716 - val_accuracy: 0.7807\n",
      "Epoch 256/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7514 - accuracy: 0.7464 - val_loss: 0.6675 - val_accuracy: 0.7803\n",
      "Epoch 257/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7425 - accuracy: 0.7484 - val_loss: 0.7015 - val_accuracy: 0.7696\n",
      "Epoch 258/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7374 - accuracy: 0.7500 - val_loss: 0.6579 - val_accuracy: 0.7847\n",
      "Epoch 259/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7382 - accuracy: 0.7482 - val_loss: 0.6761 - val_accuracy: 0.7775\n",
      "Epoch 260/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7460 - accuracy: 0.7473 - val_loss: 0.6699 - val_accuracy: 0.7790\n",
      "Epoch 261/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7417 - accuracy: 0.7478 - val_loss: 0.6996 - val_accuracy: 0.7629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 262/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7395 - accuracy: 0.7497 - val_loss: 0.7220 - val_accuracy: 0.7565\n",
      "Epoch 263/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7356 - accuracy: 0.7521 - val_loss: 0.6603 - val_accuracy: 0.7833\n",
      "Epoch 264/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7479 - accuracy: 0.7446 - val_loss: 0.7012 - val_accuracy: 0.7689\n",
      "Epoch 265/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7457 - accuracy: 0.7468 - val_loss: 0.6551 - val_accuracy: 0.7828\n",
      "Epoch 266/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7420 - accuracy: 0.7491 - val_loss: 0.7313 - val_accuracy: 0.7600\n",
      "Epoch 267/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7445 - accuracy: 0.7485 - val_loss: 0.6477 - val_accuracy: 0.7815\n",
      "Epoch 268/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7496 - accuracy: 0.7471 - val_loss: 0.6790 - val_accuracy: 0.7761\n",
      "Epoch 269/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7406 - accuracy: 0.7476 - val_loss: 0.6748 - val_accuracy: 0.7780\n",
      "Epoch 270/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7394 - accuracy: 0.7479 - val_loss: 0.6895 - val_accuracy: 0.7711\n",
      "Epoch 271/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7434 - accuracy: 0.7485 - val_loss: 0.6608 - val_accuracy: 0.7825\n",
      "Epoch 272/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7343 - accuracy: 0.7502 - val_loss: 0.6759 - val_accuracy: 0.7772\n",
      "Epoch 273/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7556 - accuracy: 0.7429 - val_loss: 0.6533 - val_accuracy: 0.7860\n",
      "Epoch 274/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7476 - accuracy: 0.7456 - val_loss: 0.7295 - val_accuracy: 0.7565\n",
      "Epoch 275/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7498 - accuracy: 0.7475 - val_loss: 0.6918 - val_accuracy: 0.7718\n",
      "Epoch 276/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7393 - accuracy: 0.7499 - val_loss: 0.6908 - val_accuracy: 0.7751\n",
      "Epoch 277/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7379 - accuracy: 0.7498 - val_loss: 0.6716 - val_accuracy: 0.7833\n",
      "Epoch 278/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7465 - accuracy: 0.7467 - val_loss: 0.6713 - val_accuracy: 0.7752\n",
      "Epoch 279/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7504 - accuracy: 0.7450 - val_loss: 0.6821 - val_accuracy: 0.7698\n",
      "Epoch 280/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7502 - accuracy: 0.7458 - val_loss: 0.7025 - val_accuracy: 0.7632\n",
      "Epoch 281/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7409 - accuracy: 0.7513 - val_loss: 0.6764 - val_accuracy: 0.7699\n",
      "Epoch 282/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7431 - accuracy: 0.7484 - val_loss: 0.6917 - val_accuracy: 0.7694\n",
      "Epoch 283/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7421 - accuracy: 0.7495 - val_loss: 0.7016 - val_accuracy: 0.7652\n",
      "Epoch 284/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7508 - accuracy: 0.7458 - val_loss: 0.6609 - val_accuracy: 0.7845\n",
      "Epoch 285/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7405 - accuracy: 0.7479 - val_loss: 0.6938 - val_accuracy: 0.7710\n",
      "Epoch 286/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7462 - accuracy: 0.7479 - val_loss: 0.7258 - val_accuracy: 0.7617\n",
      "Epoch 287/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7511 - accuracy: 0.7449 - val_loss: 0.6858 - val_accuracy: 0.7711\n",
      "Epoch 288/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7509 - accuracy: 0.7461 - val_loss: 0.6809 - val_accuracy: 0.7735\n",
      "Epoch 289/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7482 - accuracy: 0.7467 - val_loss: 0.6558 - val_accuracy: 0.7766\n",
      "Epoch 290/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7409 - accuracy: 0.7505 - val_loss: 0.6785 - val_accuracy: 0.7736\n",
      "Epoch 291/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7419 - accuracy: 0.7488 - val_loss: 0.6650 - val_accuracy: 0.7825\n",
      "Epoch 292/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7372 - accuracy: 0.7508 - val_loss: 0.6730 - val_accuracy: 0.7750\n",
      "Epoch 293/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7493 - accuracy: 0.7446 - val_loss: 0.6659 - val_accuracy: 0.7742\n",
      "Epoch 294/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7400 - accuracy: 0.7484 - val_loss: 0.6752 - val_accuracy: 0.7763\n",
      "Epoch 295/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7455 - accuracy: 0.7464 - val_loss: 0.6706 - val_accuracy: 0.7767\n",
      "Epoch 296/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7401 - accuracy: 0.7497 - val_loss: 0.6725 - val_accuracy: 0.7741\n",
      "Epoch 297/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7496 - accuracy: 0.7445 - val_loss: 0.6660 - val_accuracy: 0.7812\n",
      "Epoch 298/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7412 - accuracy: 0.7497 - val_loss: 0.7180 - val_accuracy: 0.7617\n",
      "Epoch 299/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7465 - accuracy: 0.7444 - val_loss: 0.7164 - val_accuracy: 0.7608\n",
      "Epoch 300/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7525 - accuracy: 0.7450 - val_loss: 0.6723 - val_accuracy: 0.7780\n",
      "Epoch 301/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7461 - accuracy: 0.7463 - val_loss: 0.6898 - val_accuracy: 0.7718\n",
      "Epoch 302/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7430 - accuracy: 0.7487 - val_loss: 0.6643 - val_accuracy: 0.7796\n",
      "Epoch 303/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7517 - accuracy: 0.7455 - val_loss: 0.6839 - val_accuracy: 0.7715\n",
      "Epoch 304/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7472 - accuracy: 0.7466 - val_loss: 0.6662 - val_accuracy: 0.7755\n",
      "Epoch 305/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7427 - accuracy: 0.7448 - val_loss: 0.6736 - val_accuracy: 0.7816\n",
      "Epoch 306/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7396 - accuracy: 0.7517 - val_loss: 0.6854 - val_accuracy: 0.7730\n",
      "Epoch 307/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7596 - accuracy: 0.7430 - val_loss: 0.7026 - val_accuracy: 0.7675\n",
      "Epoch 308/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7464 - accuracy: 0.7470 - val_loss: 0.6681 - val_accuracy: 0.7786\n",
      "Epoch 309/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7442 - accuracy: 0.7485 - val_loss: 0.6796 - val_accuracy: 0.7762\n",
      "Epoch 310/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7459 - accuracy: 0.7451 - val_loss: 0.6920 - val_accuracy: 0.7702\n",
      "Epoch 311/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7449 - accuracy: 0.7498 - val_loss: 0.6919 - val_accuracy: 0.7684\n",
      "Epoch 312/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7424 - accuracy: 0.7482 - val_loss: 0.7202 - val_accuracy: 0.7602\n",
      "Epoch 313/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7491 - accuracy: 0.7468 - val_loss: 0.6933 - val_accuracy: 0.7680\n",
      "Epoch 314/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7406 - accuracy: 0.7491 - val_loss: 0.6598 - val_accuracy: 0.7823\n",
      "Epoch 315/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7465 - accuracy: 0.7466 - val_loss: 0.6541 - val_accuracy: 0.7769\n",
      "Epoch 316/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7493 - accuracy: 0.7465 - val_loss: 0.6966 - val_accuracy: 0.7667\n",
      "Epoch 317/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7637 - accuracy: 0.7428 - val_loss: 0.6772 - val_accuracy: 0.7784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 318/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7489 - accuracy: 0.7456 - val_loss: 0.7043 - val_accuracy: 0.7667\n",
      "Epoch 319/400\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.7515 - accuracy: 0.7458 - val_loss: 0.6660 - val_accuracy: 0.7778\n",
      "Epoch 320/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7372 - accuracy: 0.7483 - val_loss: 0.6990 - val_accuracy: 0.7661\n",
      "Epoch 321/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7415 - accuracy: 0.7494 - val_loss: 0.6584 - val_accuracy: 0.7855\n",
      "Epoch 322/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7495 - accuracy: 0.7455 - val_loss: 0.6878 - val_accuracy: 0.7728\n",
      "Epoch 323/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7422 - accuracy: 0.7474 - val_loss: 0.6733 - val_accuracy: 0.7742\n",
      "Epoch 324/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7448 - accuracy: 0.7457 - val_loss: 0.6676 - val_accuracy: 0.7801\n",
      "Epoch 325/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7623 - accuracy: 0.7398 - val_loss: 0.7300 - val_accuracy: 0.7540\n",
      "Epoch 326/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7539 - accuracy: 0.7437 - val_loss: 0.6857 - val_accuracy: 0.7710\n",
      "Epoch 327/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7420 - accuracy: 0.7488 - val_loss: 0.7042 - val_accuracy: 0.7636\n",
      "Epoch 328/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7421 - accuracy: 0.7483 - val_loss: 0.6687 - val_accuracy: 0.7779\n",
      "Epoch 329/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7440 - accuracy: 0.7496 - val_loss: 0.6963 - val_accuracy: 0.7684\n",
      "Epoch 330/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7415 - accuracy: 0.7462 - val_loss: 0.6897 - val_accuracy: 0.7727\n",
      "Epoch 331/400\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.7401 - accuracy: 0.7468 - val_loss: 0.6694 - val_accuracy: 0.7738\n",
      "Epoch 332/400\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.7308 - accuracy: 0.7515 - val_loss: 0.6713 - val_accuracy: 0.7779\n",
      "Epoch 333/400\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.7454 - accuracy: 0.7481 - val_loss: 0.6948 - val_accuracy: 0.7759\n",
      "Epoch 334/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7351 - accuracy: 0.7520 - val_loss: 0.6624 - val_accuracy: 0.7726\n",
      "Epoch 335/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7527 - accuracy: 0.7466 - val_loss: 0.6718 - val_accuracy: 0.7797\n",
      "Epoch 336/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7360 - accuracy: 0.7504 - val_loss: 0.6653 - val_accuracy: 0.7838\n",
      "Epoch 337/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7389 - accuracy: 0.7516 - val_loss: 0.6977 - val_accuracy: 0.7688\n",
      "Epoch 338/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7419 - accuracy: 0.7496 - val_loss: 0.6915 - val_accuracy: 0.7727\n",
      "Epoch 339/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7439 - accuracy: 0.7505 - val_loss: 0.6683 - val_accuracy: 0.7795\n",
      "Epoch 340/400\n",
      "782/782 [==============================] - 28s 35ms/step - loss: 0.7531 - accuracy: 0.7461 - val_loss: 0.6613 - val_accuracy: 0.7801\n",
      "Epoch 341/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7548 - accuracy: 0.7449 - val_loss: 0.7031 - val_accuracy: 0.7650\n",
      "Epoch 342/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7564 - accuracy: 0.7448 - val_loss: 0.6686 - val_accuracy: 0.7727\n",
      "Epoch 343/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7409 - accuracy: 0.7465 - val_loss: 0.6734 - val_accuracy: 0.7755\n",
      "Epoch 344/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7525 - accuracy: 0.7464 - val_loss: 0.6891 - val_accuracy: 0.7674\n",
      "Epoch 345/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7508 - accuracy: 0.7441 - val_loss: 0.6729 - val_accuracy: 0.7745\n",
      "Epoch 346/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7485 - accuracy: 0.7473 - val_loss: 0.6756 - val_accuracy: 0.7772\n",
      "Epoch 347/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7577 - accuracy: 0.7432 - val_loss: 0.6735 - val_accuracy: 0.7737\n",
      "Epoch 348/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7393 - accuracy: 0.7475 - val_loss: 0.6776 - val_accuracy: 0.7731\n",
      "Epoch 349/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7457 - accuracy: 0.7471 - val_loss: 0.6675 - val_accuracy: 0.7755\n",
      "Epoch 350/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7546 - accuracy: 0.7467 - val_loss: 0.6725 - val_accuracy: 0.7725\n",
      "Epoch 351/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7471 - accuracy: 0.7466 - val_loss: 0.6781 - val_accuracy: 0.7720\n",
      "Epoch 352/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7492 - accuracy: 0.7462 - val_loss: 0.6608 - val_accuracy: 0.7797\n",
      "Epoch 353/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7451 - accuracy: 0.7480 - val_loss: 0.6872 - val_accuracy: 0.7778\n",
      "Epoch 354/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7452 - accuracy: 0.7455 - val_loss: 0.6821 - val_accuracy: 0.7754\n",
      "Epoch 355/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7583 - accuracy: 0.7429 - val_loss: 0.6703 - val_accuracy: 0.7763\n",
      "Epoch 356/400\n",
      "782/782 [==============================] - 28s 36ms/step - loss: 0.7444 - accuracy: 0.7452 - val_loss: 0.6833 - val_accuracy: 0.7792\n",
      "Epoch 357/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7543 - accuracy: 0.7435 - val_loss: 0.6940 - val_accuracy: 0.7679\n",
      "Epoch 358/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7577 - accuracy: 0.7441 - val_loss: 0.6693 - val_accuracy: 0.7781\n",
      "Epoch 359/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7481 - accuracy: 0.7445 - val_loss: 0.6879 - val_accuracy: 0.7717\n",
      "Epoch 360/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7541 - accuracy: 0.7435 - val_loss: 0.6991 - val_accuracy: 0.7700\n",
      "Epoch 361/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7470 - accuracy: 0.7471 - val_loss: 0.7484 - val_accuracy: 0.7529\n",
      "Epoch 362/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7503 - accuracy: 0.7456 - val_loss: 0.6622 - val_accuracy: 0.7829\n",
      "Epoch 363/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7600 - accuracy: 0.7447 - val_loss: 0.6749 - val_accuracy: 0.7705\n",
      "Epoch 364/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7622 - accuracy: 0.7410 - val_loss: 0.6923 - val_accuracy: 0.7690\n",
      "Epoch 365/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7523 - accuracy: 0.7450 - val_loss: 0.6767 - val_accuracy: 0.7752\n",
      "Epoch 366/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7593 - accuracy: 0.7408 - val_loss: 0.7034 - val_accuracy: 0.7630\n",
      "Epoch 367/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7520 - accuracy: 0.7440 - val_loss: 0.6827 - val_accuracy: 0.7747\n",
      "Epoch 368/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7471 - accuracy: 0.7482 - val_loss: 0.6939 - val_accuracy: 0.7691\n",
      "Epoch 369/400\n",
      "782/782 [==============================] - 27s 35ms/step - loss: 0.7527 - accuracy: 0.7457 - val_loss: 0.6839 - val_accuracy: 0.7727\n",
      "Epoch 370/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7480 - accuracy: 0.7457 - val_loss: 0.6766 - val_accuracy: 0.7775\n",
      "Epoch 371/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7542 - accuracy: 0.7422 - val_loss: 0.6914 - val_accuracy: 0.7633\n",
      "Epoch 372/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7515 - accuracy: 0.7440 - val_loss: 0.6830 - val_accuracy: 0.7750\n",
      "Epoch 373/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7592 - accuracy: 0.7423 - val_loss: 0.6686 - val_accuracy: 0.7810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 374/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7480 - accuracy: 0.7481 - val_loss: 0.6821 - val_accuracy: 0.7683\n",
      "Epoch 375/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7525 - accuracy: 0.7449 - val_loss: 0.6754 - val_accuracy: 0.7784\n",
      "Epoch 376/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7507 - accuracy: 0.7469 - val_loss: 0.6964 - val_accuracy: 0.7672\n",
      "Epoch 377/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7482 - accuracy: 0.7462 - val_loss: 0.6762 - val_accuracy: 0.7762\n",
      "Epoch 378/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7460 - accuracy: 0.7473 - val_loss: 0.6545 - val_accuracy: 0.7816\n",
      "Epoch 379/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7548 - accuracy: 0.7427 - val_loss: 0.6713 - val_accuracy: 0.7783\n",
      "Epoch 380/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7473 - accuracy: 0.7469 - val_loss: 0.6934 - val_accuracy: 0.7681\n",
      "Epoch 381/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7572 - accuracy: 0.7422 - val_loss: 0.6961 - val_accuracy: 0.7676\n",
      "Epoch 382/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7424 - accuracy: 0.7480 - val_loss: 0.6964 - val_accuracy: 0.7720\n",
      "Epoch 383/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7639 - accuracy: 0.7409 - val_loss: 0.7018 - val_accuracy: 0.7640\n",
      "Epoch 384/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7503 - accuracy: 0.7456 - val_loss: 0.6753 - val_accuracy: 0.7756\n",
      "Epoch 385/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7491 - accuracy: 0.7463 - val_loss: 0.6787 - val_accuracy: 0.7798\n",
      "Epoch 386/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7459 - accuracy: 0.7475 - val_loss: 0.6682 - val_accuracy: 0.7815\n",
      "Epoch 387/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7486 - accuracy: 0.7451 - val_loss: 0.7019 - val_accuracy: 0.7640\n",
      "Epoch 388/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7620 - accuracy: 0.7410 - val_loss: 0.6774 - val_accuracy: 0.7719\n",
      "Epoch 389/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7557 - accuracy: 0.7459 - val_loss: 0.7048 - val_accuracy: 0.7624\n",
      "Epoch 390/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7460 - accuracy: 0.7487 - val_loss: 0.6702 - val_accuracy: 0.7797\n",
      "Epoch 391/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7557 - accuracy: 0.7451 - val_loss: 0.6953 - val_accuracy: 0.7645\n",
      "Epoch 392/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7499 - accuracy: 0.7468 - val_loss: 0.6625 - val_accuracy: 0.7782\n",
      "Epoch 393/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7507 - accuracy: 0.7445 - val_loss: 0.7358 - val_accuracy: 0.7629\n",
      "Epoch 394/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7600 - accuracy: 0.7415 - val_loss: 0.6835 - val_accuracy: 0.7747\n",
      "Epoch 395/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7599 - accuracy: 0.7435 - val_loss: 0.6985 - val_accuracy: 0.7664\n",
      "Epoch 396/400\n",
      "782/782 [==============================] - 27s 34ms/step - loss: 0.7486 - accuracy: 0.7457 - val_loss: 0.6763 - val_accuracy: 0.7773\n",
      "Epoch 397/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7590 - accuracy: 0.7439 - val_loss: 0.6815 - val_accuracy: 0.7763\n",
      "Epoch 398/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7490 - accuracy: 0.7469 - val_loss: 0.6815 - val_accuracy: 0.7803\n",
      "Epoch 399/400\n",
      "782/782 [==============================] - 26s 34ms/step - loss: 0.7619 - accuracy: 0.7415 - val_loss: 0.7192 - val_accuracy: 0.7643\n",
      "Epoch 400/400\n",
      "782/782 [==============================] - 26s 33ms/step - loss: 0.7500 - accuracy: 0.7454 - val_loss: 0.6978 - val_accuracy: 0.7638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda601ddd90>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "model_10.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', \n",
    "                    input_shape=(32, 32, 3)))\n",
    "model_10.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.2))\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.3))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.4))\n",
    "model_10.add(Flatten())\n",
    "model_10.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model_10.add(Dropout(0.5))\n",
    "model_10.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "model_10.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_10_tiny_Base-CubeAI_Dual_3Conv.h5', save_freq='epoch')\n",
    "\n",
    "model_10.summary()\n",
    "\n",
    "# Train and evaluate the quantization aware model\n",
    "model_10.fit(x_train,y_train, batch_size=64,epochs=400,callbacks=[checkpoint_callback],\n",
    "             validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6acbb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 0.6978 - accuracy: 0.7638\n",
      "Test accuracy: 0.7638000249862671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 7). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpojnlx8_a/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpojnlx8_a/assets\n",
      "/home/themandalorian/anaconda3/envs/mlonmcu_lab2/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:765: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-05-29 21:54:56.493528: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2023-05-29 21:54:56.493564: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2023-05-29 21:54:56.494771: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /tmp/tmpojnlx8_a\n",
      "2023-05-29 21:54:56.498400: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2023-05-29 21:54:56.498419: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /tmp/tmpojnlx8_a\n",
      "2023-05-29 21:54:56.506054: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2023-05-29 21:54:56.507648: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2023-05-29 21:54:56.558480: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /tmp/tmpojnlx8_a\n",
      "2023-05-29 21:54:56.571549: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 77129 microseconds.\n",
      "2023-05-29 21:54:56.613048: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Input details ==\n",
      "name: serving_default_conv2d_52_input:0\n",
      "shape: [ 1 32 32  3]\n",
      "type: <class 'numpy.int8'>\n",
      "\n",
      "== Output details ==\n",
      "name: StatefulPartitionedCall:0\n",
      "shape: [ 1 10]\n",
      "type: <class 'numpy.int8'>\n",
      "Accuracy of model_10_tiny_Base-CubeAI_Dual_3Conv_PTQ is 76.62%\n"
     ]
    }
   ],
   "source": [
    "# load the pre-saved model\n",
    "model_10 = load_model('model_10_tiny_Base-CubeAI_Dual_3Conv.h5')\n",
    "\n",
    "# test the accuracy of the model on the test set\n",
    "loss, accuracy = model_10.evaluate(x_test, y_test)\n",
    "\n",
    "# print the accuracy of the model on the test set\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "# Convert Keras model to a tflite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_10)\n",
    "\n",
    "# Convert the model to the TensorFlow Lite format with quantization\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "tflite_model_name = 'model_10_tiny_Base-CubeAI_Dual_3Conv_PTQ'\n",
    "        \n",
    "# Set the optimization flag.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "# Enforce full-int8 quantization\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8  # or tf.uint8\n",
    "converter.inference_output_type = tf.int8  # or tf.uint8\n",
    "# Provide a representative dataset to ensure we quantize correctly.\n",
    "converter.representative_dataset = representative_data_gen\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "open(tflite_model_name + '.tflite', 'wb').write(tflite_model)\n",
    "\n",
    "tflite_interpreter = tf.lite.Interpreter(model_path=tflite_model_name + '.tflite')\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "predictions = np.zeros((len(x_test),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(len(x_test)):\n",
    "    val_batch = x_test[i]\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()\n",
    "    \n",
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 10000\n",
    "print(\"Accuracy of model_10_tiny_Base-CubeAI_Dual_3Conv_PTQ is {}%\".format(accuracy_score*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc89b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_model_name = 'cifar10_tiny_base_cubeAI_dual_3conv_ptq'\n",
    "# check if dir 'cfiles' exists, if not create it\n",
    "if not os.path.exists('cfiles'):\n",
    "    os.makedirs('cfiles')\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb099578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/themandalorian/anaconda3/envs/mlonmcu_lab2/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/themandalorian/anaconda3/envs/mlonmcu_lab2/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " quantize_layer (QuantizeLay  (None, 32, 32, 3)        3         \n",
      " er)                                                             \n",
      "                                                                 \n",
      " quant_conv2d_58 (QuantizeWr  (None, 32, 32, 16)       483       \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_59 (QuantizeWr  (None, 32, 32, 16)       2355      \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_34 (Qua  (None, 16, 16, 16)       1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_dropout_43 (QuantizeW  (None, 16, 16, 16)       1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_conv2d_60 (QuantizeWr  (None, 16, 16, 32)       4707      \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_61 (QuantizeWr  (None, 16, 16, 32)       9315      \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_35 (Qua  (None, 8, 8, 32)         1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_dropout_44 (QuantizeW  (None, 8, 8, 32)         1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_conv2d_62 (QuantizeWr  (None, 8, 8, 64)         18627     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_conv2d_63 (QuantizeWr  (None, 8, 8, 64)         37059     \n",
      " apperV2)                                                        \n",
      "                                                                 \n",
      " quant_max_pooling2d_36 (Qua  (None, 4, 4, 64)         1         \n",
      " ntizeWrapperV2)                                                 \n",
      "                                                                 \n",
      " quant_dropout_45 (QuantizeW  (None, 4, 4, 64)         1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_flatten_10 (QuantizeW  (None, 1024)             1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_20 (QuantizeWra  (None, 128)              131205    \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      " quant_dropout_46 (QuantizeW  (None, 128)              1         \n",
      " rapperV2)                                                       \n",
      "                                                                 \n",
      " quant_dense_21 (QuantizeWra  (None, 10)               1295      \n",
      " pperV2)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 205,057\n",
      "Trainable params: 204,570\n",
      "Non-trainable params: 487\n",
      "_________________________________________________________________\n",
      "Epoch 1/400\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 2.0399 - accuracy: 0.2309 - val_loss: 1.8083 - val_accuracy: 0.2986\n",
      "Epoch 2/400\n",
      "782/782 [==============================] - 33s 43ms/step - loss: 1.7031 - accuracy: 0.3596 - val_loss: 1.4590 - val_accuracy: 0.4540\n",
      "Epoch 3/400\n",
      "782/782 [==============================] - 34s 43ms/step - loss: 1.5442 - accuracy: 0.4301 - val_loss: 1.2969 - val_accuracy: 0.5253\n",
      "Epoch 4/400\n",
      "643/782 [=======================>......] - ETA: 5s - loss: 1.4498 - accuracy: 0.4679"
     ]
    }
   ],
   "source": [
    "model_10 = Sequential()\n",
    "model_10.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', \n",
    "                    input_shape=(32, 32, 3)))\n",
    "model_10.add(Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.2))\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.3))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "model_10.add(MaxPooling2D((2, 2)))\n",
    "model_10.add(Dropout(0.4))\n",
    "model_10.add(Flatten())\n",
    "model_10.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "model_10.add(Dropout(0.5))\n",
    "model_10.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "\n",
    "# Convert the model to a quantization aware model\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(model_10)\n",
    "\n",
    "quant_aware_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(filepath='model_10_tiny_Base-CubeAI_Dual_3Conv_QAT.h5', save_freq='epoch')\n",
    "\n",
    "quant_aware_model.summary()\n",
    "\n",
    "# Train and evaluate the quantization aware model\n",
    "quant_aware_model.fit(x_train,y_train, batch_size=64,epochs=400,\n",
    "                      callbacks=[checkpoint_callback],validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed39fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model on the test set\n",
    "quant_loss, quant_acc = quant_aware_model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Quantization aware training loss: ', quant_loss)\n",
    "print('Quantization aware training accuracy: ', quant_acc)\n",
    "\n",
    "# convert the QAT model to a fully quantized model using TFLite\n",
    "def representative_data_gen():\n",
    "    for input_value in tf.data.Dataset.from_tensor_slices(x_train).batch(1).take(100):\n",
    "        yield [input_value]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_data_gen\n",
    "# Ensure that if any ops can't be quantized, the converter throws an error\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "# Set the input and output tensors to uint8 (APIs added in r2.3)\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "\n",
    "tflite_model_quant_int8_qat = converter.convert()\n",
    "\n",
    "open(\"model_10_tiny_Base-CubeAI_Dual_3Conv_QAT.tflite\", \"wb\").write(tflite_model_quant_int8_qat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e45b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_interpreter = tf.lite.Interpreter(\"model_10_tiny_Base-CubeAI_Dual_3Conv_QAT.tflite\")\n",
    "tflite_interpreter.allocate_tensors()\n",
    "input_details = tflite_interpreter.get_input_details()\n",
    "output_details = tflite_interpreter.get_output_details()\n",
    "\n",
    "print(\"== Input details ==\")\n",
    "print(\"name:\", input_details[0]['name'])\n",
    "print(\"shape:\", input_details[0]['shape'])\n",
    "print(\"type:\", input_details[0]['dtype'])\n",
    "\n",
    "print(\"\\n== Output details ==\")\n",
    "print(\"name:\", output_details[0]['name'])\n",
    "print(\"shape:\", output_details[0]['shape'])\n",
    "print(\"type:\", output_details[0]['dtype'])\n",
    "\n",
    "predictions = np.zeros((len(x_test),), dtype=int)\n",
    "input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
    "for i in range(len(x_test)):\n",
    "    val_batch = x_test[i]\n",
    "    val_batch = val_batch / input_scale + input_zero_point\n",
    "    val_batch = np.expand_dims(val_batch, axis=0).astype(input_details[0][\"dtype\"])\n",
    "    tflite_interpreter.set_tensor(input_details[0]['index'], val_batch)\n",
    "    tflite_interpreter.allocate_tensors()\n",
    "    tflite_interpreter.invoke()\n",
    "\n",
    "    tflite_model_predictions = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    #print(\"Prediction results shape:\", tflite_model_predictions.shape)\n",
    "    output = tflite_interpreter.get_tensor(output_details[0]['index'])\n",
    "    predictions[i] = output.argmax()\n",
    "    \n",
    "sum = 0\n",
    "for i in range(len(predictions)):\n",
    "    if (predictions[i] == test_labels[i]):\n",
    "        sum = sum + 1\n",
    "accuracy_score = sum / 10000\n",
    "print(\"Accuracy of model_10_tiny_Base-CubeAI_Dual_3Conv_QAT.tflite is {}%\".format(accuracy_score*100))\n",
    "\n",
    "c_model_name = 'cifar10_tiny_base_cubeAI_dual_3conv_qat'\n",
    "# check if dir 'cfiles' exists, if not create it\n",
    "if not os.path.exists('cfiles'):\n",
    "    os.makedirs('cfiles')\n",
    "# Write TFLite model to a C source (or header) file\n",
    "with open('cfiles/' + c_model_name + '.h', 'w') as file:\n",
    "    file.write(hex_to_c_array(tflite_model_quant_int8_qat, c_model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06699f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
